{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b9cea67",
   "metadata": {},
   "source": [
    "# Rutina de análisis inicial y de resultados de datos etiquetados durante QC a EMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20690e7",
   "metadata": {},
   "source": [
    "> Elaborado por Paola Álvarez, profesional contratista IDEAM, contrato 196 de 2024. Comentarios o inquietudes, remitir a *palvarez@ideam.gov.co* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d67452",
   "metadata": {},
   "source": [
    "El análisis de resultados se incluye dentro del documento de diagnóstico de series temporales.\n",
    "___\n",
    "**Librerías:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42949f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.image as mpimg\n",
    "import glob\n",
    "import os\n",
    "import statistics\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1de0e53",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9a310",
   "metadata": {},
   "source": [
    "### Longitud y continuidad series de datos de EMA - Gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc27f4f3-99ce-434a-9245-e56e45fc74f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the directory containing the CSV files\n",
    "#data_directory = '../OE_3_QC_Variables/1_PresionAtmosferica/RawUnmodified_PA/'\n",
    "data_directory = '../OE_3_QC_Variables/1_PresionAtmosferica/QCResult_PA/'\n",
    "\n",
    "# List to hold dataframes\n",
    "dataframes = []\n",
    "\n",
    "# Load each CSV file into a dataframe and add it to the list\n",
    "for filename in os.listdir(data_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(data_directory, filename), parse_dates=['event_time'], encoding='latin-1') #Fecha\n",
    "\n",
    "            # Se verifica si 'Estado' existe y se aplica filtro\n",
    "            if 'state' in df.columns: #Estado\n",
    "                df = df[df['state'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PCO']]))] #Estado\n",
    "\n",
    "            # Only proceed if the DataFrame is not empty after filtering\n",
    "            if not df.empty:\n",
    "                df = df.copy()\n",
    "                df.set_index('event_time', inplace=True)\n",
    "                df['Presence'] = 1  # Add a column to indicate data presence\n",
    "                dataframes.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {filename}: {e}\")\n",
    "\n",
    "# Se muestran los headers y fechas por archivo para ver estructura - Para caso de ejemplo\n",
    "dataframes_info = [(df.head(), df.index.min(), df.index.max()) for df in dataframes]\n",
    "dataframes_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08485480-900a-4190-9743-b90645f38526",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esta si\n",
    "# Ordenar lista de dataframes\n",
    "dataframes = sorted(dataframes, key=lambda x: x['station'].iloc[0])\n",
    "\n",
    "# Se crea el date range con las fechas inicial y final conocidas, para presión atmosférica 01/01/2001 a 03/04/2024, horaria\n",
    "date_range = pd.date_range(start=\"2001-01-01 00:00\", end=\"2024-04-08 23:00\", freq='h')\n",
    "\n",
    "# Número de estaciones por gráfico\n",
    "estaciones_por_grafico = 50\n",
    "\n",
    "# Total de gráficos a generar\n",
    "total_graficos = len(dataframes) // estaciones_por_grafico + (len(dataframes) % estaciones_por_grafico > 0)\n",
    "\n",
    "# Configuración de la fuente para todo el gráfico\n",
    "font = {'family': 'Franklin Gothic Book',\n",
    "        'weight': 'normal',\n",
    "        'size': 10}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "for j in range(total_graficos):\n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    inicio = j * estaciones_por_grafico\n",
    "    fin = min(inicio + estaciones_por_grafico, len(dataframes))  # Asegurarse de no pasarse del rango\n",
    "\n",
    "    # Define a helper function to plot data for clarity\n",
    "    def plot_station_data(ax, data, index, station_label):\n",
    "        # Find where data is present\n",
    "        presence_mask = data['Presence'] == 1\n",
    "        # Plot only where data is present\n",
    "        ax.fill_between(data.index, index - 0.45, index + 0.45, where=~presence_mask, color='#f0f0f0', step='mid', label='Sin dato' if i == 1 else \"\")\n",
    "        ax.fill_between(data.index, index - 0.45, index + 0.45, where=presence_mask, color='orange', step='mid', label='Con dato' if i == 1 else \"\")\n",
    "\n",
    "    for i, df in enumerate(dataframes[inicio:fin], start=1):\n",
    "        # Reindex the dataframe to the full date range, filling missing data with 0 (indicating absence)\n",
    "        df_complete = df.reindex(date_range, fill_value=0)\n",
    "        plot_station_data(ax, df_complete, i, f\"Station {df['station'].iloc[0]}\")\n",
    "\n",
    "    #ax.legend(loc='upper left')  # Agrega la leyenda en la esquina superior izquierda\n",
    "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), fancybox=True, shadow=True, ncol=2)\n",
    "    # Setting the labels and ticks for better readability\n",
    "    ax.set_yticks(range(1, fin - inicio + 1))\n",
    "    ax.set_yticklabels([f\"{df['station'].iloc[0]}\" for df in dataframes[inicio:fin]], fontsize=8)\n",
    "    ax.set_ylim(0.5, fin - inicio + 0.5)\n",
    "    ax.set_xlim(date_range[0], date_range[-1])\n",
    "    ax.xaxis.set_major_locator(mdates.YearLocator())\n",
    "    ax.xaxis.set_minor_locator(mdates.MonthLocator())\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.ylabel('Código de estación CNE')\n",
    "    plt.title(f'Completitud de datos por estación - Grupo de estaciones {j+1} de {total_graficos}')\n",
    "    plt.grid(True, which='both', linestyle='-', linewidth=0.25)\n",
    "\n",
    "    # Show and save the plot\n",
    "    plt.tight_layout()\n",
    "    # Definir la carpeta donde se guardará el gráfico\n",
    "    output_folder = \"Figs_LongyCompletDatos_PA_qc\"  # Cambia esto por la carpeta que quieras\n",
    "    os.makedirs(output_folder, exist_ok=True)  # Crear la carpeta si no existe\n",
    "    # Guardar el gráfico en la carpeta especificada\n",
    "    plt.savefig(os.path.join(output_folder, f'completitud_datos_grafico_{j+1}.png'))\n",
    "    plt.close(fig)  # Cierra la figura para liberar memoria"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0970dd4d-3803-4807-87a7-87fb535d7f49",
   "metadata": {},
   "source": [
    "____________"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271f66c",
   "metadata": {},
   "source": [
    "### Cantidad total de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0eff5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se indaga sobre cuál es la cantidad de datos de todas las series descargadas en el período 2001-01-01 al 2024-03-31\n",
    "# Se crea la función cantidad_datos\n",
    "def cantidad_datos(archivos):\n",
    "    total_filas = 0\n",
    "\n",
    "    for archivo in archivos:\n",
    "        # Se utiliza el método 'chunksize' de pandas para leer los archivos por partes (chunks)\n",
    "        # y procesarlos de forma incremental sin cargar todos los datos en memoria al mismo tiempo\n",
    "        chunks = pd.read_csv(archivo, encoding='latin-1', chunksize=100000)  # Se establece este valor de chunk para poder hacer otras actividades\n",
    "\n",
    "        for chunk in chunks:\n",
    "            total_filas += len(chunk)\n",
    "\n",
    "    return total_filas\n",
    "\n",
    "# Ruta de la carpeta con los archivos CSV\n",
    "carpeta_proces = 'RawUnmodified_Patm/'\n",
    "\n",
    "# Obtener la lista de archivos en la carpeta\n",
    "archivos = [carpeta_proces + archivo for archivo in os.listdir(carpeta_proces) if archivo.endswith('.csv')]\n",
    "\n",
    "# Llamar a la función para contar las filas\n",
    "total_filas = cantidad_datos(archivos)\n",
    "\n",
    "# Imprimir el resultado\n",
    "print(\"El número total de datos es:\", total_filas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ae3a4b-8a6f-42a6-8e20-dec3623e126e",
   "metadata": {},
   "source": [
    "### Contar datos por fecha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc70dd-488c-4fb8-ba51-6f3ff6d1b6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contar_datos_por_fechas(archivos):\n",
    "    resultados = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        chunks = pd.read_csv(archivo, encoding='latin-1', chunksize=100000)\n",
    "        \n",
    "        for chunk in chunks:\n",
    "            chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "            estacion = chunk['Station'].iloc[0]\n",
    "            \n",
    "            # Conteo por año, mes y día\n",
    "            conteo_dia = chunk['Fecha'].dt.to_period('D').value_counts().rename_axis('Año-mes-dia').reset_index(name='Cantidad_año-mes-dia')\n",
    "            conteo_dia['Año-mes'] = conteo_dia['Año-mes-dia'].dt.to_timestamp().dt.to_period('M')\n",
    "            conteo_dia['Año'] = conteo_dia['Año-mes'].dt.year\n",
    "\n",
    "            # Conteo por año y mes\n",
    "            conteo_mes = chunk['Fecha'].dt.to_period('M').value_counts().rename_axis('Año-mes').reset_index(name='Cantidad_año-mes')\n",
    "            conteo_mes['Año'] = conteo_mes['Año-mes'].dt.year\n",
    "\n",
    "            # Conteo por año\n",
    "            conteo_anual = chunk['Fecha'].dt.year.value_counts().rename_axis('Año').reset_index(name='Cantidad_año')\n",
    "            \n",
    "            # Combinar con la información de la estación\n",
    "            conteo_anual['Station'] = estacion\n",
    "            conteo_mes['Station'] = estacion\n",
    "            conteo_dia['Station'] = estacion\n",
    "            \n",
    "            # Merge considerando Station, Año y Año-mes\n",
    "            merged_data = pd.merge(pd.merge(conteo_anual, conteo_mes, on=['Station', 'Año']), conteo_dia, on=['Station', 'Año', 'Año-mes'])\n",
    "            resultados.append(merged_data)\n",
    "    \n",
    "    df_resultados = pd.concat(resultados, ignore_index=True)\n",
    "    # Reordenar columnas\n",
    "    df_resultados = df_resultados[['Station', 'Año', 'Cantidad_año', 'Año-mes', 'Cantidad_año-mes','Año-mes-dia','Cantidad_año-mes-dia']]\n",
    "    df_resultados.sort_values(by=['Station', 'Año', 'Año-mes', 'Año-mes-dia'], inplace=True)\n",
    "    \n",
    "    # Rellenar la columna 'Cantidad_año' sólo una vez por cada año y estación\n",
    "    df_resultados['Cantidad_año'] = df_resultados.groupby(['Station', 'Año'])['Cantidad_año'].transform(lambda x: x.iloc[0] if not x.isna().all() else pd.NA)\n",
    "    \n",
    "    df_resultados.to_csv('ConteoDatos_AnioMesDia_PA.csv', index=False, encoding='latin-1')\n",
    "    return df_resultados\n",
    "\n",
    "# Uso de la función\n",
    "carpeta_proces = 'RawUnmodified_Patm/'\n",
    "archivos = [carpeta_proces + archivo for archivo in os.listdir(carpeta_proces) if archivo.endswith('.csv')]\n",
    "resultados = contar_datos_por_fechas(archivos)\n",
    "print(resultados.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05e174-1767-4db4-9ccb-7b686eb25bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lectura de frecuencias para revisión inicial\n",
    "dffreq = pd.read_csv('ConteoDatos_AnioMesDia_PA.csv', encoding='latin-1')\n",
    "dffreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa70df61-4769-4d37-a892-3b84d3d75829",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se agrupa por 'Station' y se suma la columna respectiva\n",
    "sttn_qtty = dffreq.groupby('Station')['Cantidad_año-mes-dia'].sum().reset_index()\n",
    "\n",
    "# Se renombra la columna\n",
    "sttn_qtty.rename(columns={'Cantidad_año-mes-dia': 'CantDatos'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7de7b4-cc67-448d-b338-52144688fcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sttn_qtty.to_csv('CantDatosPorEstacion_PA.csv', encoding='latin-1', index=True, sep=';')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44cc995",
   "metadata": {},
   "source": [
    "### Obtener fechas iniciales y finales de estaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6980883c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_fechas(archivo):\n",
    "    try:\n",
    "        # Se lee el archivo\n",
    "        datos = pd.read_csv(archivo, encoding='latin-1')#, names=['Fecha', 'Valor'])\n",
    "        try:\n",
    "            datos['Fecha'] = pd.to_datetime(datos['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        except ValueError:\n",
    "            datos['Fecha'] = pd.to_datetime(datos['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Se obtiene el código de la estación\n",
    "        station = datos['Station'].values[0]\n",
    "        \n",
    "        # Se obtiene primera y última fecha \n",
    "        fecha_inicial = datos['Fecha'].iloc[0]\n",
    "        fecha_final = datos['Fecha'].iloc[-1]\n",
    "        \n",
    "        return station, fecha_inicial, fecha_final\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error con el archivo {archivo}: {e}\")\n",
    "        return None, None, None\n",
    "\n",
    "def main():\n",
    "    # Cambia la ruta según la ubicación de tu carpeta\n",
    "    ruta_carpeta = '../OE_3_QC_Variables/1_PresionAtmosferica/RawUnmodified_Patm'\n",
    "\n",
    "    # Listamos todos los archivos en la carpeta que terminen con .csv\n",
    "    archivos = [f for f in os.listdir(ruta_carpeta) if f.endswith('.csv')]\n",
    "    \n",
    "    # Creamos una lista para almacenar los resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Iteramos sobre cada archivo y obtenemos las fechas\n",
    "    for archivo in archivos:\n",
    "        ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "        station, fecha_inicial, fecha_final = obtener_fechas(ruta_archivo)\n",
    "        \n",
    "        if station and fecha_inicial and fecha_final:\n",
    "            resultados.append([station, fecha_inicial, fecha_final])\n",
    "    \n",
    "    # Convertimos los resultados a un DataFrame\n",
    "    resultados_df = pd.DataFrame(resultados, columns=['CodEstacion', 'fecha_inicial', 'fecha_final'])\n",
    "    \n",
    "    # Guardamos el DataFrame como un archivo CSV\n",
    "    resultados_df.to_csv('FechaInicialFinal_Patm.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340adf4d",
   "metadata": {},
   "source": [
    "### Obtener estadísticos descriptivos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0582ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtener_EstadDescript(archivo):\n",
    "    try:\n",
    "        # Se lee el archivo\n",
    "        datos = pd.read_csv(archivo, encoding='latin-1')#, names=['Fecha', 'Valor'])\n",
    "        try:\n",
    "            datos['Fecha'] = pd.to_datetime(datos['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        except ValueError:\n",
    "            datos['Fecha'] = pd.to_datetime(datos['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Se obtiene el código de la estación\n",
    "        station = datos['Station'].values[0]\n",
    "        \n",
    "        # Se obtienen los estadísticos descriptivos\n",
    "        minimo = datos['Valor'].min()\n",
    "        maximo = datos['Valor'].max()\n",
    "        media = datos['Valor'].mean()\n",
    "        mediana = datos['Valor'].median()\n",
    "        desvest = datos['Valor'].std()\n",
    "        varianza = datos['Valor'].var()\n",
    "        #first_q = datos['Valor'].quantile(0.25)\n",
    "        #third_q = datos['Valor'].quantile(0.75)\n",
    "        \n",
    "        return station, minimo, maximo, media, mediana, desvest, varianza, #first_q, third_q, mediana --no fue posible calcular estos estad.\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error con el archivo {archivo}: {e}\")\n",
    "        print(e.__class__)\n",
    "        return None, None, None, None, None, None, None#, None, None --no fue posible calcular estos estad.\n",
    "\n",
    "def main():\n",
    "    # Cambia la ruta según la ubicación de tu carpeta\n",
    "    ruta_carpeta = 'RawUnmodified_PA'\n",
    "\n",
    "    # Listamos todos los archivos en la carpeta que terminen con .csv\n",
    "    archivos = [f for f in os.listdir(ruta_carpeta) if f.endswith('.csv')]\n",
    "    \n",
    "    # Se crea una lista para almacenar los resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Se itera sobre cada archivo y se obtienen fechas las fechas\n",
    "    for archivo in archivos:\n",
    "        ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "        station, minimo, maximo, media, mediana, desvest, varianza= obtener_EstadDescript(ruta_archivo)\n",
    "        print(station, minimo, maximo, media, mediana, desvest, varianza)\n",
    "        resultados.append([station, minimo, maximo, media, mediana, desvest, varianza])\n",
    "        #if station and minimo: #and maximo and media and mediana and desvest and varianza:\n",
    "            #resultados.append([station, minimo, maximo, media, mediana, desvest, varianza])\n",
    "        #else:\n",
    "            #print(f'Sin suficientes resultado para {archivo}. No se generaron sus estadísticos')\n",
    "    \n",
    "    # Se convierten los resultados a un DataFrame\n",
    "    resultados_df = pd.DataFrame(resultados, columns=['Station', 'minimo', 'maximo', 'media', 'mediana','desvest', 'varianza'])\n",
    "    \n",
    "    print(resultados_df)\n",
    "    \n",
    "    # Se guarda el DataFrame como un archivo CSV\n",
    "    resultados_df.to_csv('EstadDescript_Patm_raw.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df16d760-dda9-4659-af72-45c3e70cee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "resultados_df = pd.read_csv('EstadDescript_Patm_Raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c86dcad-8226-4e4e-a3c3-d5513ba0c97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "altipatm = pd.read_table('EMA_Patm_TMaxMin.txt', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582de89c-cb55-4dc2-935b-5a5fdfb9c1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "altipatm.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8778590-87b1-46eb-a8b0-19d2f607e0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "estadalt = pd.merge(resultados_df, altipatm[['Station','Altitud']], on='Station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b1c9dc-c99e-479f-ae9d-67bb4a80ddc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se sobreescriben los estadísticos para incluir altitud\n",
    "estadalt.to_csv('EstadDescript_Patm_Raw.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facb8869-5586-4431-a71a-bf49c612a291",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb150cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# De datos con QC\n",
    "def obtener_EstadDescript(archivo):\n",
    "    try:\n",
    "        # Se lee el archivo\n",
    "        datos = pd.read_csv(archivo, encoding='latin-1')#, names=['Fecha', 'Valor'])\n",
    "        try:\n",
    "            datos['Fecha'] = pd.to_datetime(datos['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "        except ValueError:\n",
    "            datos['Fecha'] = pd.to_datetime(datos['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "        \n",
    "        # Se hace el filtro para que solo queden los valores que superaron las pruebas\n",
    "        dfC = datos[~datos['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PSO','0PAT','0PER']]))]\n",
    "        \n",
    "        # Se obtiene el código de la estación\n",
    "        station = dfC['Station'].values[0]\n",
    "        \n",
    "        # Se obtienen los estadísticos descriptivos\n",
    "        minimo = dfC['Valor'].min()\n",
    "        maximo = dfC['Valor'].max()\n",
    "        media = dfC['Valor'].mean()\n",
    "        #mediana = datos['Valor'].median()\n",
    "        desvest = dfC['Valor'].std()\n",
    "        varianza = dfC['Valor'].var()\n",
    "        #first_q = datos['Valor'].quantile(0.25)\n",
    "        #third_q = datos['Valor'].quantile(0.75)\n",
    "        \n",
    "        return station, minimo, maximo, media, desvest, varianza, #first_q, third_q, mediana --no fue posible calcular estos estad.\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error con el archivo {archivo}: {e}\")\n",
    "        print(e.__class__)\n",
    "        return None, None, None, None, None, None#, None, None, None --no fue posible calcular estos estad.\n",
    "\n",
    "def main():\n",
    "    # Cambia la ruta según la ubicación de tu carpeta\n",
    "    #ruta_carpeta = '../OE_3_QC_Variables/1_PresionAtmosferica/QCResult_Patm/V2'\n",
    "    ruta_carpeta = '../OE_3_QC_Variables/1_PresionAtmosferica/QCResult_Patm/V3'\n",
    "\n",
    "    # Listamos todos los archivos en la carpeta que terminen con .csv\n",
    "    archivos = [f for f in os.listdir(ruta_carpeta) if f.endswith('.csv')]\n",
    "    \n",
    "    # Creamos una lista para almacenar los resultados\n",
    "    resultados = []\n",
    "\n",
    "    # Iteramos sobre cada archivo y obtenemos las fechas\n",
    "    for archivo in archivos:\n",
    "        ruta_archivo = os.path.join(ruta_carpeta, archivo)\n",
    "        station, minimo, maximo, media, desvest, varianza= obtener_EstadDescript(ruta_archivo)\n",
    "        \n",
    "        if station and minimo and maximo and media and desvest and varianza:\n",
    "            resultados.append([station, minimo, maximo, media, desvest, varianza])\n",
    "    \n",
    "    # Convertimos los resultados a un DataFrame\n",
    "    resultados_df = pd.DataFrame(resultados, columns=['Station', 'minimo', 'maximo', 'media', 'desvest', 'varianza'])\n",
    "    \n",
    "    # Guardamos el DataFrame como un archivo CSV\n",
    "    resultados_df.to_csv('EstadDescript_PA_qc.csv', index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c424855f",
   "metadata": {},
   "source": [
    "_____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745830f2",
   "metadata": {},
   "source": [
    "## Cantidad de datos abanderados al aplicar QC automatizados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc4f0495",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cantidad y porcentaje de datos abanderados por estación\n",
    "def generar_dataframe_compilado(archivos):\n",
    "    datos = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        # Leer el archivo CSV en modo iterador y especificar los tipos de datos\n",
    "        reader = pd.read_csv(archivo, iterator=True, dtype={'station': str, 'state': str}, chunksize=70000, \n",
    "                             encoding='latin-1')\n",
    "\n",
    "        # Inicializar contador de filas y contador de '0P'\n",
    "        total_filas = 0\n",
    "        cantidad_0PSO = 0\n",
    "        cantidad_0PER = 0\n",
    "        cantidad_0PAT = 0\n",
    "        cantidad_0PCO = 0\n",
    "\n",
    "        # Obtener los chunks de datos del archivo\n",
    "        for chunk in reader:\n",
    "            total_filas += len(chunk)\n",
    "            cantidad_0PSO +=  chunk[chunk['state'].str.startswith('0PSO', na=False)].shape[0]\n",
    "            cantidad_0PER +=  chunk[chunk['state'].str.startswith('0PER', na=False)].shape[0]\n",
    "            cantidad_0PAT +=  chunk[chunk['state'].str.startswith('0PAT', na=False)].shape[0]\n",
    "            cantidad_0PCO += chunk[chunk['state'].str.startswith('0PCO', na=False)].shape[0]\n",
    "\n",
    "        cantidad_0P = cantidad_0PSO + cantidad_0PER + cantidad_0PAT\n",
    "        \n",
    "        # Obtener el 'Station' respectivo\n",
    "        station = chunk['station'].iloc[0]\n",
    "        \n",
    "        # Obtener porcentajes\n",
    "        perc_0PSO = round((cantidad_0PSO/total_filas*100),2)\n",
    "        perc_0PER = round((cantidad_0PER/total_filas*100),2)\n",
    "        perc_0PAT = round((cantidad_0PAT/total_filas*100),2)\n",
    "        perc_0P = round((cantidad_0P/total_filas*100),2)\n",
    "        perc_0PCO = round((cantidad_0PCO/total_filas*100),2)\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        datos.append({'Estacion': station, 'TotalDatos': total_filas, 'Estado_0PER': cantidad_0PER,\n",
    "                      'Estado_0PAT': cantidad_0PAT, 'Estado_0PSO': cantidad_0PSO, 'Estado_0PCO': cantidad_0PCO, 'Estado_0P': cantidad_0P,\n",
    "                      'Per_0PER': perc_0PER, 'Per_0PAT': perc_0PAT, 'Per_0PSO': perc_0PSO, 'Per_0P': perc_0P, 'Per_0PCO': perc_0PCO})\n",
    "        #print(datos)\n",
    "\n",
    "    # Crear el nuevo DataFrame compilado\n",
    "    df_compilado = pd.DataFrame(datos)\n",
    "\n",
    "    return df_compilado\n",
    "\n",
    "# Ruta de la carpeta con los archivos CSV\n",
    "carpeta = '../OE_3_QC_Variables/1_PresionAtmosferica/QCResult_PA/'\n",
    "\n",
    "# Obtener la lista de archivos en la carpeta\n",
    "archivos = [carpeta + archivo for archivo in os.listdir(carpeta) if archivo.endswith('.csv')]\n",
    "\n",
    "# Llamar a la función para generar el nuevo DataFrame compilado\n",
    "df_compilado = generar_dataframe_compilado(archivos)\n",
    "\n",
    "# Imprimir el nuevo DataFrame\n",
    "print(df_compilado)\n",
    "\n",
    "# Exportar el df compilado\n",
    "df_compilado.to_csv('CantBanderas_QCResult_PA.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873f16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cantidad y porcentaje de datos abanderados por entidad\n",
    "def generar_dataframe_compilado(archivos):\n",
    "    datos = []\n",
    "\n",
    "    for archivo in archivos:\n",
    "        # Leer el archivo CSV en modo iterador y especificar los tipos de datos\n",
    "        reader = pd.read_csv(archivo, iterator=True, dtype={'station': str, 'state': str}, chunksize=70000, \n",
    "                             encoding='latin-1')\n",
    "\n",
    "        # Inicializar contador de filas y contador de '0P'\n",
    "        total_filas = 0\n",
    "        cantidad_0PSO = 0\n",
    "        cantidad_0PER = 0\n",
    "        cantidad_0PAT = 0\n",
    "        cantidad_0PCO = 0\n",
    "\n",
    "        # Obtener los chunks de datos del archivo\n",
    "        for chunk in reader:\n",
    "            total_filas += len(chunk)\n",
    "            cantidad_0PSO +=  chunk[chunk['state'].str.startswith('0PSO', na=False)].shape[0]\n",
    "            cantidad_0PER +=  chunk[chunk['state'].str.startswith('0PER', na=False)].shape[0]\n",
    "            cantidad_0PAT +=  chunk[chunk['state'].str.startswith('0PAT', na=False)].shape[0]\n",
    "            cantidad_0PCO += chunk[chunk['state'].str.startswith('0PCO', na=False)].shape[0]\n",
    "\n",
    "        cantidad_0P = cantidad_0PSO + cantidad_0PER + cantidad_0PAT\n",
    "\n",
    "        cantidad_0P = cantidad_0PSO + cantidad_0PER + cantidad_0PAT\n",
    "        \n",
    "        # Obtener el 'Station' respectivo\n",
    "        station = chunk['station'].iloc[0]\n",
    "        \n",
    "        # Obtener porcentajes\n",
    "        perc_0PSO = round((cantidad_0PSO/total_filas*100),2)\n",
    "        perc_0PER = round((cantidad_0PER/total_filas*100),2)\n",
    "        perc_0PAT = round((cantidad_0PAT/total_filas*100),2)\n",
    "        perc_0P = round((cantidad_0P/total_filas*100),2)\n",
    "        perc_0PCO = round((cantidad_0PCO/total_filas*100),2)\n",
    "\n",
    "        # Agregar los datos a la lista\n",
    "        datos.append({'station': station, 'TotalDatos': total_filas, 'Estado_0PER': cantidad_0PER,\n",
    "                      'Estado_0PAT': cantidad_0PAT, 'Estado_0PSO': cantidad_0PSO, 'Estado_0PCO': cantidad_0PCO, 'Estado_0P': cantidad_0P,\n",
    "                      'Per_0PER': perc_0PER, 'Per_0PAT': perc_0PAT, 'Per_0PSO': perc_0PSO, 'Per_0P': perc_0P, 'Per_0PCO': perc_0PCO})\n",
    "\n",
    "    # Crear el nuevo DataFrame compilado\n",
    "    df_compilado = pd.DataFrame(datos)\n",
    "    # Convertir la columna 'Station' a tipo 'int64'\n",
    "    df_compilado['station'] = df_compilado['station'].astype('int64')\n",
    "\n",
    "    # Cargar el archivo 'EMA_AllInfo.txt'\n",
    "    ema_all = pd.read_csv('../OE_3_QC_Variables/1_PresionAtmosferica/EMAPatm_allinfo.csv', sep=';', encoding='latin-1')\n",
    "    \n",
    "    # Convertir la columna 'Station' a tipo 'str'\n",
    "    ema_all['station'] = ema_all['station'].astype('int64')\n",
    "    \n",
    "    # Unir los dataframes para agregar la columna 'project'\n",
    "    df_compilado = pd.merge(df_compilado, ema_all[['station', 'entidad']], on='station', how='left')\n",
    "\n",
    "    # Agrupar por proyecto y obtener los resultados deseados\n",
    "    resultado = df_compilado.groupby('entidad').agg({\n",
    "        'TotalDatos': 'sum',\n",
    "        'Estado_0PSO': 'sum',\n",
    "        'Estado_0PAT': 'sum',\n",
    "        'Estado_0PER': 'sum',\n",
    "        'Estado_0P': 'sum',\n",
    "        'Estado_0PCO': 'sum',\n",
    "        'Per_0PSO': 'mean',\n",
    "        'Per_0PAT': 'mean',\n",
    "        'Per_0PER': 'mean',\n",
    "        'Per_0P': 'mean',\n",
    "        'Per_0PCO': 'mean',\n",
    "    }).reset_index()\n",
    "\n",
    "    return resultado\n",
    "\n",
    "# Ruta de la carpeta con los archivos CSV\n",
    "carpeta = '../OE_3_QC_Variables/1_PresionAtmosferica/QCResult_PA/'\n",
    "\n",
    "# Obtener la lista de archivos en la carpeta\n",
    "archivos = [carpeta + archivo for archivo in os.listdir(carpeta) if archivo.endswith('.csv')]\n",
    "\n",
    "# Llamar a la función para generar el nuevo DataFrame compilado\n",
    "df_compilado = generar_dataframe_compilado(archivos)\n",
    "\n",
    "# Imprimir el nuevo DataFrame\n",
    "print(df_compilado)\n",
    "\n",
    "# Exportar el df compilado\n",
    "df_compilado.to_csv('CantBanderas_QCResult_PA_entidad.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c7991d-5e05-4e5a-af9c-95b2be539031",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87c064-ae27-47b0-b87a-3284512434a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Se genera rutina para analizar los porcentajes obtenidos de datos etiquetados\n",
    "dfflag = pd.read_csv('CantBanderas_QCResult_PA.csv')\n",
    "dfflag.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f19ec7-0b47-455b-a8be-4abb3e0e6008",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generar gráfico por rango de porcentajes obtenidos de 'QC'\n",
    "# Opción de rangos de 5 en 5\n",
    "en5 = range(0, 106, 5)  # Rango de bins\n",
    "n5 = list(en5)\n",
    "bins = n5\n",
    "labels = [f'{i}-{i + 4} %' for i in range(0, 100, 5)] + ['100 %']\n",
    "dfflag['RangPerc0P'] = pd.cut(dfflag['Per_0P'], bins=bins, labels=labels, right=False)\n",
    "sizes = dfflag['RangPerc0P'].value_counts()\n",
    "print(sizes)\n",
    "\n",
    "# Convirtiendo a DataFrame para mejor manejo\n",
    "df = pd.DataFrame({'Rangos': labels, 'Valores': sizes})\n",
    "\n",
    "# Calcula el total y el umbral\n",
    "total = df['Valores'].sum()\n",
    "threshold = 0.015  # 1%\n",
    "total = sizes.sum()\n",
    "\n",
    "# Filtrar aquellos que cumplen con el umbral\n",
    "mask = sizes/total >= threshold\n",
    "\n",
    "# Las categorías y valores que cumplen con el umbral\n",
    "filtered_labels = sizes.index[mask].tolist()\n",
    "filtered_values = sizes.values[mask].tolist()\n",
    "\n",
    "# La suma de las categorías que NO cumplen con el umbral\n",
    "remaining_sum = sizes[~mask].sum()\n",
    "\n",
    "# Añadir la categoría 'Otros' si hay valores que cumplen el umbral\n",
    "if remaining_sum > 0:\n",
    "    filtered_labels.append('Otros')\n",
    "    filtered_values.append(remaining_sum)\n",
    "\n",
    "# Colores\n",
    "colors = plt.cm.Paired(range(len(filtered_labels)))\n",
    "\n",
    "# Gráfico de torta\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "wedges, texts, autotexts = ax.pie(filtered_values, labels=None, colors=colors, autopct='%1.1f%%', pctdistance=0.7,\n",
    "                                  startangle=90, wedgeprops=dict(width=0.5, edgecolor='white'))\n",
    "\n",
    "# Modificar el estilo de los porcentajes\n",
    "for autotext in autotexts:\n",
    "    autotext.set_fontname('Verdana')\n",
    "    autotext.set_fontsize(7)\n",
    "    autotext.set_fontstretch('condensed')\n",
    "    autotext.set_bbox(dict(facecolor='white', alpha=0.6, edgecolor='gray', boxstyle='round,pad=0.2'))\n",
    "\n",
    "# Estilo para las etiquetas\n",
    "bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "kw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n",
    "          bbox=bbox_props, zorder=0, va=\"center\")\n",
    "\n",
    "# Colocar etiquetas con líneas\n",
    "for i, p in enumerate(wedges):\n",
    "    ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "    y = np.sin(np.deg2rad(ang))\n",
    "    x = np.cos(np.deg2rad(ang))\n",
    "    horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "    connectionstyle = f\"angle,angleA=0,angleB={ang}\"\n",
    "    kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n",
    "    ax.annotate(filtered_labels[i], xy=(x, y), xytext=(1.15*np.sign(x), 1.15*y),\n",
    "                horizontalalignment=horizontalalignment, fontname='Verdana', fontsize=8, fontstretch='condensed', **kw)\n",
    "\n",
    "# Título y formato\n",
    "plt.title('Distribución de estaciones por rango de porcentaje de fallo en QC', fontname='Verdana', fontstretch='condensed', \n",
    "          fontsize=13, fontweight='bold', y=1.05)\n",
    "ax.axis('equal')  # Igualar la relación de aspecto para que la torta sea circular\n",
    "\n",
    "# Cargar el logo IDEAM\n",
    "logo = mpimg.imread('Logo_IDEAM_Color2.png')\n",
    "\n",
    "# Agregar el logo en la esquina superior izquierda\n",
    "newax = fig.add_axes([0.04, 0.8, 0.2, 0.1], anchor='NE', zorder=1)\n",
    "newax.imshow(logo, alpha=0.5) \n",
    "newax.axis('off')\n",
    "\n",
    "plt.savefig(f'FalloQC_porcEstaciones_PA.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bcd37-8bc4-4ba4-83c9-75676b65b6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar gráfico por rango de porcentajes obtenidos de '0PC0'\n",
    "# Opción de rangos de 5 en 5\n",
    "en5 = range(0, 106, 5)  # Rango de bins\n",
    "n5 = list(en5)\n",
    "bins = n5\n",
    "labels = [f'{i}-{i + 4}' for i in range(0, 100, 5)] + ['100']\n",
    "dfflag['RangPerc'] = pd.cut(dfflag['Per_0PCO'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "\n",
    "# Crear el gráfico\n",
    "fig = plt.figure(figsize=(10,6))\n",
    "ax = dfflag['RangPerc'].value_counts(sort=False).iloc[:-1].plot(kind='bar', color='orange', edgecolor='dimgray')\n",
    "plt.xlabel('Rango de porcentaje (%)', fontname='Verdana', fontsize=10, fontweight='bold', fontstretch='condensed')\n",
    "plt.ylabel('Cantidad de estaciones', fontname='Verdana', fontsize=10, fontweight='bold', fontstretch='condensed')\n",
    "plt.title('Cantidad de estaciones por rango de porcentaje de datos que no fallaron en los QC', fontname='Verdana', \n",
    "          fontsize=13, fontweight='bold', fontstretch='condensed')\n",
    "plt.xticks(fontname='Verdana', fontsize=9, fontstretch='condensed', rotation=45) # si de 5 en 5 rotation=45\n",
    "plt.yticks(fontname='Verdana', fontsize=9, fontstretch='condensed')\n",
    "plt.ylim(0,200)\n",
    "ax.set_facecolor('whitesmoke')\n",
    "#plt.grid(axis='y', linestyle='-', alpha=0.7, color='gainsboro')\n",
    "\n",
    "color = mcolors.to_rgba('papayawhip')#, alpha=0.1)\n",
    "# Añadir etiquetas de datos\n",
    "ax.bar_label(ax.containers[0], label_type='edge', padding=5, fontname='Verdana', fontsize=9, color='dimgray', \n",
    "             backgroundcolor=color)\n",
    "\n",
    "# Cargar el logo IDEAM\n",
    "logo = mpimg.imread('Logo_IDEAM_Color2.png')\n",
    "\n",
    "# Agregar el logo en la esquina superior izquierda\n",
    "newax = fig.add_axes([0.01, 0.75, 0.2, 0.1], anchor='NE', zorder=1)\n",
    "newax.imshow(logo, alpha=0.5) \n",
    "newax.axis('off')\n",
    "\n",
    "plt.savefig(f'CorrectoQC_cantEstaciones_PA.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29922c7-0b2d-46ad-a6ae-026dd209bcd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
