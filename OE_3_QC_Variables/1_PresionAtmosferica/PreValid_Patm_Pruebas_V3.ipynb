{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f2b16b-912e-4a9b-98c0-5a393e545013",
   "metadata": {},
   "source": [
    "# Pruebas pre-validación datos presión atmosférica - Clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885f6f6",
   "metadata": {},
   "source": [
    "> Elaborado por Paola Álvarez, profesional contratista IDEAM, contrato 196 de 2024. Comentarios o inquietudes, remitir a *palvarez@ideam.gov.co* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ebabc-ed05-4660-a06e-7c242cb0ffeb",
   "metadata": {},
   "source": [
    "**Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28398810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from functools import wraps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14c1a4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7267c",
   "metadata": {},
   "source": [
    "A continuación, se encuentran las pruebas de pre-validación de datos de EMA para verificar su capacidad de detección de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2be3c",
   "metadata": {},
   "source": [
    "## Clase con métodos de aplicación de QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c9474e9-bcdb-4d60-a03d-7e72ebdf588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del logger para guardar en el directorio de archivos y escribir cada vez\n",
    "def setup_logger(log_file_path):\n",
    "    logger = logging.getLogger('RawUnmodified_Patm')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='a')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "def log_failures(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(self, chunk, archivo):\n",
    "        try:\n",
    "            result, mask = func(self, chunk, archivo)\n",
    "            if mask is not None:\n",
    "                try:\n",
    "                    aligned_mask = mask.reindex(chunk.index, fill_value=False)  # Asegura que la máscara esté alineada con el índice del DataFrame\n",
    "                except AttributeError:\n",
    "                    aligned_mask = mask  # Si no se puede reindexar, usa la máscara tal como está\n",
    "                for index, row in chunk[aligned_mask].iterrows():\n",
    "                    self.logger.info('File: %s - Row: %s - Failed in %s: %s', archivo, index, func.__name__, row['Valor'])\n",
    "            return result\n",
    "        except ValueError as e:\n",
    "            self.logger.error('Error procesando el archivo %s: %s', archivo, str(e))\n",
    "            return chunk  # Devuelve una máscara falsa para manejar el error\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddb424af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatPatmEMA:\n",
    "    \n",
    "    def __init__(self, dir_files, chunk_size=54000):\n",
    "        self.dir_files = dir_files\n",
    "        self.ruta_archivos = os.listdir(dir_files)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.last_rows = None\n",
    "        self.current_file = None\n",
    "        # Sección configuración de logs\n",
    "        log_file_path = os.path.join(dir_files, 'QC_Patm.log')\n",
    "        self.logger = setup_logger(log_file_path)\n",
    "        self.logger.info('Inicialización de AutomatPatmEMA en directorio: %s', dir_files)\n",
    "\n",
    "    def p_transm(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si existe al menos el 70% de datos esperados por día y hora\n",
    "        en la serie de datos; aquellos que no superen la prueba, son marcados como sospechosos'''\n",
    "        # Se encontraron diferentes frecuencias en la transmisión de Patm, por lo tanto:\n",
    "        freqinst100b = pd.read_csv('EMAPatm_allinfo.csv', encoding='latin-1', sep=';')\n",
    "        \n",
    "        # Se define un diccionario de frecuencias y cantidades esperadas\n",
    "        frecuencias = {\n",
    "            'min': {'cant_esperd_h': 60, 'cant_esperd_d': 1440},\n",
    "            '2min': {'cant_esperd_h': 30, 'cant_esperd_d': 720},\n",
    "            '10min': {'cant_esperd_h': 6, 'cant_esperd_d': 144},\n",
    "            'h': {'cant_esperd_h': 1, 'cant_esperd_d': 24}\n",
    "        }\n",
    "        \n",
    "        # Obtener la frecuencia de 'freqinst100b' basado en 'Station' y asignar a 'periodos'\n",
    "        station_value = chunk['Station'].values[0]\n",
    "        if pd.isna(station_value):\n",
    "            periodos = None\n",
    "        else:\n",
    "            freqinst100b_station = freqinst100b.loc[freqinst100b['Station'] == station_value]\n",
    "            if freqinst100b_station.empty:\n",
    "                print(f\"No se encontró la estación {station_value} en freqinst100b\")\n",
    "                return chunk\n",
    "    \n",
    "            freq_inf_value = freqinst100b_station['FreqInf'].values[0]\n",
    "    \n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    print(periodos)\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return chunk\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return chunk\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return chunk\n",
    "    \n",
    "        # Obtener las cantidades esperadas de acuerdo a la frecuencia\n",
    "        cant_esperd_h = frecuencias[periodos]['cant_esperd_h']\n",
    "        cant_esperd_d = frecuencias[periodos]['cant_esperd_d']\n",
    "    \n",
    "        # Se establecen los aceptables\n",
    "        cant_aceptab_hora = 0.5 * cant_esperd_h\n",
    "        cant_aceptab_dia = 0.5 * cant_esperd_d\n",
    "    \n",
    "        # Agregar columna de etiquetas al dataframe original\n",
    "        chunk['Estado'] = ''\n",
    "        \n",
    "        # Definir función para asignar etiquetas\n",
    "        def asignar_etiqueta(row):\n",
    "            if row['count'] < cant_aceptab_hora:\n",
    "                chunk.loc[chunk['Fecha'].dt.floor('h') == row['Fecha'].floor('h'), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        # Evaluar por cada grupo de datos por hora y asignar la etiqueta\n",
    "        canthora = chunk.groupby(chunk['Fecha'].dt.floor('h')).size().reset_index(name='count')\n",
    "        canthora.apply(asignar_etiqueta, axis=1)\n",
    "        \n",
    "        # Definir función para asignar etiquetas de acumulado diario\n",
    "        def asignar_etiqueta_diaria(row):\n",
    "            if row['count'] < cant_aceptab_dia:\n",
    "                chunk.loc[chunk['Fecha'].dt.floor('D') == row['Fecha'].floor('D'), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        # Evaluar por cada grupo de datos por día y asignar la etiqueta\n",
    "        cantdia = chunk.groupby(chunk['Fecha'].dt.floor('D')).size().reset_index(name='count')\n",
    "        cantdia.apply(asignar_etiqueta_diaria, axis=1)\n",
    "        \n",
    "        return chunk\n",
    "        \n",
    "    @log_failures\n",
    "    def p_estruct(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los datos fueron transmitidos en horas y minutos exactos al ser el\n",
    "        comportamiento esperado'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "\n",
    "        freqinst100b = pd.read_csv('EMAPatm_allinfo.csv', sep=';', encoding='latin-1') # Antiguo 'freq10binst_manualcorrect.csv'\n",
    "        \n",
    "        # Define un diccionario de frecuencias y cantidades esperadas\n",
    "        frecuencias = {\n",
    "            '2min': {'num_para_modulo': 2},\n",
    "            '10min': {'num_para_modulo': 10}\n",
    "        }\n",
    "        \n",
    "        # Obtener la frecuencia de 'freqinst100b' basado en 'Station' y asignar a 'periodos'\n",
    "        station_value = chunk['Station'].values[0]\n",
    "        if pd.isna(station_value):\n",
    "            periodos = None\n",
    "        else:\n",
    "            freqinst100b_station = freqinst100b.loc[freqinst100b['Station'] == station_value]\n",
    "            if freqinst100b_station.empty:\n",
    "                print(f\"No se encontró la estación {station_value} en freqinst100b\")\n",
    "                return chunk\n",
    "            freq_inf_value = freqinst100b_station['FreqInf'].values[0]\n",
    "    \n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    print(periodos)\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return chunk\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return chunk\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        # Se hace frente al caso de no encontrar la estación\n",
    "        if periodos is not None:\n",
    "            \n",
    "            # Generar la operación para observar si la estructura es exacta en minutos\n",
    "            fecha = chunk['Fecha']\n",
    "    \n",
    "            # Se vectoriza la evaluación de la estructura por minuto\n",
    "            # Para cada chunk:\n",
    "            if periodos == 'min':\n",
    "                mask = fecha.dt.second != 0\n",
    "            elif periodos == 'h':\n",
    "                mask = (fecha.dt.minute != 0) | (fecha.dt.second != 0)\n",
    "            else:\n",
    "                # Se obtiene num_para_modulo\n",
    "                num_para_modulo = frecuencias[periodos]['num_para_modulo']\n",
    "                mask = fecha.dt.minute % num_para_modulo != 0\n",
    "\n",
    "            chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "            # Cambiar '0PSO0' a '0PSO1' donde la máscara se cumple\n",
    "            chunk.loc[mask & (chunk['Estado'] == '0PSO0'), 'Estado'] = '0PSO1'\n",
    "            # Asignar '0PSO0' a los NaN donde la máscara se cumple\n",
    "            chunk.loc[mask & chunk['Estado'].isnull(), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        else:  # Si el periodo es None, no se hace ninguna modificación al chunk, pero puedes imprimir un mensaje si quieres\n",
    "            print(f\"No se encontró la frecuencia de la estación {station_value} ni un proyecto correspondiente en freqinst100b\")\n",
    "            \n",
    "        return chunk, mask\n",
    "\n",
    "    @log_failures\n",
    "    def p_limrigidos(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los datos crudos se encuentran fuera del umbral físico inferior o superior'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna de estado anterior\n",
    "        chunk['Estado_Anterior'] = ''\n",
    "        \n",
    "        # Se establecen los umbrales físicos/rígidos a datos crudos en nuevas colummnas para vectorizar\n",
    "        chunk['umbr_crud_inf'] = 484.5\n",
    "        chunk['umbr_crud_sup'] = 1013.26\n",
    "\n",
    "        # Compara el dato con umbrales inferiores y superiores \n",
    "        mask_outbounds = (chunk['Valor'] < chunk['umbr_crud_inf']) | (chunk['Valor'] > chunk['umbr_crud_sup'])\n",
    "\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        chunk['Estado_Anterior'] = chunk['Estado_Anterior'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado_Anterior'\n",
    "        condicion_0PSO0 = mask_outbounds & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "        # Lógica de etiquetado para 'Estado'\n",
    "        condicion_0PER0 = mask_outbounds & (chunk['Estado'].isnull() | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        \n",
    "        # Se eliminan las columnas no deseadas\n",
    "        if 'umbr_crud_inf' in chunk.columns:\n",
    "            chunk.drop(columns=['umbr_crud_inf', 'umbr_crud_sup'], axis=1, inplace=True)\n",
    "                \n",
    "        return chunk, mask_outbounds\n",
    "\n",
    "    @log_failures\n",
    "    def p_limpsicrom(self, chunk, archivo):\n",
    "        '''Esta prueba calcula, con base en la ubicación de la estación y la estimación de tmax y tmin según las normales\n",
    "        climatológicas, la escala de altura y la presión con la fórmula psicrométrica para fijar límites superior e inferior\n",
    "        válidos; y reconocer datos atípicos en los conjuntos de datos'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Se establece la ecuación de interpolación lineal\n",
    "        def interp_lineal(x, x1, y1, x2, y2):\n",
    "            if x1 == x2:  # Para evitar división por cero\n",
    "                return y1\n",
    "            return y1 + ((y2 - y1) / (x2 - x1)) * (x - x1)\n",
    "        \n",
    "        # Se lee el archivo con las altitudes por estación y ## medias máximas y mínimas estimadas según las normales\n",
    "        EMN_TMaxMin = pd.read_csv('EMAPatm_allinfo.csv', sep=';', encoding='latin-1')\n",
    "        # Se lee el archivo con el gradiente de temperatura calculado\n",
    "        TempCalcGrad = pd.read_excel('TempCalcGrad.xlsx')\n",
    "\n",
    "        # Se establecen los valores de altura y código de la estación\n",
    "        Alt = EMN_TMaxMin['Altitud'].ravel()\n",
    "        Statncod = chunk['Station']\n",
    "\n",
    "        # Se itera sobre las altitudes y estación\n",
    "        for dem, s in zip(Alt, Statncod):\n",
    "            if s not in EMN_TMaxMin['Station'].values:\n",
    "                print(f'La estación {s} no se encuentra en EMN_TMaxMin.')\n",
    "                return chunk\n",
    "\n",
    "            AltSttn = EMN_TMaxMin[EMN_TMaxMin['Station'] == s]['Altitud'].values[0]\n",
    "            \n",
    "            if AltSttn == 0.0:\n",
    "                Talt = TempCalcGrad['TempCalcGradiente'].values[0]\n",
    "            else:\n",
    "                # Encontrar los puntos de interpolación\n",
    "                alturas = TempCalcGrad['Altura']\n",
    "                temp_grad = TempCalcGrad['TempCalcGradiente']\n",
    "    \n",
    "                # Verificar que la altura está dentro del rango de datos\n",
    "                if AltSttn < alturas.min() or AltSttn > alturas.max():\n",
    "                    print(f'La altura {AltSttn} de la estación {s} está fuera del rango de datos.')\n",
    "                    return chunk\n",
    "    \n",
    "                # Encontrar los puntos de interpolación más cercanos\n",
    "                alt_min = alturas[alturas <= AltSttn].max()\n",
    "                alt_max = alturas[alturas >= AltSttn].min()\n",
    "                temp_min = temp_grad[alturas == alt_min].values[0]\n",
    "                temp_max = temp_grad[alturas == alt_max].values[0]\n",
    "    \n",
    "                # Realizar la interpolación lineal\n",
    "                Talt = interp_lineal(AltSttn, alt_min, temp_min, alt_max, temp_max)\n",
    "            \n",
    "            # Se hacen los cálculos según fórmula psicrométrica\n",
    "            EAsttn = (8.3144598 * (273.15 + Talt)) / (9.80665 * 0.0289644)\n",
    "            Patmpsicrom = (1013.25 * np.exp(-(AltSttn / EAsttn)))\n",
    "\n",
    "            # Se calculan los límites según fórmula psicrométrica, se tienen en cuenta dos valores de aumento y disminución\n",
    "            chunk['LimSup_psicr_AT'] = (Patmpsicrom + 9.0) # Antes 8.0\n",
    "            chunk['LimInf_psicr_AT'] = (Patmpsicrom - 9.0)\n",
    "            chunk['LimSup_psicr_ER'] = (Patmpsicrom + 13.0) # Antes 10.0\n",
    "            chunk['LimInf_psicr_ER'] = (Patmpsicrom - 13.0)\n",
    "\n",
    "\n",
    "            chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "            chunk['Estado_Anterior'] = chunk['Estado_Anterior'].fillna('')\n",
    "            ## Comparación con umbrales inferiores y superiores, caso erróneos (fuera de +-10.0hPa) \n",
    "            mask_outbPsicrom_ER = (chunk['Valor'] < chunk['LimInf_psicr_ER']) | (chunk['Valor'] > chunk['LimSup_psicr_ER'])\n",
    "            # Lógica de etiquetado para 'Estado_Anterior'\n",
    "            condicion_0PSO0 = mask_outbPsicrom_ER & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "            chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "            # Lógica de etiquetado para 'Estado', '0PER0'\n",
    "            condicion_0PER0 = mask_outbPsicrom_ER & (chunk['Estado'].isnull() | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "            chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "            mask_outbPsicrom_ER = mask_outbPsicrom_ER & ~condicion_0PER0\n",
    "            # Condición 0PER1\n",
    "            condicion_0PER1 = mask_outbPsicrom_ER & (chunk['Estado'] == '0PER0')\n",
    "            chunk.loc[condicion_0PER1, 'Estado'] = '0PER1'\n",
    "\n",
    "            ## Comparación con umbrales inferiores y superiores, caso atípicos (fuera de +-8.0hPa e inf. a +-10.0hPa)\n",
    "            # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "            if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "                chunk_psic = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "            else:\n",
    "                # Si todos los valores son NaN, simplemente copia el chunk\n",
    "                chunk_psic = chunk.copy()\n",
    "            # Comparación\n",
    "            mask_outbPsicrom_AT = ((chunk_psic['Valor'] < chunk_psic['LimInf_psicr_AT']) & (chunk_psic['Valor'] > chunk_psic['LimInf_psicr_ER'])) | ((chunk_psic['Valor'] > chunk_psic['LimSup_psicr_AT']) & (chunk_psic['Valor'] < chunk_psic['LimSup_psicr_ER']))\n",
    "            \n",
    "            chunk_psic['Estado'] = chunk_psic['Estado'].fillna('')\n",
    "            chunk_psic['Estado_Anterior'] = chunk_psic['Estado_Anterior'].fillna('')\n",
    "            # Lógica de etiquetado para 'Estado_Anterior'\n",
    "            condicion_0PSO0 = mask_outbPsicrom_AT & chunk_psic['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "            chunk_psic.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_psic.loc[condicion_0PSO0, 'Estado']\n",
    "            # Lógica de etiquetado para 'Estado', '0PAT0'\n",
    "            condicion_0PAT0 = mask_outbPsicrom_AT & (chunk_psic['Estado'].isnull() | chunk_psic['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "            chunk_psic.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "\n",
    "            # Se copian los datos del filtro al chunk original\n",
    "            chunk.loc[chunk_psic.index] = chunk_psic\n",
    "            \n",
    "            # Se eliminan las columnas no deseadas\n",
    "            if 'LimSup_psicr_AT' in chunk.columns:\n",
    "                chunk.drop(columns=['LimSup_psicr_AT', 'LimInf_psicr_AT', 'LimSup_psicr_ER','LimInf_psicr_ER'], axis=1, inplace=True)\n",
    "                \n",
    "            return chunk, (mask_outbPsicrom_ER | mask_outbPsicrom_AT)\n",
    "\n",
    "    @log_failures\n",
    "    def p_persist(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los registros tienen datos persistentes, que podrían significar errores en el sensor'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "                \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "            \n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "            \n",
    "        # Crear máscaras para cada comparación de las 6 filas consecutivas\n",
    "        mask_1 = (chunk['Valor'] == chunk['Valor'].shift(1))\n",
    "        mask_2 = (chunk['Valor'] == chunk['Valor'].shift(2))\n",
    "        mask_3 = (chunk['Valor'] == chunk['Valor'].shift(3))\n",
    "        mask_4 = (chunk['Valor'] == chunk['Valor'].shift(4))\n",
    "        mask_5 = (chunk['Valor'] == chunk['Valor'].shift(5))\n",
    "        \n",
    "        # Combinar todas las máscaras para obtener la condición deseada\n",
    "        mask_pers3datos = mask_1 & mask_2 & mask_3 & mask_4 & mask_5\n",
    "\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        chunk['Estado_Anterior'] = chunk['Estado_Anterior'].fillna('')\n",
    "        # Etiquetado de valores, se inicia con el Estado Anterior\n",
    "        condicion_0PSO0 = mask_pers3datos & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado'\n",
    "        condicion_0PER0 = mask_pers3datos & (chunk['Estado'].isnull() | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        mask_pers3datos = mask_pers3datos & ~condicion_0PER0\n",
    "\n",
    "        condicion_0PER1 = mask_pers3datos & (chunk['Estado'] == '0PER0')\n",
    "        chunk.loc[condicion_0PER1, 'Estado'] = '0PER1'\n",
    "        mask_pers3datos = mask_pers3datos & ~condicion_0PER1\n",
    "\n",
    "        condicion_0PER2 = mask_pers3datos & (chunk['Estado'] == '0PER1')\n",
    "        chunk.loc[condicion_0PER2, 'Estado'] = '0PER2'\n",
    "        \n",
    "        self.last_rows = chunk.tail(3)\n",
    "        \n",
    "        return chunk, mask_pers3datos\n",
    "\n",
    "    @log_failures\n",
    "    def p_salto(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si la variación entre valores consecutivos excede 3.0 hPa'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "                \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "            \n",
    "        # Se toma nuevamente el archivo de frecuencias para analizar datos estrictamente consecutivos\n",
    "        freqinst100b = pd.read_csv('EMAPatm_allinfo.csv', encoding='latin-1', sep=';')\n",
    "\n",
    "        # Obtener la frecuencia de 'freqinst100b' basado en 'Station' y asignar a 'periodos'\n",
    "        sttn_code = chunk['Station'].values[0]\n",
    "        if pd.isna(sttn_code):\n",
    "            periodos = None\n",
    "        else:\n",
    "            freqinst100b_station = freqinst100b.loc[freqinst100b['Station'] == sttn_code]\n",
    "            if freqinst100b_station.empty:\n",
    "                print(f\"No se encontró la estación {sttn_code} en freqinst100b\")\n",
    "                return chunk\n",
    "    \n",
    "            freq_inf_value = freqinst100b_station['FreqInf'].values[0]\n",
    "    \n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    print(periodos)\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return chunk\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return chunk\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return chunk\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "        \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk['Fecha_anterior'] = chunk['Fecha'].shift(1)\n",
    "        chunk['Delta_tiempo'] = chunk['Fecha'] - chunk['Fecha_anterior']\n",
    "        \n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
    "        \n",
    "        # Calcular la diferencia absoluta entre los valores consecutivos\n",
    "        chunk['Delta'] = chunk['Valor'].diff().abs()\n",
    "        chunk['Delta'] = chunk['Delta'].where(mask_consecutivo)\n",
    "\n",
    "        # Máscara para identificar variaciones mayores a 3.0\n",
    "        mask_variacion = chunk['Delta'] > 3.0\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_jmp = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, simplemente copia el chunk\n",
    "            chunk_jmp = chunk.copy()\n",
    "\n",
    "        chunk_jmp['Estado'] = chunk_jmp['Estado'].fillna('')\n",
    "        chunk_jmp['Estado_Anterior'] = chunk_jmp['Estado_Anterior'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado_Anterior'\n",
    "        condicion_0PSO0 = mask_variacion & chunk_jmp['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_jmp.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_jmp.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos y para prueba de otra forma\n",
    "        condicion_0PAT0 = mask_variacion & (chunk_jmp['Estado'].isnull() | chunk_jmp['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk_jmp.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "        mask_variacion = mask_variacion & ~condicion_0PAT0\n",
    "        # 0PAT1\n",
    "        condicion_0PAT1 = mask_variacion & (chunk_jmp['Estado'] == '0PAT0')\n",
    "        chunk_jmp.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "\n",
    "        # Se copian los datos del filtro al chunk original\n",
    "        chunk.loc[chunk_jmp.index] = chunk_jmp\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        chunk.drop(columns=['Delta', 'Fecha_anterior', 'Delta_tiempo'], axis=1, inplace=True)\n",
    "\n",
    "        return chunk, mask_variacion\n",
    "    \n",
    "    @log_failures\n",
    "    def p_limclimsigma(self, chunk, archivo):\n",
    "        '''Esta prueba calcula, con los datos no etiquetados en la anterior prueba, la 3sigmas +- la media para detectar\n",
    "        datos atípicos en los conjuntos de los datos'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_sgm = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, simplemente copia el chunk\n",
    "            chunk_sgm = chunk.copy()\n",
    "\n",
    "        # Se calculan los estadísticos para sigma\n",
    "        mean = chunk_sgm['Valor'].mean()\n",
    "        std = chunk_sgm['Valor'].std()\n",
    "        # Con ellos, se establecen los límites superior e inferior\n",
    "        chunk_sgm['LimSup_Sigma'] = (mean + (4 * std))\n",
    "        chunk_sgm['LimInf_Sigma'] = (mean - (4 * std))\n",
    "\n",
    "        # Se etiquetan los valores que sobrepasen el límite\n",
    "        mask_outbsigma = (chunk_sgm['Valor'] < chunk_sgm['LimInf_Sigma']) | (chunk_sgm['Valor'] > chunk_sgm['LimSup_Sigma'])\n",
    "\n",
    "        chunk_sgm['Estado'] = chunk_sgm['Estado'].fillna('')\n",
    "        chunk_sgm['Estado_Anterior'] = chunk_sgm['Estado_Anterior'].fillna('')\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = mask_outbsigma & chunk_sgm['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_sgm.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_sgm.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos\n",
    "        condicion_0PAT0 = mask_outbsigma & (chunk_sgm['Estado'].isnull() | chunk_sgm['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk_sgm.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "        mask_outbsigma = mask_outbsigma & ~condicion_0PAT0\n",
    "        # 0PAT1\n",
    "        condicion_0PAT1 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT0')\n",
    "        chunk_sgm.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "        mask_outbsigma = mask_outbsigma & ~condicion_0PAT1\n",
    "        # 0PAT2\n",
    "        condicion_0PAT2 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT1')\n",
    "        chunk_sgm.loc[condicion_0PAT2, 'Estado'] = '0PAT2'\n",
    "                    \n",
    "        # Se eliminan las columnas no deseadas\n",
    "        if 'LimSup_Sigma' in chunk.columns:\n",
    "            chunk.drop(columns=['LimSup_Sigma', 'LimInf_Sigma'], axis=1, inplace=True)\n",
    "    \n",
    "        chunk.loc[chunk_sgm.index] = chunk_sgm\n",
    "        return chunk, mask_outbsigma\n",
    "\n",
    "    @log_failures\n",
    "    def p_horavmaxmin(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los datos que son máximos y mínimos en horarios distintos a los conocidos en cada periodo semidiurno\n",
    "        y los marca como atípicos'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        chunk['Estado_Anterior'] = chunk['Estado_Anterior'].fillna('')\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_hmm = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, simplemente copia el chunk\n",
    "            chunk_hmm = chunk.copy()\n",
    "            \n",
    "        # Se crean máscaras para los intervalos de tiempo conocidos para valores máximos y mínimos\n",
    "        mask_max_morning = (chunk_hmm['Fecha'].dt.hour >= 8) & (chunk_hmm['Fecha'].dt.hour < 13) | (chunk_hmm['Fecha'].dt.hour == 12)\n",
    "        mask_max_evening = (chunk_hmm['Fecha'].dt.hour >= 20) & (chunk_hmm['Fecha'].dt.hour <= 23) | (chunk_hmm['Fecha'].dt.hour == 1)\n",
    "        mask_min_morning = (chunk_hmm['Fecha'].dt.hour >= 2) & (chunk_hmm['Fecha'].dt.hour < 7)\n",
    "        mask_min_afternoon = (chunk_hmm['Fecha'].dt.hour > 14) & (chunk_hmm['Fecha'].dt.hour < 19)\n",
    "        \n",
    "        # Se filtran los datos para esas horas\n",
    "        max_validdata = chunk_hmm[mask_max_morning | mask_max_evening]\n",
    "        min_validdata = chunk_hmm[mask_min_morning | mask_min_afternoon]\n",
    "\n",
    "        # Encontrar los dos valores máximos por día\n",
    "        max_values = chunk_hmm.groupby(chunk_hmm['Fecha'].dt.date).apply(lambda x: x.nlargest(2, 'Valor')).reset_index(level=0, drop=True)\n",
    "        \n",
    "        # Encontrar los dos valores mínimos por día\n",
    "        min_values = chunk_hmm.groupby(chunk_hmm['Fecha'].dt.date).apply(lambda x: x.nsmallest(2, 'Valor')).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Verificar los máximos y mínimos y obtener las horas correspondientes\n",
    "        notvalid_max_values = max_values[~max_values.index.isin(max_validdata.index)]\n",
    "        notvalid_min_values = min_values[~min_values.index.isin(min_validdata.index)]\n",
    "\n",
    "        # Combinar los valores no válidos en un solo DataFrame\n",
    "        notvalid_values = pd.concat([notvalid_max_values, notvalid_min_values])\n",
    "        # Crear una máscara para identificar los índices de los valores no válidos\n",
    "        notval_maxmin = chunk_hmm.index.isin(notvalid_values.index)\n",
    "\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = notval_maxmin & chunk_hmm['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_hmm.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_hmm.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        chunk_hmm['Estado'] = chunk_hmm['Estado'].fillna('')\n",
    "        chunk_hmm['Estado_Anterior'] = chunk_hmm['Estado_Anterior'].fillna('')\n",
    "        # Se etiquetan los atípicos\n",
    "        condicion_0PAT0 = notval_maxmin & (chunk_hmm['Estado'].isnull() | chunk_hmm['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk_hmm.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "        notval_maxmin = notval_maxmin & ~condicion_0PAT0\n",
    "        # 0PAT1\n",
    "        condicion_0PAT1 = notval_maxmin & (chunk_hmm['Estado'] == '0PAT0')\n",
    "        chunk_hmm.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "        notval_maxmin = notval_maxmin & ~condicion_0PAT1\n",
    "        # 0PAT2\n",
    "        condicion_0PAT2 = notval_maxmin & (chunk_hmm['Estado'] == '0PAT1')\n",
    "        chunk_hmm.loc[condicion_0PAT2, 'Estado'] = '0PAT2'\n",
    "        notval_maxmin = notval_maxmin & ~condicion_0PAT2\n",
    "        # 0PAT3\n",
    "        condicion_0PAT2 = notval_maxmin & (chunk_hmm['Estado'] == '0PAT2')\n",
    "        chunk_hmm.loc[condicion_0PAT2, 'Estado'] = '0PAT3'\n",
    "\n",
    "        chunk.loc[chunk_hmm.index] = chunk_hmm\n",
    "        return chunk, notval_maxmin\n",
    "\n",
    "    def llenado_0PCO0(self, chunk, archivo):\n",
    "        '''Esta función permite el llenado de los NaN de la columna 'Estado' como '0PCO0' al haber superado las pruebas'''\n",
    "        chunk.loc[chunk['Estado'].isnull(), 'Estado'] = '0PCO0'\n",
    "        return chunk\n",
    "       \n",
    "    def procesar_archivos(self, funcion_evaluacion):\n",
    "        '''Este método procesa la lectura y guardado de los archivos para todas las pruebas'''\n",
    "        archivos = self.ruta_archivos\n",
    "\n",
    "        archivos_salida = []  # Lista para almacenar nombres de archivos de salida\n",
    "\n",
    "        # Se recorre cada archivo en la carpeta\n",
    "        for archivo in archivos:\n",
    "            if archivo.endswith('.csv'):\n",
    "                ruta_archivo = os.path.join(self.dir_files, archivo)\n",
    "\n",
    "                reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=self.chunk_size)#,dtype={7: 'str'}, low_memory=False)\n",
    "                resultados = []\n",
    "\n",
    "                for chunk in reader:\n",
    "                    try:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                    except ValueError:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    chunk['Station'] = chunk['Station'].astype('int64')\n",
    "                    chunk_resultado = funcion_evaluacion(chunk, archivo)\n",
    "                    resultados.append(chunk_resultado)\n",
    "\n",
    "                if not resultados:  # Se verifica si la lista está vacía\n",
    "                    self.logger.warning('No hay resultados válidos para concatenar en el archivo %s. Continuando con el siguiente archivo.', archivo)\n",
    "                    #print(f\"No hay resultados válidos para concatenar en el archivo {archivo}. Continuando con el siguiente archivo.\")\n",
    "                    continue\n",
    "                    \n",
    "                resultados_consolidados = pd.concat(resultados)\n",
    "\n",
    "                # Genera el nombre del archivo de salida conservando los primeros 19 caracteres del nombre del archivo original\n",
    "                nombre_archivo_salida = archivo[:19] + '_qc.csv'\n",
    "\n",
    "                resultados_consolidados.to_csv(os.path.join(self.dir_files, nombre_archivo_salida), encoding='latin-1', index=False)\n",
    "\n",
    "                archivos_salida.append(nombre_archivo_salida)  # Agregar el nombre del archivo a la lista\n",
    "            \n",
    "        # Actualiza self.ruta_archivos para que la próxima prueba procese los resultados de esta prueba\n",
    "        self.ruta_archivos = archivos_salida\n",
    "        # Se fija el log de procesamiento completo de archivos\n",
    "        self.logger.info('Procesamiento completo de archivos de estaciones Patm. Archivos generados: %s', archivos_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58f92b9d-0c3a-4652-aa4b-f41a66ab8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador = AutomatPatmEMA('Test_QC') #RawUnmodified_Patm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d4348e0-435a-4f34-ad20-e1d5d8da9827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min\n",
      "h\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0048015040.csv\n",
      "min\n",
      "min\n",
      "min\n",
      "min\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0048015040.csv\n",
      "min\n",
      "min\n",
      "min\n",
      "min\n",
      "2min\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0057015010.csv\n",
      "h\n",
      "2min\n",
      "2min\n",
      "2min\n",
      "2min\n",
      "2min\n",
      "2min\n",
      "2min\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_transm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d2d470b-c286-4289-90e5-8d9148c65a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min\n",
      "h\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0048015040_qc.csv\n",
      "min\n",
      "min\n",
      "min\n",
      "min\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0048015040_qc.csv\n",
      "min\n",
      "min\n",
      "min\n",
      "min\n",
      "2min\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0057015010_qc.csv\n",
      "h\n",
      "2min\n",
      "2min\n",
      "2min\n",
      "2min\n",
      "2min\n",
      "2min\n",
      "2min\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_estruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c17071-31e8-4484-9ae9-7ce1af648709",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_limrigidos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "26356fca-6682-48f0-a3b2-24de4a4fd171",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:211: FutureWarning: Series.ravel is deprecated. The underlying array is already 1D, so ravel is not necessary.  Use `to_numpy()` for conversion to a numpy array instead.\n",
      "  Alt = EMN_TMaxMin['Altitud'].ravel()\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_limpsicrom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "85a3b507-942f-468c-b03a-03f6735e8bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1fdb9d8c-fe40-4349-b26e-2d9f56827a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min\n",
      "h\n",
      "h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:437: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_jmp.index] = chunk_jmp\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:437: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_jmp.index] = chunk_jmp\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min\n",
      "min\n",
      "min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:437: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_jmp.index] = chunk_jmp\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:437: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_jmp.index] = chunk_jmp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min\n",
      "min\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0048015040_qc.csv\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0048015040_qc.csv\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0048015040_qc.csv\n",
      "min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min\n",
      "None\n",
      "Frecuencia inferida es None para el archivo Estacion_0057015010_qc.csv\n",
      "h\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min\n",
      "2min\n",
      "2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min\n",
      "2min\n",
      "2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:406: FutureWarning: 'H' is deprecated and will be removed in a future version. Please use 'h' instead of 'H'.\n",
      "  mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta('1H')\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_salto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "890aa9e8-779a-4ea3-8167-4ce53af56f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:494: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_sgm.index] = chunk_sgm\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:494: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_sgm.index] = chunk_sgm\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:494: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_sgm.index] = chunk_sgm\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:494: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_sgm.index] = chunk_sgm\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_limclimsigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "db6f17e8-cb48-4eb0-83f5-9c90859b98b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:563: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_hmm.index] = chunk_hmm\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_39572\\1993923774.py:563: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '['' '' '' ... '' '' '']' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk_hmm.index] = chunk_hmm\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_horavmaxmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a4b8d2a-8da3-4629-8884-bdc299e12d47",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'AutomatPatmEMA' object has no attribute 'llenado_0PC0'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m procesador\u001b[38;5;241m.\u001b[39mprocesar_archivos(procesador\u001b[38;5;241m.\u001b[39mllenado_0PC0)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'AutomatPatmEMA' object has no attribute 'llenado_0PC0'"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.llenado_0PC0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2ef353-89cc-4567-9361-aebf0da4cece",
   "metadata": {},
   "source": [
    "## Alistamiento archivos Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eca923c-8bce-4dfb-b6d2-c1c6d3154fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se reemplaza con la carpeta de archivos del resultado de pruebas con columnas extra\n",
    "input_dir = '_______'\n",
    "\n",
    "# Se genera lista de todos los archivos CSV en la carpeta de salida de pruebas y que ya hayan sido corregidos \n",
    "# y por tanto inician con el prefijo \"cor\", si no es el caso, omitir 'and f.startswith('cor')'\n",
    "files = [f for f in os.listdir(input_dir) if f.endswith('.csv') and f.startswith('cor')]\n",
    "\n",
    "# Se itera por cada archivo\n",
    "for file in files:\n",
    "    toCass_filepath = os.path.join(input_dir, file)\n",
    "\n",
    "    # Se lee el archivo a transformar a formato adecuado para Cassandra\n",
    "    toCass_df = pd.read_csv(toCass_filepath, encoding='latin-1')\n",
    "    toCass_df['Fecha'] = pd.to_datetime(toCass_df['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    # Se eliminan las columnas no aptas para Cassandra que se conoce que están en todos los archivos\n",
    "    toCass_df.drop(columns=['Unnamed: 0','Name'], inplace=True)\n",
    "\n",
    "    # Se reemplazan sensores por los acordes en el glosario de variables + 'QC'\n",
    "    # Se desconoce\n",
    "    # Se guarda el archivo listo para Cassandra\n",
    "    toCassReady_filepath = os.path.join(input_dir, f\"cass_{file}\")\n",
    "    toCass_df.to_csv(toCassReady_filepath, index=False, encoding='latin-1')\n",
    "\n",
    "print(\"¡Proceso completado!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ee7c9c-a750-4c79-ba4e-a247b876f77c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7f0107-3b95-4cf6-88d2-e4c8284cadb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4ba6f6-64c8-4607-a2d9-2c26d5c5abba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab4f32-45d4-4680-aa2d-ba190c78affc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd7d2ae-41e6-408b-9ad9-fa5629917005",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762dea9f-a69b-4d42-a1e1-c9da20eb6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81053d2-0b17-4280-9773-5041e083b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V1 p_limpsicrom --fue reemplazada\n",
    "    def p_limpsicrom(self, chunk, archivo):\n",
    "        '''Esta prueba calcula, con base en la ubicación de la estación y la estimación de tmax y tmin según las normales\n",
    "        climatológicas, la escala de altura y la presión con la fórmula psicrométrica para fijar límites superior e inferior\n",
    "        válidos; y reconocer datos atípicos en los conjuntos de datos'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "        \n",
    "        # Se lee el archivo con las medias máximas y mínimas estimadas según las normales\n",
    "        EMN_TMaxMin = pd.read_table('EMA_Patm_TMaxMin.txt', sep=';', encoding='latin-1')\n",
    "\n",
    "        # Se establecen los valores de Tmax y Tmin\n",
    "        Tmax = EMN_TMaxMin['TmaxNorm'].ravel()\n",
    "        Tmin = EMN_TMaxMin['TminNorm'].ravel()\n",
    "        Alt = EMN_TMaxMin['Altitud'].ravel()\n",
    "        Statncod = chunk['Station']\n",
    "\n",
    "        # Se itera sobre los valores de temperatura, altitudes y estación debidos\n",
    "        for i,j,dem,s in zip(Tmax,Tmin,Alt,Statncod):\n",
    "            if s not in EMN_TMaxMin['Station'].values:\n",
    "                print(f'La estación {s} no se encuentra en EMN_TMaxMin.')\n",
    "                return chunk\n",
    "            \n",
    "            TmaxSttn = EMN_TMaxMin[EMN_TMaxMin['Station'] == s]['TmaxNorm'].values[0]\n",
    "            TminSttn = EMN_TMaxMin[EMN_TMaxMin['Station'] == s]['TminNorm'].values[0]\n",
    "            AltSttn = EMN_TMaxMin[EMN_TMaxMin['Station'] == s]['Altitud'].values[0]\n",
    "\n",
    "            # Se hacen los cálculos según fórmula psicrométrica\n",
    "            EAsttnSup = (8.3144598*(273.15+TmaxSttn))/(9.80665*0.0289644)\n",
    "            PatmpsicromSup = (1013.25*np.exp(-(AltSttn/EAsttnSup))) + 15.0\n",
    "            EAsttnInf = (8.3144598*(273.15+TminSttn))/(9.80665*0.0289644)\n",
    "            PatmpsicromInf = (1013.25*np.exp(-(AltSttn/EAsttnInf))) - 15.0\n",
    "\n",
    "            # Se establecen los umbrales según fórmula psicrométrica\n",
    "            chunk['LimSup_Sttn'] = PatmpsicromSup\n",
    "            chunk['LimInf_Sttn'] = PatmpsicromInf\n",
    "    \n",
    "            # Compara el dato con umbrales inferiores y superiores \n",
    "            mask_outbPsicrom = (chunk['Valor'] < chunk['LimInf_Sttn']) | (chunk['Valor'] > chunk['LimSup_Sttn'])\n",
    "            # Lógica de etiquetado para 'Estado_Anterior'\n",
    "            condicion_0PSO0 = mask_outbPsicrom & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "            chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "    \n",
    "            # Lógica de etiquetado para 'Estado'\n",
    "            condicion_0PER0 = mask_outbPsicrom & (chunk['Estado'].isnull() | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "            chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "            mask_outbPsicrom = mask_outbPsicrom & ~condicion_0PER0\n",
    "\n",
    "            condicion_0PER1 = mask_outbPsicrom & (chunk['Estado'] == '0PER0')\n",
    "            chunk.loc[condicion_0PER1, 'Estado'] = '0PER1'\n",
    "            \n",
    "            # Se eliminan las columnas no deseadas\n",
    "            if 'LimSup_Sttn' in chunk.columns:\n",
    "                chunk.drop(columns=['LimSup_Sttn', 'LimInf_Sttn'], axis=1, inplace=True)\n",
    "                \n",
    "            return chunk"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
