{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f2b16b-912e-4a9b-98c0-5a393e545013",
   "metadata": {},
   "source": [
    "# Pruebas automatizadas datos humedad relativa - Clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885f6f6",
   "metadata": {},
   "source": [
    "> Elaborado por Paola Álvarez, profesional contratista IDEAM, contrato 196 de 2024. Comentarios o inquietudes, remitir a *palvarez@ideam.gov.co* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ebabc-ed05-4660-a06e-7c242cb0ffeb",
   "metadata": {},
   "source": [
    "**Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28398810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from functools import wraps\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14c1a4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7267c",
   "metadata": {},
   "source": [
    "A continuación, se encuentran las pruebas de pre-validación de datos de EMA para verificar su capacidad de detección de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3c8ea",
   "metadata": {},
   "source": [
    "## Clase con métodos de aplicación de QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21dab107-ab7e-48ae-939b-7b0a6b24ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del logger para guardar en el directorio de archivos y sobrescribir cada vez\n",
    "def setup_logger(log_file_path):\n",
    "    logger = logging.getLogger('Test_QC')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='a')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ddb424af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatHREMA:\n",
    "    \n",
    "    def __init__(self, dir_files, chunk_size=540000):\n",
    "        self.dir_files = dir_files\n",
    "        self.ruta_archivos = os.listdir(dir_files)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.last_rows = None\n",
    "        self.current_file = None\n",
    "        # Sección configuración de logs\n",
    "        log_file_path = os.path.join(dir_files, 'QC_HR.log')\n",
    "        self.logger = setup_logger(log_file_path)\n",
    "        self.logger.info('Inicialización de PreValidPatmEMA en directorio: %s', dir_files)\n",
    "\n",
    "    def process_freqs(self, chunk, archivo):\n",
    "        '''Esta función procesa las frecuencias para abreviar variedad de métodos en adelante'''\n",
    "        # Convertir la columna de fecha a datetime si aún no lo es\n",
    "        if not pd.api.types.is_datetime64_any_dtype(chunk['Fecha']):\n",
    "            chunk['Fecha'] = pd.to_datetime(chunk['Fecha'])\n",
    "            \n",
    "        # Cargar el archivo de frecuencias\n",
    "        freqinst200b = pd.read_csv('EMAHR_Allinfo_Replcbl.csv', encoding='latin-1', sep=';')\n",
    "    \n",
    "        # Definir el diccionario de frecuencias y cantidades esperadas\n",
    "        frecuencias = {\n",
    "            'T': {'cant_esperd_h': 60, 'cant_esperd_d': 1440, 'cant_esperd_m': 43200, \n",
    "                  'cant_esperd_a': 518400, 'minutos': 1, 'shiftnum': 85, 'jumpnum':12},\n",
    "            '5T': {'cant_esperd_h': 12, 'cant_esperd_d': 288, 'cant_esperd_m': 8640, \n",
    "                   'cant_esperd_a': 103680, 'minutos': 5, 'shiftnum': 20, 'jumpnum':18},\n",
    "            '10T': {'cant_esperd_h': 6, 'cant_esperd_d': 144, 'cant_esperd_m': 4320, \n",
    "                    'cant_esperd_a': 51840, 'minutos': 10, 'shiftnum': 12, 'jumpnum':24},\n",
    "            'H': {'cant_esperd_h': 1, 'cant_esperd_d': 24, 'cant_esperd_m': 720, \n",
    "                  'cant_esperd_a': 8640, 'shiftnum': 6, 'jumpnum':32} ## Cambiar para adaptarse a directriz GGD\n",
    "        }\n",
    "    \n",
    "        # Obtener el valor de la estación\n",
    "        station_value = chunk['Station'].values[0]\n",
    "        if pd.isna(station_value):\n",
    "            print(f'La estación {station_value} no se encuentra en el análisis de frecuencias')\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "        else:\n",
    "            freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == station_value]\n",
    "            if freqinst200b_station.empty:\n",
    "                print(f\"No se encontró la estación {station_value} en freqinst200b\")\n",
    "                return {'periodos': None, 'frecuencias': None}\n",
    "            else:\n",
    "                freq_inf_value = freqinst200b_station['FreqInf'].values[0]\n",
    "    \n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    print(periodos)\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return {'periodos': None, 'frecuencias': None}\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return {'periodos': None, 'frecuencias': None}\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "    \n",
    "        if periodos in frecuencias:\n",
    "            return {'periodos': periodos, 'frecuencias': frecuencias[periodos]}\n",
    "        else:\n",
    "            print(f\"Periodo {periodos} no es reconocido en el diccionario de frecuencias\")\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "\n",
    "    def p_transm(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si existe al menos el 70% de datos esperados por día y hora\n",
    "        en la serie de datos; aquellos que no superen la prueba, son marcados como sospechosos'''\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "        \n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "\n",
    "        # Obtener las cantidades esperadas de acuerdo a la frecuencia\n",
    "        cant_esperd_h = frecuencias['cant_esperd_h']\n",
    "        cant_esperd_d = frecuencias['cant_esperd_d']\n",
    "    \n",
    "        # Se establecen los aceptables\n",
    "        cant_aceptab_hora = 0.7 * cant_esperd_h\n",
    "        cant_aceptab_dia = 0.7 * cant_esperd_d\n",
    "    \n",
    "        # Agregar columna de etiquetas al dataframe original\n",
    "        chunk['Estado'] = ''\n",
    "        \n",
    "        # Definir función para asignar etiquetas y llenar archivo log\n",
    "        def asignar_etiqueta(row):\n",
    "            if row['count'] < cant_aceptab_hora:\n",
    "                filas_fallidas = chunk.loc[chunk['Fecha'].dt.floor('h') == row['Fecha'].floor('h')]\n",
    "                chunk.loc[filas_fallidas.index, 'Estado'] = '0PSO0'\n",
    "                # Registrar en el log las filas que fallaron\n",
    "                for index, fila in filas_fallidas.iterrows():\n",
    "                    self.logger.info('File: %s - Row: %s - failed hour p_transm: %s', archivo, index, fila['Fecha'])\n",
    "\n",
    "        # Evaluar por cada grupo de datos por hora y asignar la etiqueta\n",
    "        canthora = chunk.groupby(chunk['Fecha'].dt.floor('h')).size().reset_index(name='count')\n",
    "        canthora.apply(asignar_etiqueta, axis=1)\n",
    "        \n",
    "        # Definir función para asignar etiquetas de acumulado diario y llenar archivo log\n",
    "        def asignar_etiqueta_diaria(row):\n",
    "            if row['count'] < cant_aceptab_dia:\n",
    "                filas_fallidas_dia = chunk.loc[chunk['Fecha'].dt.floor('D') == row['Fecha'].floor('D')]\n",
    "                chunk.loc[filas_fallidas_dia.index, 'Estado'] = '0PSO0'\n",
    "                # Registrar en el log las filas que fallaron\n",
    "                for index, fila in filas_fallidas_dia.iterrows():\n",
    "                    self.logger.info('File: %s - Row: %s - failed day p_transm: %s', archivo, index, fila['Fecha'])\n",
    "\n",
    "        # Evaluar por cada grupo de datos por día y asignar la etiqueta\n",
    "        cantdia = chunk.groupby(chunk['Fecha'].dt.floor('D')).size().reset_index(name='count')\n",
    "        cantdia.apply(asignar_etiqueta_diaria, axis=1)\n",
    "        \n",
    "        return chunk\n",
    "\n",
    "    def p_estruct(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los datos fueron transmitidos en horas y minutos exactos al ser el\n",
    "        comportamiento esperado'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        # Se verifica si 'freq_info' is None\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "        periodos =  freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "        # Se hace frente al caso de no encontrar la estación\n",
    "        # Generar la operación para observar si la estructura es exacta en minutos\n",
    "        fecha = chunk['Fecha']\n",
    "    \n",
    "        # Se vectoriza la evaluación de la estructura por minuto\n",
    "        # Para cada chunk:\n",
    "        if periodos == 'T':\n",
    "            mask_estr = fecha.dt.second != 0\n",
    "        elif periodos == 'H':\n",
    "            mask_estr = (fecha.dt.minute != 0) | (fecha.dt.second != 0)\n",
    "        else:\n",
    "            # Se obtiene num_para_modulo\n",
    "            num_para_modulo = frecuencias['minutos']\n",
    "            mask_estr = fecha.dt.minute % num_para_modulo != 0\n",
    "    \n",
    "        # Se registran los errores en el log\n",
    "        if mask_estr.any():\n",
    "           aligned_mask = mask_estr.reindex(chunk.index, fill_value=False)\n",
    "           for index, row in chunk[aligned_mask].iterrows():\n",
    "               self.logger.info('File: %s - Row: %s - Failed time p_estruct: %s', archivo, index, row['Fecha'])\n",
    "        else:\n",
    "           self.logger.info('File: %s - No se encontraron fallos en p_estruct', archivo)\n",
    "        \n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_estr & (chunk['Estado']=='')\n",
    "        chunk.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_estr = mask_estr & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_estr & (chunk['Estado'] == '0PSO0')\n",
    "        chunk.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "\n",
    "        return chunk\n",
    "\n",
    "    def p_limrig(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los datos crudos se encuentran fuera del umbral físico inferior o superior'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna de estado anterior\n",
    "        chunk['Estado_Anterior'] = ''\n",
    "        \n",
    "        # Se establecen los umbrales físicos/rígidos a datos crudos en nuevas colummnas para vectorizar\n",
    "        chunk['umbr_crud_inf'] = 5.0\n",
    "        chunk['umbr_crud_sup'] = 100.0\n",
    "\n",
    "        # Compara el dato con umbrales inferiores y superiores \n",
    "        mask_outbounds = (chunk['Valor'] < chunk['umbr_crud_inf']) | (chunk['Valor'] > chunk['umbr_crud_sup'])\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_outbounds.any():\n",
    "            aligned_mask_lr = mask_outbounds.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask_lr].iterrows():\n",
    "                self.logger.info('File: %s - Row: %s - Failed value p_limrig: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_limrig', archivo)\n",
    "\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado_Anterior'\n",
    "        condicion_0PSO0 = mask_outbounds & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "        # Lógica de etiquetado para 'Estado'\n",
    "        condicion_0PER0 = mask_outbounds & ((chunk['Estado']=='') | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        \n",
    "        # Se eliminan las columnas no deseadas\n",
    "        if 'umbr_crud_inf' in chunk.columns:\n",
    "            chunk.drop(columns=['umbr_crud_inf', 'umbr_crud_sup'], axis=1, inplace=True)\n",
    "                \n",
    "        return chunk\n",
    "\n",
    "    def p_persist(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los datos que se repiten por más de cuatro horas consecutivas'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''    \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "            \n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "\n",
    "        # Verificar y eliminar duplicados en el índice después de la concatenación\n",
    "        if not chunk.index.is_unique:\n",
    "            chunk = chunk.reset_index(drop=True)  # Crear un nuevo índice único\n",
    "\n",
    "        # Se crean máscaras para el intervalo del día con radiación solar que puede afectar la humedad\n",
    "        mask_sunny = (chunk['Fecha'].dt.hour >= 3) & (chunk['Fecha'].dt.hour <= 20)\n",
    "        # Se filtran los datos para esas horas\n",
    "        mask_sun = chunk[mask_sunny]\n",
    "\n",
    "        ## Se manejan las distintas frecuencias para verificar adecuadamente las persistencias\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        # Se verifica si 'freq_info' is None\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "        # Se accede a las claves del diccionario\n",
    "        periodos =  freq_info['periodos']\n",
    "        # Se accede a los datos del diccionario\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "        # Se asignan los shiftnums\n",
    "        cantshifts = frecuencias['shiftnum']\n",
    "        \n",
    "        # Crear una lista de máscaras usando una lista por comprensión\n",
    "        masks = [(mask_sun['Valor'] == mask_sun['Valor'].shift(i)) for i in range(1, cantshifts + 1)]\n",
    "\n",
    "        # Combinar todas las máscaras usando reduce y operador &\n",
    "        from functools import reduce\n",
    "        mask_pers4datos = reduce(lambda x, y: x & y, masks)\n",
    "        \n",
    "        #chunk.reset_index(drop=True)\n",
    "        # Se registran los errores en el log\n",
    "        if mask_pers4datos.any():\n",
    "            aligned_mask_pers = mask_pers4datos.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask_pers].iterrows():\n",
    "                self.logger.info('File: %s - Row: %s - Failed value p_persist: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_persist', archivo)\n",
    "\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        # Etiquetado de valores, se inicia con el Estado Anterior\n",
    "        condicion_0PSO0 = mask_pers4datos & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado'\n",
    "        condicion_0PER0 = mask_pers4datos & ((chunk['Estado']=='') | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        mask_pers4datos = mask_pers4datos & ~condicion_0PER0\n",
    "\n",
    "        condicion_0PER1 = mask_pers4datos & (chunk['Estado'] == '0PER0')\n",
    "        chunk.loc[condicion_0PER1, 'Estado'] = '0PER1'\n",
    "\n",
    "        # Eliminar las primeras 'cantshifts' filas utilizadas para la evaluación\n",
    "        chunk = chunk.iloc[cantshifts:]\n",
    "\n",
    "        # Actualizar las filas de self.last_rows para el próximo chunk\n",
    "        self.last_rows = chunk.tail(cantshifts).copy()\n",
    "\n",
    "        return chunk\n",
    "\n",
    "    def p_salto(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si la variación entre valores consecutivos excede 45.0 %'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Se hace una instancia del método de 'process_freqs' para obtener las frecuencias\n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "\n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "        \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk['Fecha_anterior'] = chunk['Fecha'].shift(1)\n",
    "        chunk['Delta_tiempo'] = chunk['Fecha'] - chunk['Fecha_anterior']\n",
    "        \n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta(periodos)\n",
    "        \n",
    "        # Calcular la diferencia absoluta entre los valores consecutivos\n",
    "        chunk['Delta'] = chunk['Valor'].diff().abs()\n",
    "        chunk['Delta'] = chunk['Delta'].where(mask_consecutivo)\n",
    "\n",
    "        # Se determina número de salto\n",
    "        jumpnum = frecuencias['jumpnum']\n",
    "        \n",
    "        # Máscara para identificar variaciones mayores a 45.0\n",
    "        mask_variacion = chunk['Delta'] > jumpnum\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_variacion.any():\n",
    "            aligned_mask_salto = mask_variacion.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask_salto].iterrows():\n",
    "                self.logger.info('File: %s - Row: %s - Failed value p_salto: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_salto', archivo)\n",
    "      \n",
    "        # Etiquetado de valores, se inicia con el Estado Anterior\n",
    "        condicion_0PSO0 = mask_variacion & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "        \n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = mask_variacion & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos\n",
    "        condicion_0PAT0 = mask_variacion & ((chunk['Estado'] == '') | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        chunk.drop(columns=['Delta', 'Fecha_anterior', 'Delta_tiempo'], axis=1, inplace=True)\n",
    "\n",
    "        return chunk\n",
    "\n",
    "    def p_horavmaxmin(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los datos que son máximos y mínimos en horarios distintos a los posibles por la temperatura máxima en el\n",
    "        día según la radiación solar'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_hmm = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, se copia el chunk\n",
    "            chunk_hmm = chunk.copy()\n",
    "            \n",
    "        # Se crean máscaras para los intervalos de tiempo conocidos para valores máximos y mínimos\n",
    "        #mask_max_morning = (chunk_hmm['Fecha'].dt.hour >= 0) & (chunk_hmm['Fecha'].dt.hour < 9) \n",
    "        #mask_max_afternoon = (chunk_hmm['Fecha'].dt.hour > 17) & (chunk_hmm['Fecha'].dt.hour <= 23)\n",
    "        mask_min_afternoon = (chunk_hmm['Fecha'].dt.hour >= 9) & (chunk_hmm['Fecha'].dt.hour <= 17)\n",
    "        \n",
    "        # Se filtran los datos para esas horas\n",
    "        #max_validdata = chunk_hmm[mask_max_morning | mask_max_afternoon]\n",
    "        min_validdata = chunk_hmm[mask_min_afternoon]\n",
    "\n",
    "        # Encontrar dos valores máximos por día\n",
    "        #max_values = chunk_hmm.groupby(chunk_hmm['Fecha'].dt.date).apply(lambda x: x.nlargest(2, 'Valor')).reset_index(level=0, drop=True)\n",
    "        \n",
    "        # Encontrar dos valores mínimos por día\n",
    "        min_values = chunk_hmm.groupby(chunk_hmm['Fecha'].dt.date).apply(lambda x: x.nsmallest(1, 'Valor')).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Verificar los máximos y mínimos y obtener las horas correspondientes\n",
    "        #notvalid_max_values = max_values[~max_values.index.isin(max_validdata.index)]\n",
    "        notvalid_min_values = min_values[~min_values.index.isin(min_validdata.index)]\n",
    "\n",
    "        # Combinar los valores no válidos en un solo DataFrame\n",
    "        #notvalid_values = pd.concat([notvalid_max_values, notvalid_min_values])\n",
    "        #print(notvalid_values)\n",
    "        # Crear una máscara para identificar los índices de los valores no válidos\n",
    "        notval_min = chunk_hmm.index.isin(notvalid_min_values.index) #notval_maxmin = chunk_hmm.index.isin(notvalid_values.index)\n",
    "        #print(notval_maxmin)\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        #if notval_maxmin.any():\n",
    "        #    aligned_mask_hmaxmin = notvalid_values.reindex(chunk.index, fill_value=False)\n",
    "        #    for index, row in chunk[aligned_mask_hmaxmin].iterrows():\n",
    "        #        self.logger.info('File: %s - Row: %s - Failed value p_horavmaxmin: %s', archivo, index, row['Valor'])\n",
    "        if notval_min.any():#if notval_maxmin.any():\n",
    "            for index, row in chunk[notval_min].iterrows():#chunk[notval_maxmin].iterrows():\n",
    "                self.logger.info('File: %s - Row: %s - Failed value p_horavmaxmin: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_horavmaxmin', archivo)\n",
    "        \n",
    "        chunk_hmm['Estado'] = chunk_hmm['Estado'].fillna('')\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = notval_min & chunk_hmm['Estado'].isin(['0PSO0', '0PSO1']) #notval_maxmin\n",
    "        chunk_hmm.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_hmm.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos\n",
    "        condicion_0PAT0 = notval_min & ((chunk_hmm['Estado'] == '') | chunk_hmm['Estado'].isin(['0PSO0', '0PSO1'])) #notval_maxmin\n",
    "        chunk_hmm.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "        notval_min = notval_min & ~condicion_0PAT0 #notval_maxmin\n",
    "        \n",
    "        condicion_0PAT1 = notval_min & (chunk_hmm['Estado'] == '0PAT0') #notval_maxmin\n",
    "        chunk_hmm.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "\n",
    "        # Se copia al chunk original\n",
    "        chunk.loc[chunk_hmm.index] = chunk_hmm\n",
    "        return chunk\n",
    "\n",
    "    def p_sigma(self, chunk, archivo):\n",
    "        '''Esta prueba calcula, con los datos no etiquetados en la anterior prueba, la 4sigmas +- la media para detectar\n",
    "        datos atípicos en los conjuntos de los datos'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_sgm = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, simplemente copia el chunk\n",
    "            chunk_sgm = chunk.copy()\n",
    "\n",
    "        # Se calculan los estadísticos para sigma\n",
    "        mean = chunk_sgm['Valor'].mean()\n",
    "        std = chunk_sgm['Valor'].std()\n",
    "        # Con ellos, se establecen los límites superior e inferior\n",
    "        chunk_sgm['LimSup_Sigma'] = (mean + (4 * std))\n",
    "        chunk_sgm['LimInf_Sigma'] = (mean - (4 * std))\n",
    "\n",
    "        # Se etiquetan los valores que sobrepasen el límite\n",
    "        mask_outbsigma = (chunk_sgm['Valor'] < chunk_sgm['LimInf_Sigma']) | (chunk_sgm['Valor'] > chunk_sgm['LimSup_Sigma'])\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_outbsigma.any():\n",
    "            aligned_mask_sigma = mask_outbsigma.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask_sigma].iterrows():\n",
    "                self.logger.info('File: %s - Row: %s - Failed value p_sigma: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_sigma', archivo)\n",
    "        \n",
    "        chunk_sgm['Estado'] = chunk_sgm['Estado'].fillna('')\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = mask_outbsigma & chunk_sgm['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_sgm.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_sgm.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos - '0PAT0'\n",
    "        condicion_0PAT0 = mask_outbsigma & ((chunk_hmm['Estado'] == '') | chunk_sgm['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk_sgm.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "        mask_outbsigma = mask_outbsigma & ~condicion_0PAT0\n",
    "        # 0PAT1\n",
    "        condicion_0PAT1 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT0')\n",
    "        chunk_sgm.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "        mask_outbsigma = mask_outbsigma & ~condicion_0PAT1\n",
    "        # 0PAT2\n",
    "        condicion_0PAT2 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT1')\n",
    "        chunk_sgm.loc[condicion_0PAT2, 'Estado'] = '0PAT2'\n",
    "                    \n",
    "        # Se eliminan las columnas no deseadas\n",
    "        if 'LimSup_Sigma' in chunk.columns:\n",
    "            chunk.drop(columns=['LimSup_Sigma', 'LimInf_Sigma'], axis=1, inplace=True)\n",
    "    \n",
    "        chunk.loc[chunk_sgm.index] = chunk_sgm\n",
    "        return chunk\n",
    "\n",
    "    def p_coherPFvals(self, chunk, archivo):\n",
    "        '''Esta prueba verifica que un valor tenga coherencia con los 5 valores anteriores y los 5 posteriores\n",
    "        según su desviación estándar y media'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''    \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "            \n",
    "        ## Trabajo con frecuencias\n",
    "        # Se hace una instancia del método de 'process_freqs' para obtener las frecuencias\n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "\n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "\n",
    "        ## Se asegura la revisión de 5 datos anteriores aún si cambia el chunk\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "    \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "\n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk['Fecha_anterior'] = chunk['Fecha'].shift(1)\n",
    "        chunk['Delta_tiempo'] = chunk['Fecha'] - chunk['Fecha_anterior']\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        if 'Estado' not in chunk.columns or (chunk['Estado'] == '').all():\n",
    "            chunk_PFvals = chunk.copy()\n",
    "        else:\n",
    "            chunk_PFvals = chunk[~chunk['Estado'].str.startswith(('0PER','0PAT'), na=False)].copy()\n",
    "\n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk_PFvals['Delta_tiempo'] == pd.to_timedelta(periodos)\n",
    "        chunk_PFvals['consec_group'] = (~mask_consecutivo).cumsum()\n",
    "\n",
    "        # Se establecen diferentes ventanas según frecuencias\n",
    "        windows = {'T': {'window': 240}, '5T': {'window': 72}, '10T': {'window': 48}, 'H': {'window': 11}}\n",
    "        window_size = windows[periodos]['window']\n",
    "        half_window = window_size // 2\n",
    "\n",
    "        # Filtrar grupos que tienen al menos el tamaño de ventana necesario\n",
    "        group_counts = chunk_PFvals['consec_group'].value_counts()\n",
    "        valid_groups = group_counts[group_counts >= window_size].index\n",
    "        chnk_cohPFvl = chunk_PFvals[chunk_PFvals['consec_group'].isin(valid_groups)]\n",
    "\n",
    "        # Verificar que los datos anteriores y posteriores sean consecutivos\n",
    "        valid_indices = []\n",
    "        for i in range(half_window, len(chnk_cohPFvl) - half_window):\n",
    "            if all(mask_consecutivo[i-half_window:i+half_window]):\n",
    "                valid_indices.append(chnk_cohPFvl.index[i])\n",
    "\n",
    "        chnk_cohPFvl = chnk_cohPFvl.loc[valid_indices]\n",
    "\n",
    "        # Calcular el promedio y desviación estándar de los registros anteriores y posteriores\n",
    "        chnk_cohPFvl['mean_PF'] = chnk_cohPFvl['Valor'].rolling(window=window_size, center=True).mean()\n",
    "        chnk_cohPFvl['std_PF'] = chnk_cohPFvl['Valor'].rolling(window=window_size, center=True).std()\n",
    "\n",
    "        # Calcular los límites superior e inferior\n",
    "        chnk_cohPFvl['lim_inf'] = chnk_cohPFvl['mean_PF'] - (3 * chnk_cohPFvl['std_PF'])\n",
    "        chnk_cohPFvl['lim_sup'] = chnk_cohPFvl['mean_PF'] + (3 * chnk_cohPFvl['std_PF'])\n",
    "\n",
    "        # Añadir mensajes de depuración para verificar los límites\n",
    "        print(chnk_cohPFvl[['Fecha', 'Valor', 'mean_PF', 'std_PF', 'lim_inf', 'lim_sup']].head(20))\n",
    "\n",
    "        # Máscara para identificar valores fuera de los límites\n",
    "        mask_varPF = (chnk_cohPFvl['Valor'] < chnk_cohPFvl['lim_inf']) | (chnk_cohPFvl['Valor'] > chnk_cohPFvl['lim_sup'])\n",
    "        \n",
    "        # Añadir mensajes de depuración para verificar los valores fuera de los límites\n",
    "        #print(chnk_cohPFvl[mask_var5prev][['Fecha', 'Valor', 'lim_inf', 'lim_sup']])\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_varPF.any():\n",
    "            aligned_mask_sigma = mask_outbsigma.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask_sigma].iterrows():\n",
    "                self.logger.info('File: %s - Row: %s - Failed value p_sigma: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_sigma', archivo)\n",
    "        \n",
    "        chnk_cohPFvl['Estado'] = chnk_cohPFvl['Estado'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_varPF & ((chnk_cohPFvl['Estado'] == ''))\n",
    "        chnk_cohPFvl.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_varPF = mask_varPF & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_varPF & (chnk_cohPFvl['Estado'] == '0PSO0')\n",
    "        chnk_cohPFvl.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "        mask_varPF = mask_varPF & ~condicion_0PSO1\n",
    "        # 0PSO2\n",
    "        condicion_0PSO2 = mask_varPF & (chnk_cohPFvl['Estado'] == '0PSO1')\n",
    "        chnk_cohPFvl.loc[condicion_0PSO2, 'Estado'] = '0PSO2'\n",
    "    \n",
    "        # Se asegura la verificación de los valores anteriores si hubo cambio de chunk\n",
    "        self.last_rows = chnk_cohPFvl.tail(5)\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        if 'Fecha_anterior' in chunk.columns:\n",
    "            chunk.drop(columns=['Fecha_anterior','Delta_tiempo'], axis=1, inplace=True)\n",
    "            \n",
    "        # Copiar datos de chunk_coher al chunk original\n",
    "        chunk.loc[chnk_cohPFvl.index] = chnk_cohPFvl\n",
    "        # Continuar eliminando filas\n",
    "        if 'mean_5' in chunk.columns:\n",
    "            chunk.drop(columns=['mean_5','std_5','lim_inf','lim_sup'], axis=1, inplace=True)\n",
    "        \n",
    "        return chunk\n",
    "\n",
    "    def p_varminh(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los valores horarios que no varían en 1.0 % durante la hora'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_Anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "        ## Trabajo con frecuencias\n",
    "        # Se hace una instancia del método de 'process_freqs' para obtener las frecuencias\n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "\n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "    \n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "    \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "    \n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos o atípicos\n",
    "        if 'Estado' in chunk.columns and chunk['Estado'].notna().all():\n",
    "            chunk_varhmn = chunk[~chunk['Estado'].str.startswith(('0PER', '0PAT'), na=False)].copy()\n",
    "        else:\n",
    "            chunk_varhmn = chunk.copy()\n",
    "    \n",
    "        # Crear una columna de diferencia temporal y otras columnas temporales\n",
    "        chunk_varhmn['Fecha_anterior'] = chunk_varhmn['Fecha'].shift(1)\n",
    "        chunk_varhmn['Delta_tiempo'] = chunk_varhmn['Fecha'] - chunk_varhmn['Fecha_anterior']\n",
    "    \n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        freq_map = {'H': '1H', 'T': '1T', '5T': '5T', '10T': '10T'}\n",
    "        expected_delta = pd.to_timedelta(freq_map.get(periodos, '1H'))\n",
    "        mask_consecut = chunk_varhmn['Delta_tiempo'] == expected_delta\n",
    "    \n",
    "        # Calcular la diferencia absoluta entre los valores consecutivos\n",
    "        chunk_varhmn['Delta'] = chunk_varhmn['Valor'].diff().abs()\n",
    "        chunk_varhmn['Delta'] = chunk_varhmn['Delta'].where(mask_consecut)\n",
    "    \n",
    "        # Aplicar la máscara de las horas soleadas después de crear las columnas temporales\n",
    "        mask_sunny2 = (chunk_varhmn['Fecha'].dt.hour >= 6) & (chunk_varhmn['Fecha'].dt.hour <= 18)\n",
    "        mask_sun2 = chunk_varhmn[mask_sunny2]\n",
    "    \n",
    "        if periodos == 'H':\n",
    "            # Máscara para identificar variaciones menores a 1.0\n",
    "            mask_varhmin = mask_sun2['Delta'] < 1.0\n",
    "        else:\n",
    "            # Agrupar por horas y verificar si alguna variación dentro de la hora excede 1.0\n",
    "            mask_sun2['Fecha_hora'] = mask_sun2['Fecha'].dt.floor('h')\n",
    "            hora_groups = mask_sun2.groupby('Fecha_hora')\n",
    "            mask_varhmin = hora_groups['Delta'].transform(lambda x: (x < 1.0).any())\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_varPF.any():\n",
    "            aligned_mask_sigma = mask_outbsigma.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask_sigma].iterrows():\n",
    "                self.logger.info('File: %s - Row: %s - Failed value p_sigma: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_sigma', archivo)\n",
    "        \n",
    "        mask_sun2['Estado'] = mask_sun2['Estado'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_varhmin & ((chnk_cohPFvl['Estado'] == ''))\n",
    "        mask_sun2.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_varhmin & (mask_sun2['Estado'] == '0PSO0')\n",
    "        mask_sun2.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO1\n",
    "        # 0PSO2\n",
    "        condicion_0PSO2 = mask_varhmin & (mask_sun2['Estado'] == '0PSO1')\n",
    "        mask_sun2.loc[condicion_0PSO2, 'Estado'] = '0PSO2'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO2\n",
    "        # 0PSO2\n",
    "        condicion_0PSO3 = mask_varhmin & (mask_sun2['Estado'] == '0PSO2')\n",
    "        mask_sun2.loc[condicion_0PSO3, 'Estado'] = '0PSO3'\n",
    "    \n",
    "        # Copiar datos de chunk_jmp al chunk original\n",
    "        chunk.loc[mask_sun2.index] = mask_sun2\n",
    "    \n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        columns_to_drop = ['Delta', 'Fecha_anterior', 'Delta_tiempo', 'Fecha_hora']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in chunk.columns]\n",
    "        chunk.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "        return chunk\n",
    "\n",
    "    def procesar_archivos(self, funcion_evaluacion):\n",
    "        '''Este método procesa la lectura y guardado de los archivos para todas las pruebas'''\n",
    "        archivos = self.ruta_archivos\n",
    "\n",
    "        archivos_salida = []  # Lista para almacenar nombres de archivos de salida\n",
    "\n",
    "        # Se recorre cada archivo en la carpeta\n",
    "        for archivo in archivos:\n",
    "            if archivo.endswith('.csv'):\n",
    "                ruta_archivo = os.path.join(self.dir_files, archivo)\n",
    "\n",
    "                reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=self.chunk_size)#,dtype={7: 'str'}, low_memory=False)\n",
    "                resultados = []\n",
    "\n",
    "                for chunk in reader:\n",
    "                    try:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                    except ValueError:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    chunk['Station'] = chunk['Station'].astype('int64')\n",
    "\n",
    "                    # try:\n",
    "                    #     chunk_resultado, _ = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    # except ValueError:\n",
    "                    chunk_resultado = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    resultados.append(chunk_resultado)\n",
    "\n",
    "                if not resultados:  # Se verifica si la lista está vacía\n",
    "                    self.logger.warning('No hay resultados válidos para concatenar en el archivo %s. Continuando con el siguiente.', archivo)\n",
    "                    continue\n",
    "                    \n",
    "                resultados_consolidados = pd.concat(resultados)\n",
    "\n",
    "                # Genera el nombre del archivo de salida conservando los primeros 19 caracteres del nombre del archivo original\n",
    "                nombre_archivo_salida = archivo[:19] + '_qc.csv'\n",
    "\n",
    "                resultados_consolidados.to_csv(os.path.join(self.dir_files, nombre_archivo_salida), encoding='latin-1', index=False)\n",
    "\n",
    "                archivos_salida.append(nombre_archivo_salida)  # Agregar el nombre del archivo a la lista\n",
    "            \n",
    "        # Actualiza self.ruta_archivos para que la próxima prueba procese los resultados de esta prueba\n",
    "        self.ruta_archivos = archivos_salida\n",
    "        # Se fija el log de procesamiento completo de archivos\n",
    "        self.logger.info('Procesamiento completo de archivos de estaciones HR. Archivos generados: %s', archivos_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "58f92b9d-0c3a-4652-aa4b-f41a66ab8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador = AutomatHREMA('RawUnmodified_HR') #Test_QC/H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9d4348e0-435a-4f34-ad20-e1d5d8da9827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_transm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d2d470b-c286-4289-90e5-8d9148c65a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_estruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7c17071-31e8-4484-9ae9-7ce1af648709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_limrig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea10b68-a8cb-4b67-b977-65d83029a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b8d2a-8da3-4629-8884-bdc299e12d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_salto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcc6e5-8bee-4f94-9d04-14d4dd4d97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_horavmaxmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca86b59-71b3-4471-87b3-150eb60eae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5423c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analizar presencia de '0PER0'\n",
    "# Cambia este directorio al lugar donde tengas tu carpeta 'ReadytoCassandraFiles'\n",
    "directorio = 'RawUnmodified_HR' #r'Test_QC/10T'\n",
    "# Lista para almacenar los nombres de los archivos que contienen '0PSO1' en la columna 'Estado'\n",
    "cantdatos_0PAT = []\n",
    "\n",
    "# Itera sobre cada archivo en el directorio\n",
    "for archivo in os.listdir(directorio):\n",
    "    if archivo.endswith('_qc.csv'):\n",
    "        # Construye la ruta completa al archivo\n",
    "        ruta_archivo = os.path.join(directorio, archivo)\n",
    "        # Especifica los tipos de dato para las columnas deseadas\n",
    "        tipos_de_dato = {'Estado': str, 'Estado_Anterior': str}\n",
    "        # Lee el archivo CSV en un DataFrame de pandas\n",
    "        df = pd.read_csv(ruta_archivo, encoding='latin-1',dtype=tipos_de_dato)\n",
    "        # Checa cuántos '0PER0' hay en la columna 'Estado'\n",
    "        count_0PAT0 = df['Estado'].value_counts().get('0PAT0', 0)\n",
    "        # Guarda el nombre del archivo y la cantidad de datos con '0PER0'\n",
    "        cantdatos_0PAT.append((archivo, count_0PAT0))\n",
    "\n",
    "# Imprime la cantidad de datos con '0PER0' en cada archivo\n",
    "#for archivo, count in cantdatos_0PER:\n",
    "    #print(f\"Archivo: {archivo}, Datos con '0PER0': {count}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los resultados\n",
    "df_resultados = pd.DataFrame(cantdatos_0PAT, columns=['Archivo', 'Cantidad_0PAT0'])\n",
    "# Guardar resultados\n",
    "prueba = 'hmaxhmin'\n",
    "df_resultados.to_csv(f'cant_{prueba}_6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3f1d7-4219-42d7-b574-e61723082392",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_coherPFvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1add052-3700-456b-b350-ffbb7610289f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_varminh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449b253-9dcb-4b22-896d-a898101a2cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffae449-bdf5-43d3-8a79-d968c7460a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "fd7f6b2a",
   "metadata": {},
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Crear datos de prueba\n",
    "def crear_datos_prueba(frecuencia, num_datos):\n",
    "    fechas = pd.date_range(start='2024-01-01', periods=num_datos, freq=frecuencia)\n",
    "    valores = np.random.normal(loc=100, scale=10, size=num_datos)\n",
    "    datos = {'Fecha': fechas, 'Valor': valores, 'Station': ['TestStation'] * num_datos}\n",
    "    df_prueba = pd.DataFrame(datos)\n",
    "    return df_prueba\n",
    "\n",
    "# Simular la clase y la función\n",
    "class Validador:\n",
    "    def __init__(self):\n",
    "        self.last_rows = None\n",
    "        self.current_file = None\n",
    "    \n",
    "    def p_coher5vals(self, chunk, archivo):\n",
    "        '''Esta prueba verifica que un valor tenga coherencia con los 5 anteriores y 5 posteriores según su desviación estándar y media'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None    \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "\n",
    "        ## Trabajo con frecuencias\n",
    "        # Asumimos un periodo fijo para simplificar\n",
    "        periodos = '5T'\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return chunk, None\n",
    "\n",
    "        ## Se asegura la revisión de 5 datos anteriores aún si cambia el chunk\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "\n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "\n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk['Fecha_anterior'] = chunk['Fecha'].shift(1)\n",
    "        chunk['Delta_tiempo'] = chunk['Fecha'] - chunk['Fecha_anterior']\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        if 'Estado' not in chunk.columns or (chunk['Estado'] == '').all():\n",
    "            chunk_5vals = chunk.copy()\n",
    "        else:\n",
    "            chunk_5vals = chunk[~chunk['Estado'].str.startswith(('0PER','0PAT'), na=False)].copy()\n",
    "\n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk_5vals['Delta_tiempo'] == pd.to_timedelta(periodos)\n",
    "        chunk_5vals['consec_group'] = (~mask_consecutivo).cumsum()\n",
    "\n",
    "        # Se establecen diferentes ventanas según frecuencias\n",
    "        windows = {'T': {'window': 240}, '5T': {'window': 72}, '10T': {'window': 48}, 'H': {'window': 11}}\n",
    "        window_size = windows[periodos]['window']\n",
    "        half_window = window_size // 2\n",
    "\n",
    "        # Filtrar grupos que tienen al menos el tamaño de ventana necesario\n",
    "        group_counts = chunk_5vals['consec_group'].value_counts()\n",
    "        valid_groups = group_counts[group_counts >= window_size].index\n",
    "        chnk_coh5vl = chunk_5vals[chunk_5vals['consec_group'].isin(valid_groups)]\n",
    "\n",
    "        # Verificar que los datos anteriores y posteriores sean consecutivos\n",
    "        valid_indices = []\n",
    "        for i in range(half_window, len(chnk_coh5vl) - half_window):\n",
    "            if all(mask_consecutivo[i-half_window:i+half_window]):\n",
    "                valid_indices.append(chnk_coh5vl.index[i])\n",
    "\n",
    "        chnk_coh5vl = chnk_coh5vl.loc[valid_indices]\n",
    "\n",
    "        # Calcular el promedio y desviación estándar de los registros anteriores y posteriores\n",
    "        chnk_coh5vl['mean_5'] = chnk_coh5vl['Valor'].rolling(window=window_size, center=True).mean()\n",
    "        chnk_coh5vl['std_5'] = chnk_coh5vl['Valor'].rolling(window=window_size, center=True).std()\n",
    "\n",
    "        # Calcular los límites superior e inferior\n",
    "        chnk_coh5vl['lim_inf'] = chnk_coh5vl['mean_5'] - (3 * chnk_coh5vl['std_5'])\n",
    "        chnk_coh5vl['lim_sup'] = chnk_coh5vl['mean_5'] + (3 * chnk_coh5vl['std_5'])\n",
    "\n",
    "        # Añadir mensajes de depuración para verificar los límites\n",
    "        print(chnk_coh5vl[['Fecha', 'Valor', 'mean_5', 'std_5', 'lim_inf', 'lim_sup']].head(20))\n",
    "\n",
    "        # Máscara para identificar valores fuera de los límites\n",
    "        mask_var5prev = (chnk_coh5vl['Valor'] < chnk_coh5vl['lim_inf']) | (chnk_coh5vl['Valor'] > chnk_coh5vl['lim_sup'])\n",
    "        \n",
    "        # Añadir mensajes de depuración para verificar los valores fuera de los límites\n",
    "        print(chnk_coh5vl[mask_var5prev][['Fecha', 'Valor', 'lim_inf', 'lim_sup']])\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_var5prev & (chnk_coh5vl['Estado'].isnull())\n",
    "        chnk_coh5vl.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_var5prev = mask_var5prev & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_var5prev & (chnk_coh5vl['Estado'] == '0PSO0')\n",
    "        chnk_coh5vl.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "        mask_var5prev = mask_var5prev & ~condicion_0PSO1\n",
    "        # 0PSO2\n",
    "        condicion_0PSO2 = mask_var5prev & (chnk_coh5vl['Estado'] == '0PSO1')\n",
    "        chnk_coh5vl.loc[condicion_0PSO2, 'Estado'] = '0PSO2'\n",
    "\n",
    "        # Se asegura la verificación de los valores anteriores si hubo cambio de chunk\n",
    "        self.last_rows = chnk_coh5vl.tail(5)\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        if 'Fecha_anterior' in chunk.columns:\n",
    "            chunk.drop(columns=['Fecha_anterior','Delta_tiempo'], axis=1, inplace=True)\n",
    "\n",
    "        # Copiar datos de chunk_coher al chunk original\n",
    "        chunk.loc[chnk_coh5vl.index] = chnk_coh5vl\n",
    "        # Continuar eliminando filas\n",
    "        if 'mean_5' in chunk.columns:\n",
    "            chunk.drop(columns=['mean_5','std_5','lim_inf','lim_sup'], axis=1, inplace=True)\n",
    "\n",
    "        return chunk, mask_var5prev\n",
    "\n",
    "# Probar la función\n",
    "def probar_p_coher5vals():\n",
    "    # Crear un conjunto de datos de prueba\n",
    "    df_prueba = crear_datos_prueba('5T', 8000)  # Cambia '5T' y el número de datos según sea necesario\n",
    "\n",
    "    # Crear instancia de la clase Validador\n",
    "    validador = Validador()\n",
    "    \n",
    "    # Introducir algunos valores fuera de los límites para probar el etiquetado\n",
    "    df_prueba.loc[10:15, 'Valor'] = 200  # Valores anómalos que deberían ser etiquetados\n",
    "\n",
    "    # Aplicar la función\n",
    "    resultado_chunk, mask_var5prev = validador.p_coher5vals(df_prueba, 'archivo_prueba.csv')\n",
    "\n",
    "    # Guardar el resultado en un CSV para revisión manual\n",
    "    resultado_chunk.to_csv('resultado_prueba.csv', index=False)\n",
    "    \n",
    "    # Verificar si el resultado parece correcto\n",
    "    print(\"Revisar el archivo 'resultado_prueba.csv' para verificar los resultados.\")\n",
    "\n",
    "# Ejecutar la prueba\n",
    "probar_p_coher5vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a62f93-ae2d-463a-b163-54b3ca3d3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d8a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade notebook"
   ]
  },
  {
   "cell_type": "raw",
   "id": "301767cd",
   "metadata": {},
   "source": [
    "### V1 - Procesar archivos:\n",
    "    def procesar_archivos(self, funcion_evaluacion):\n",
    "        '''Este método procesa la lectura y guardado de los archivos para todas las pruebas'''\n",
    "        archivos = self.ruta_archivos\n",
    "\n",
    "        archivos_salida = []  # Lista para almacenar nombres de archivos de salida\n",
    "\n",
    "        # Se recorre cada archivo en la carpeta\n",
    "        for archivo in archivos:\n",
    "            if archivo.endswith('.csv'):\n",
    "                ruta_archivo = os.path.join(self.dir_files, archivo)\n",
    "\n",
    "                reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=self.chunk_size)#,dtype={7: 'str'}, low_memory=False)\n",
    "                resultados = []\n",
    "\n",
    "                for chunk in reader:\n",
    "                    try:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                    except ValueError:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    chunk['Station'] = chunk['Station'].astype('int64')\n",
    "\n",
    "                    try:\n",
    "                        chunk_resultado, _ = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    except ValueError:\n",
    "                        chunk_resultado = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    resultados.append(chunk_resultado)\n",
    "\n",
    "                if not resultados:  # Se verifica si la lista está vacía\n",
    "                    self.logger.warning('No hay resultados válidos para concatenar en el archivo %s. Continuando con el siguiente.', archivo)\n",
    "                    continue\n",
    "                    \n",
    "                resultados_consolidados = pd.concat(resultados)\n",
    "\n",
    "                # Genera el nombre del archivo de salida conservando los primeros 19 caracteres del nombre del archivo original\n",
    "                nombre_archivo_salida = archivo[:19] + '_qc.csv'\n",
    "\n",
    "                resultados_consolidados.to_csv(os.path.join(self.dir_files, nombre_archivo_salida), encoding='latin-1', index=False)\n",
    "\n",
    "                archivos_salida.append(nombre_archivo_salida)  # Agregar el nombre del archivo a la lista\n",
    "            \n",
    "        # Actualiza self.ruta_archivos para que la próxima prueba procese los resultados de esta prueba\n",
    "        self.ruta_archivos = archivos_salida\n",
    "        # Se fija el log de procesamiento completo de archivos\n",
    "        self.logger.info('Procesamiento completo de archivos de estaciones HR. Archivos generados: %s', archivos_salida)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "df0a6c67",
   "metadata": {},
   "source": [
    "### V1 logging\n",
    "\n",
    "# Configuración del logger para guardar en el directorio de archivos y sobrescribir cada vez\n",
    "def setup_logger(log_file_path):\n",
    "    logger = logging.getLogger('Test_QC')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='a')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "def log_failures(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(self, chunk, archivo):\n",
    "        try:\n",
    "            result, mask = func(self, chunk, archivo)\n",
    "            if mask is not None:\n",
    "                try:\n",
    "                    aligned_mask = mask.reindex(chunk.index, fill_value=False)  # Asegura que la máscara esté alineada con el índice del DataFrame\n",
    "                except AttributeError:\n",
    "                    aligned_mask = mask  # Si no se puede reindexar, usa la máscara tal como está\n",
    "                for index, row in chunk[aligned_mask].iterrows():\n",
    "                    self.logger.info('Archivo: %s - Fila: %s - Valor fallido en %s: %s', archivo, index, func.__name__, row['Valor'])\n",
    "            return result\n",
    "        except ValueError as e:\n",
    "            self.logger.error('Error procesando el archivo %s: %s', archivo, str(e))\n",
    "            return chunk  # Devuelve una máscara falsa para manejar el error\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7e15c7b3",
   "metadata": {},
   "source": [
    "## V1 prueba de transmisión\n",
    "    \n",
    "    def p_transm(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si existe al menos el 70% de datos esperados por día y hora\n",
    "        en la serie de datos; aquellos que no superen la prueba, son marcados como sospechosos'''\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk,_\n",
    "        \n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "        \n",
    "        # Obtener las cantidades esperadas de acuerdo a la frecuencia\n",
    "        cant_esperd_h = frecuencias['cant_esperd_h']\n",
    "        cant_esperd_d = frecuencias['cant_esperd_d']\n",
    "    \n",
    "        # Se establecen los aceptables\n",
    "        cant_aceptab_hora = 0.7 * cant_esperd_h\n",
    "        cant_aceptab_dia = 0.7 * cant_esperd_d\n",
    "    \n",
    "        # Agregar columna de etiquetas al dataframe original\n",
    "        chunk['Estado'] = np.nan\n",
    "        \n",
    "        # Definir función para asignar etiquetas\n",
    "        def asignar_etiqueta(row):\n",
    "            if row['count'] < cant_aceptab_hora:\n",
    "                chunk.loc[chunk['Fecha'].dt.floor('H') == row['Fecha'].floor('H'), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        # Evaluar por cada grupo de datos por hora y asignar la etiqueta\n",
    "        canthora = chunk.groupby(chunk['Fecha'].dt.floor('H')).size().reset_index(name='count')\n",
    "        canthora.apply(asignar_etiqueta, axis=1)\n",
    "        \n",
    "        # Definir función para asignar etiquetas de acumulado diario\n",
    "        def asignar_etiqueta_diaria(row):\n",
    "            if row['count'] < cant_aceptab_dia:\n",
    "                chunk.loc[chunk['Fecha'].dt.floor('D') == row['Fecha'].floor('D'), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        # Evaluar por cada grupo de datos por día y asignar la etiqueta\n",
    "        cantdia = chunk.groupby(chunk['Fecha'].dt.floor('D')).size().reset_index(name='count')\n",
    "        cantdia.apply(asignar_etiqueta_diaria, axis=1)\n",
    "        \n",
    "        return chunk, _"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eebfaaa4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "f43c4dd0",
   "metadata": {},
   "source": [
    "result = pd.read_csv('Test_QC/Estacion_0088112901_qc.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "70cfc04a",
   "metadata": {},
   "source": [
    "result['Estado'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559f9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
