{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f2b16b-912e-4a9b-98c0-5a393e545013",
   "metadata": {},
   "source": [
    "# Pruebas automatizadas datos humedad relativa - Clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885f6f6",
   "metadata": {},
   "source": [
    "> Elaborado por Paola Álvarez, profesional contratista IDEAM, contrato 196 de 2024. Comentarios o inquietudes, remitir a *palvarez@ideam.gov.co* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ebabc-ed05-4660-a06e-7c242cb0ffeb",
   "metadata": {},
   "source": [
    "**Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28398810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from functools import wraps\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14c1a4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7267c",
   "metadata": {},
   "source": [
    "A continuación, se encuentran las pruebas de pre-validación de datos de EMA para verificar su capacidad de detección de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed2be3c",
   "metadata": {},
   "source": [
    "## Clase con métodos de aplicación de QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c9474e9-bcdb-4d60-a03d-7e72ebdf588e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del logger para guardar en el directorio de archivos y sobrescribir cada vez\n",
    "def setup_logger(log_file_path):\n",
    "    logger = logging.getLogger('Test_QC')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='a')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ddb424af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatHREMA:\n",
    "    \n",
    "    def __init__(self, dir_files, chunk_size=540000):\n",
    "        self.dir_files = dir_files\n",
    "        self.ruta_archivos = os.listdir(dir_files)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.last_rows = None\n",
    "        self.current_file = None\n",
    "        # Sección configuración de logs\n",
    "        log_file_path = os.path.join(dir_files, 'QC_HR.log')\n",
    "        self.logger = setup_logger(log_file_path)\n",
    "        self.logger.info('Inicialización de PreValidPatmEMA en directorio: %s', dir_files)\n",
    "\n",
    "    def process_freqs(self, chunk, archivo):\n",
    "        '''Esta función procesa las frecuencias para abreviar variedad de métodos en adelante'''\n",
    "        # Convertir la columna de fecha a datetime si aún no lo es\n",
    "        if not pd.api.types.is_datetime64_any_dtype(chunk['Fecha']):\n",
    "            chunk['Fecha'] = pd.to_datetime(chunk['Fecha'])\n",
    "            \n",
    "        # Cargar el archivo de frecuencias\n",
    "        freqinst200b = pd.read_csv('EMAHR_Allinfo_Replcbl.csv', encoding='latin-1', sep=';')\n",
    "    \n",
    "        # Definir el diccionario de frecuencias y cantidades esperadas\n",
    "        frecuencias = {\n",
    "            'T': {'cant_esperd_h': 60, 'cant_esperd_d': 1440, 'cant_esperd_m': 43200, \n",
    "                  'cant_esperd_a': 518400, 'minutos': 1, 'shiftnum': 120},\n",
    "            '5T': {'cant_esperd_h': 12, 'cant_esperd_d': 288, 'cant_esperd_m': 8640, \n",
    "                   'cant_esperd_a': 103680, 'minutos': 5, 'shiftnum': 36},\n",
    "            '10T': {'cant_esperd_h': 6, 'cant_esperd_d': 144, 'cant_esperd_m': 4320, \n",
    "                    'cant_esperd_a': 51840, 'minutos': 10, 'shiftnum': 18},\n",
    "            'H': {'cant_esperd_h': 1, 'cant_esperd_d': 24, 'cant_esperd_m': 720, \n",
    "                  'cant_esperd_a': 8640, 'shiftnum': 3} ## Cambiar para adaptarse a directriz GGD\n",
    "        }\n",
    "    \n",
    "        # Obtener el valor de la estación\n",
    "        station_value = chunk['Station'].values[0]\n",
    "        if pd.isna(station_value):\n",
    "            print(f'La estación {station_value} no se encuentra en el análisis de frecuencias')\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "        else:\n",
    "            freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == station_value]\n",
    "            if freqinst200b_station.empty:\n",
    "                print(f\"No se encontró la estación {station_value} en freqinst200b\")\n",
    "                return {'periodos': None, 'frecuencias': None}\n",
    "            else:\n",
    "                freq_inf_value = freqinst200b_station['FreqInf'].values[0]\n",
    "    \n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    print(periodos)\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return {'periodos': None, 'frecuencias': None}\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return {'periodos': None, 'frecuencias': None}\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "    \n",
    "        if periodos in frecuencias:\n",
    "            return {'periodos': periodos, 'frecuencias': frecuencias[periodos]}\n",
    "        else:\n",
    "            print(f\"Periodo {periodos} no es reconocido en el diccionario de frecuencias\")\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "\n",
    "    def p_transm(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si existe al menos el 70% de datos esperados por día y hora\n",
    "        en la serie de datos; aquellos que no superen la prueba, son marcados como sospechosos'''\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk,_\n",
    "        \n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "        \n",
    "        # Obtener las cantidades esperadas de acuerdo a la frecuencia\n",
    "        cant_esperd_h = frecuencias['cant_esperd_h']\n",
    "        cant_esperd_d = frecuencias['cant_esperd_d']\n",
    "    \n",
    "        # Se establecen los aceptables\n",
    "        cant_aceptab_hora = 0.7 * cant_esperd_h\n",
    "        cant_aceptab_dia = 0.7 * cant_esperd_d\n",
    "    \n",
    "        # Agregar columna de etiquetas al dataframe original\n",
    "        chunk['Estado'] = np.nan\n",
    "        \n",
    "        # Definir función para asignar etiquetas\n",
    "        def asignar_etiqueta(row):\n",
    "            if row['count'] < cant_aceptab_hora:\n",
    "                chunk.loc[chunk['Fecha'].dt.floor('H') == row['Fecha'].floor('H'), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        # Evaluar por cada grupo de datos por hora y asignar la etiqueta\n",
    "        canthora = chunk.groupby(chunk['Fecha'].dt.floor('H')).size().reset_index(name='count')\n",
    "        canthora.apply(asignar_etiqueta, axis=1)\n",
    "        \n",
    "        # Definir función para asignar etiquetas de acumulado diario\n",
    "        def asignar_etiqueta_diaria(row):\n",
    "            if row['count'] < cant_aceptab_dia:\n",
    "                chunk.loc[chunk['Fecha'].dt.floor('D') == row['Fecha'].floor('D'), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        # Evaluar por cada grupo de datos por día y asignar la etiqueta\n",
    "        cantdia = chunk.groupby(chunk['Fecha'].dt.floor('D')).size().reset_index(name='count')\n",
    "        cantdia.apply(asignar_etiqueta_diaria, axis=1)\n",
    "        \n",
    "        return chunk, _\n",
    "\n",
    "    def p_estruct(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los datos fueron transmitidos en horas y minutos exactos al ser el\n",
    "        comportamiento esperado'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        # Se verifica si 'freq_info' is None\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk,None\n",
    "\n",
    "        periodos =  freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "        # Se hace frente al caso de no encontrar la estación\n",
    "        # Generar la operación para observar si la estructura es exacta en minutos\n",
    "        fecha = chunk['Fecha']\n",
    "    \n",
    "        # Se vectoriza la evaluación de la estructura por minuto\n",
    "        # Para cada chunk:\n",
    "        if periodos == 'T':\n",
    "            mask_estr = fecha.dt.second != 0\n",
    "        elif periodos == 'H':\n",
    "            mask_estr = (fecha.dt.minute != 0) | (fecha.dt.second != 0)\n",
    "        else:\n",
    "            # Se obtiene num_para_modulo\n",
    "            num_para_modulo = frecuencias['minutos']\n",
    "            mask_estr = fecha.dt.minute % num_para_modulo != 0\n",
    "    \n",
    "        # Registra los errores en el log manualmente\n",
    "        if mask_estr.any():\n",
    "            aligned_mask = mask_estr.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask].iterrows():\n",
    "                self.logger.info('Archivo: %s - Fila: %s - Valor fallido en p_estruct: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('Archivo: %s - No se encontraron fallos en p_estruct', archivo)\n",
    "            \n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_estr & (chunk['Estado'].isnull())\n",
    "        chunk.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_estr = mask_estr & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_estr & (chunk['Estado'] == '0PSO0')\n",
    "        chunk.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "\n",
    "        return chunk, mask_estr\n",
    "\n",
    "    #@log_failures\n",
    "    def p_limrig(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los datos crudos se encuentran fuera del umbral físico inferior o superior'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None\n",
    "        # Se genera la columna de estado anterior\n",
    "        chunk['Estado_Anterior'] = np.nan\n",
    "        \n",
    "        # Se establecen los umbrales físicos/rígidos a datos crudos en nuevas colummnas para vectorizar\n",
    "        chunk['umbr_crud_inf'] = 0.0\n",
    "        chunk['umbr_crud_sup'] = 100.0\n",
    "\n",
    "        # Compara el dato con umbrales inferiores y superiores \n",
    "        mask_outbounds = (chunk['Valor'] < chunk['umbr_crud_inf']) | (chunk['Valor'] > chunk['umbr_crud_sup'])\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado_Anterior'\n",
    "        condicion_0PSO0 = mask_outbounds & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado'\n",
    "        condicion_0PER0 = mask_outbounds & (chunk['Estado'].isnull() | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        \n",
    "        # Se eliminan las columnas no deseadas\n",
    "        if 'umbr_crud_inf' in chunk.columns:\n",
    "            chunk.drop(columns=['umbr_crud_inf', 'umbr_crud_sup'], axis=1, inplace=True)\n",
    "                \n",
    "        return chunk, mask_outbounds\n",
    "\n",
    "    #@log_failures\n",
    "    def p_persist(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los datos que se repiten por más de cuatro horas consecutivas'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None    \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "            \n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "\n",
    "        # Se crean máscaras para el intervalo del día con radiación solar que puede afectar la humedad\n",
    "        mask_sunny = (chunk['Fecha'].dt.hour >= 4) & (chunk['Fecha'].dt.hour <= 20)\n",
    "        # Se filtran los datos para esas horas\n",
    "        mask_sun = chunk[mask_sunny]\n",
    "\n",
    "        ## Se manejan las distintas frecuencias para verificar adecuadamente las persistencias\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        # Se verifica si 'freq_info' is None\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk,None\n",
    "        # Se accede a las claves del diccionario\n",
    "        periodos =  freq_info['periodos']\n",
    "        print(periodos)\n",
    "        # Se accede a los datos del diccionario\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "        print(frecuencias)\n",
    "\n",
    "        # Se asignan los shiftnums\n",
    "        cantshifts = frecuencias['shiftnum']\n",
    "        \n",
    "        # Crear una lista de máscaras usando una lista por comprensión\n",
    "        masks = [(mask_sun['Valor'] == mask_sun['Valor'].shift(i)) for i in range(1, cantshifts + 1)]\n",
    "\n",
    "        # Combinar todas las máscaras usando reduce y operador &\n",
    "        from functools import reduce\n",
    "        mask_pers4datos = reduce(lambda x, y: x & y, masks)\n",
    "        \n",
    "        ## Crear máscaras para cada comparación de las 4 filas consecutivas\n",
    "       # mask_1 = (mask_sun['Valor'] == mask_sun['Valor'].shift(1))\n",
    "        #mask_2 = (mask_sun['Valor'] == mask_sun['Valor'].shift(2))\n",
    "        #mask_3 = (mask_sun['Valor'] == mask_sun['Valor'].shift(3))\n",
    "        #mask_4 = (mask_sun['Valor'] == mask_sun['Valor'].shift(4))\n",
    "        \n",
    "        # Combinar todas las máscaras para obtener la condición deseada\n",
    "        #mask_pers4datos = mask_1 & mask_2 & mask_3 & mask_4\n",
    "        \n",
    "        # Etiquetado de valores, se inicia con el Estado Anterior\n",
    "        condicion_0PSO0 = mask_pers4datos & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado'\n",
    "        condicion_0PER0 = mask_pers4datos & (chunk['Estado'].isnull() | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        mask_pers4datos = mask_pers4datos & ~condicion_0PER0\n",
    "\n",
    "        condicion_0PER1 = mask_pers4datos & (chunk['Estado'] == '0PER0')\n",
    "        chunk.loc[condicion_0PER1, 'Estado'] = '0PER1'\n",
    "\n",
    "        self.last_rows = chunk.tail(4)\n",
    "        \n",
    "        return chunk, mask_pers4datos\n",
    "\n",
    "    def p_salto(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si la variación entre valores consecutivos excede 45.0 %'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "            \n",
    "        # Se toma nuevamente el archivo de frecuencias para analizar datos estrictamente consecutivos\n",
    "        freqinst200b = pd.read_csv('EMAHR_Allinfo_Replcbl.csv', encoding='latin-1') #, sep=';')\n",
    "\n",
    "        # Obtener la frecuencia de 'freqinst200b' basado en 'Station' y asignar a 'periodos'\n",
    "        sttn_code = chunk['Station'].values[0]\n",
    "        if pd.isna(sttn_code):\n",
    "            periodos = None\n",
    "        else:\n",
    "            freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == sttn_code]\n",
    "            if freqinst200b_station.empty:\n",
    "                print(f\"No se encontró la estación {sttn_code} en freqinst200b\")\n",
    "                return chunk\n",
    "            # Se toma el valor inferido de la frecuencia\n",
    "            freq_inf_value = freqinst200b_station['FreqInf'].values[0]\n",
    "            # Si el valor inferido es un NaN, se infiere dentro del código (esto porque hay estaciones con )\n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    print(periodos)\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return chunk\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return chunk\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return chunk\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "        \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk['Fecha_anterior'] = chunk['Fecha'].shift(1)\n",
    "        chunk['Delta_tiempo'] = chunk['Fecha'] - chunk['Fecha_anterior']\n",
    "        \n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk['Delta_tiempo'] == pd.to_timedelta(periodos)\n",
    "        \n",
    "        # Calcular la diferencia absoluta entre los valores consecutivos\n",
    "        chunk['Delta'] = chunk['Valor'].diff().abs()\n",
    "        chunk['Delta'] = chunk['Delta'].where(mask_consecutivo)\n",
    "\n",
    "        # Máscara para identificar variaciones mayores a 45.0\n",
    "        mask_variacion = chunk['Delta'] > 45.0\n",
    "      \n",
    "        # Etiquetado de valores, se inicia con el Estado Anterior\n",
    "        condicion_0PSO0 = mask_variacion & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado' - '0PER0'\n",
    "        condicion_0PER0 = mask_variacion & (chunk['Estado'].isnull() | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        mask_variacion = mask_variacion & ~condicion_0PER0\n",
    "        # '0PER1'\n",
    "        condicion_0PER1 = mask_variacion & (chunk['Estado'] == '0PER0')\n",
    "        chunk.loc[condicion_0PER1, 'Estado'] = '0PER1'\n",
    "        mask_variacion = mask_variacion & ~condicion_0PER1\n",
    "        # '0PER2'\n",
    "        condicion_0PER2 = mask_variacion & (chunk['Estado'] == '0PER1')\n",
    "        chunk.loc[condicion_0PER2, 'Estado'] = '0PER2'\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        chunk.drop(columns=['Delta', 'Fecha_anterior', 'Delta_tiempo'], axis=1, inplace=True)\n",
    "\n",
    "        return chunk, mask_variacion\n",
    "\n",
    "    def p_horavmaxmin(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los datos que son máximos y mínimos en horarios distintos a los posibles por la temperatura máxima en el\n",
    "        día según la radiación solar'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_hmm = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, simplemente copia el chunk\n",
    "            chunk_hmm = chunk.copy()\n",
    "            \n",
    "        # Se crean máscaras para los intervalos de tiempo conocidos para valores máximos y mínimos\n",
    "        mask_max_morning = (chunk_hmm['Fecha'].dt.hour >= 0) & (chunk_hmm['Fecha'].dt.hour < 10) \n",
    "        mask_max_afternoon = (chunk_hmm['Fecha'].dt.hour > 17) & (chunk_hmm['Fecha'].dt.hour <= 23)\n",
    "        mask_min_afternoon = (chunk_hmm['Fecha'].dt.hour >= 10) & (chunk_hmm['Fecha'].dt.hour <= 17)\n",
    "        \n",
    "        # Se filtran los datos para esas horas\n",
    "        max_validdata = chunk_hmm[mask_max_morning | mask_max_afternoon]\n",
    "        min_validdata = chunk_hmm[mask_min_afternoon]\n",
    "\n",
    "        # Encontrar dos valores máximos por día\n",
    "        max_values = chunk_hmm.groupby(chunk_hmm['Fecha'].dt.date).apply(lambda x: x.nlargest(2, 'Valor')).reset_index(level=0, drop=True)\n",
    "        \n",
    "        # Encontrar dos valores mínimos por día\n",
    "        min_values = chunk_hmm.groupby(chunk_hmm['Fecha'].dt.date).apply(lambda x: x.nsmallest(2, 'Valor')).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Verificar los máximos y mínimos y obtener las horas correspondientes\n",
    "        notvalid_max_values = max_values[~max_values.index.isin(max_validdata.index)]\n",
    "        notvalid_min_values = min_values[~min_values.index.isin(min_validdata.index)]\n",
    "\n",
    "        # Combinar los valores no válidos en un solo DataFrame\n",
    "        notvalid_values = pd.concat([notvalid_max_values, notvalid_min_values])\n",
    "        # Crear una máscara para identificar los índices de los valores no válidos\n",
    "        notval_maxmin = chunk_hmm.index.isin(notvalid_values.index)\n",
    "\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = notval_maxmin & chunk_hmm['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_hmm.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_hmm.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos\n",
    "        condicion_0PAT0 = notval_maxmin & (chunk_hmm['Estado'].isnull() | chunk_hmm['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk_hmm.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "\n",
    "        # Se copia al chunk original\n",
    "        chunk.loc[chunk_hmm.index] = chunk_hmm\n",
    "        return chunk, notval_maxmin\n",
    "\n",
    "    def p_sigma(self, chunk, archivo):\n",
    "        '''Esta prueba calcula, con los datos no etiquetados en la anterior prueba, la 4sigmas +- la media para detectar\n",
    "        datos atípicos en los conjuntos de los datos'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_sgm = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, simplemente copia el chunk\n",
    "            chunk_sgm = chunk.copy()\n",
    "\n",
    "        # Se calculan los estadísticos para sigma\n",
    "        mean = chunk_sgm['Valor'].mean()\n",
    "        std = chunk_sgm['Valor'].std()\n",
    "        # Con ellos, se establecen los límites superior e inferior\n",
    "        chunk_sgm['LimSup_Sigma'] = (mean + (4 * std))\n",
    "        chunk_sgm['LimInf_Sigma'] = (mean - (4 * std))\n",
    "\n",
    "        # Se etiquetan los valores que sobrepasen el límite\n",
    "        mask_outbsigma = (chunk_sgm['Valor'] < chunk_sgm['LimInf_Sigma']) | (chunk_sgm['Valor'] > chunk_sgm['LimSup_Sigma'])\n",
    "        \n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = mask_outbsigma & chunk_sgm['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_sgm.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_sgm.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos - '0PAT0'\n",
    "        condicion_0PAT0 = mask_outbsigma & (chunk_sgm['Estado'].isnull() | chunk_sgm['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk_sgm.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "        mask_outbsigma = mask_outbsigma & ~condicion_0PAT0\n",
    "        # 0PAT1\n",
    "        condicion_0PAT1 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT0')\n",
    "        chunk_sgm.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "        # mask_outbsigma = mask_outbsigma & ~condicion_0PAT1\n",
    "        # # 0PAT2\n",
    "        # condicion_0PAT2 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT1')\n",
    "        # chunk_sgm.loc[condicion_0PAT2, 'Estado'] = '0PAT2'\n",
    "                    \n",
    "        # Se eliminan las columnas no deseadas\n",
    "        if 'LimSup_Sigma' in chunk.columns:\n",
    "            chunk.drop(columns=['LimSup_Sigma', 'LimInf_Sigma'], axis=1, inplace=True)\n",
    "    \n",
    "        chunk.loc[chunk_sgm.index] = chunk_sgm\n",
    "        return chunk, mask_outbsigma\n",
    "\n",
    "    #@log_failures\n",
    "    def p_coherPFvals(self, chunk, archivo):\n",
    "        '''Esta prueba verifica que un valor tenga coherencia con los 5 anteriores según su desviación estándar y media'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None    \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "            \n",
    "        ## Trabajo con frecuencias\n",
    "        # Se toma nuevamente el archivo de frecuencias para analizar datos estrictamente consecutivos\n",
    "        freqinst200b = pd.read_csv('EMAHR_Allinfo_Replcbl.csv', encoding='latin-1', sep=';')\n",
    "        # Obtener la frecuencia de 'freqinst200b' basado en 'Station' y asignar a 'periodos'\n",
    "        sttn_code = chunk['Station'].values[0]\n",
    "        if pd.isna(sttn_code):\n",
    "            periodos = None\n",
    "        else:\n",
    "            freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == sttn_code]\n",
    "            if freqinst200b_station.empty:\n",
    "                print(f\"No se encontró la estación {sttn_code} en freqinst200b\")\n",
    "                return chunk, None\n",
    "            # Se toma el valor inferido de la frecuencia\n",
    "            freq_inf_value = freqinst200b_station['FreqInf'].values[0]\n",
    "            # Si el valor inferido es un NaN, se infiere dentro del código\n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return chunk, None\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return chunk, None\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return chunk, None\n",
    "\n",
    "        ## Se asegura la revisión de 5 datos anteriores aún si cambia el chunk\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "    \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "\n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk['Fecha_anterior'] = chunk['Fecha'].shift(1)\n",
    "        chunk['Delta_tiempo'] = chunk['Fecha'] - chunk['Fecha_anterior']\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        if 'Estado' not in chunk.columns or chunk['Estado'].isnull().all():\n",
    "            chunk_PFvals = chunk.copy()\n",
    "        else:\n",
    "            chunk_PFvals = chunk[~chunk['Estado'].str.startswith(('0PER','0PAT'), na=False)].copy()\n",
    "\n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk_PFvals['Delta_tiempo'] == pd.to_timedelta(periodos)\n",
    "        chunk_PFvals['consec_group'] = (~mask_consecutivo).cumsum()\n",
    "\n",
    "        # Se establecen diferentes ventanas según frecuencias\n",
    "        windows = {'T': {'window': 240}, '5T': {'window': 72}, '10T': {'window': 48}, 'H': {'window': 11}}\n",
    "        window_size = windows[periodos]['window']\n",
    "        half_window = window_size // 2\n",
    "\n",
    "        # Filtrar grupos que tienen al menos el tamaño de ventana necesario\n",
    "        group_counts = chunk_PFvals['consec_group'].value_counts()\n",
    "        valid_groups = group_counts[group_counts >= window_size].index\n",
    "        chnk_cohPFvl = chunk_PFvals[chunk_PFvals['consec_group'].isin(valid_groups)]\n",
    "\n",
    "        # Verificar que los datos anteriores y posteriores sean consecutivos\n",
    "        valid_indices = []\n",
    "        for i in range(half_window, len(chnk_cohPFvl) - half_window):\n",
    "            if all(mask_consecutivo[i-half_window:i+half_window]):\n",
    "                valid_indices.append(chnk_cohPFvl.index[i])\n",
    "\n",
    "        chnk_cohPFvl = chnk_cohPFvl.loc[valid_indices]\n",
    "\n",
    "        # Calcular el promedio y desviación estándar de los registros anteriores y posteriores\n",
    "        chnk_cohPFvl['mean_PF'] = chnk_cohPFvl['Valor'].rolling(window=window_size, center=True).mean()\n",
    "        chnk_cohPFvl['std_PF'] = chnk_cohPFvl['Valor'].rolling(window=window_size, center=True).std()\n",
    "\n",
    "        # Calcular los límites superior e inferior\n",
    "        chnk_cohPFvl['lim_inf'] = chnk_cohPFvl['mean_PF'] - (3 * chnk_cohPFvl['std_PF'])\n",
    "        chnk_cohPFvl['lim_sup'] = chnk_cohPFvl['mean_PF'] + (3 * chnk_cohPFvl['std_PF'])\n",
    "\n",
    "        # Añadir mensajes de depuración para verificar los límites\n",
    "        print(chnk_cohPFvl[['Fecha', 'Valor', 'mean_PF', 'std_PF', 'lim_inf', 'lim_sup']].head(20))\n",
    "\n",
    "        # Máscara para identificar valores fuera de los límites\n",
    "        mask_var5prev = (chnk_cohPFvl['Valor'] < chnk_cohPFvl['lim_inf']) | (chnk_cohPFvl['Valor'] > chnk_cohPFvl['lim_sup'])\n",
    "        \n",
    "        # Añadir mensajes de depuración para verificar los valores fuera de los límites\n",
    "        print(chnk_cohPFvl[mask_var5prev][['Fecha', 'Valor', 'lim_inf', 'lim_sup']])\n",
    "        \n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_var5prev & (chnk_cohPFvl['Estado'].isnull())\n",
    "        chnk_cohPFvl.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_var5prev = mask_var5prev & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_var5prev & (chnk_cohPFvl['Estado'] == '0PSO0')\n",
    "        chnk_cohPFvl.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "        mask_var5prev = mask_var5prev & ~condicion_0PSO1\n",
    "        # 0PSO2\n",
    "        condicion_0PSO2 = mask_var5prev & (chnk_cohPFvl['Estado'] == '0PSO1')\n",
    "        chnk_cohPFvl.loc[condicion_0PSO2, 'Estado'] = '0PSO2'\n",
    "    \n",
    "        # Se asegura la verificación de los valores anteriores si hubo cambio de chunk\n",
    "        self.last_rows = chnk_cohPFvl.tail(5)\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        if 'Fecha_anterior' in chunk.columns:\n",
    "            chunk.drop(columns=['Fecha_anterior','Delta_tiempo'], axis=1, inplace=True)\n",
    "            \n",
    "        # Copiar datos de chunk_coher al chunk original\n",
    "        chunk.loc[chnk_cohPFvl.index] = chnk_cohPFvl\n",
    "        # Continuar eliminando filas\n",
    "        if 'mean_5' in chunk.columns:\n",
    "            chunk.drop(columns=['mean_5','std_5','lim_inf','lim_sup'], axis=1, inplace=True)\n",
    "        \n",
    "        return chunk, mask_var5prev\n",
    "\n",
    "    def p_varminh(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los valores horarios que no varían en 1.0 % durante la hora'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None\n",
    "        # Se genera la columna 'Estado_Anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "    \n",
    "        # Se toma nuevamente el archivo de frecuencias para analizar datos estrictamente consecutivos\n",
    "        freqinst200b = pd.read_csv('EMAHR_Allinfo_Replcbl.csv', encoding='latin-1', sep=';')\n",
    "    \n",
    "        # Obtener la frecuencia de 'freqinst200b' basado en 'Station' y asignar a 'periodos'\n",
    "        sttn_code = chunk['Station'].values[0]\n",
    "        if pd.isna(sttn_code):\n",
    "            periodos = None\n",
    "        else:\n",
    "            freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == sttn_code]\n",
    "            if freqinst200b_station.empty:\n",
    "                print(f\"No se encontró la estación {sttn_code} en freqinst200b\")\n",
    "                return chunk, None\n",
    "            # Se toma el valor inferido de la frecuencia\n",
    "            freq_inf_value = freqinst200b_station['FreqInf'].values[0]\n",
    "            # Si el valor inferido es un NaN, se infiere dentro del código\n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return chunk, None\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return chunk, None\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return chunk, None\n",
    "    \n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "    \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "    \n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos o atípicos\n",
    "        if 'Estado' in chunk.columns and chunk['Estado'].notna().all():\n",
    "            chunk_varhmn = chunk[~chunk['Estado'].str.startswith(('0PER', '0PAT'), na=False)].copy()\n",
    "        else:\n",
    "            chunk_varhmn = chunk.copy()\n",
    "    \n",
    "        # Crear una columna de diferencia temporal y otras columnas temporales\n",
    "        chunk_varhmn['Fecha_anterior'] = chunk_varhmn['Fecha'].shift(1)\n",
    "        chunk_varhmn['Delta_tiempo'] = chunk_varhmn['Fecha'] - chunk_varhmn['Fecha_anterior']\n",
    "    \n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        freq_map = {'H': '1H', 'T': '1T', '5T': '5T', '10T': '10T'}\n",
    "        expected_delta = pd.to_timedelta(freq_map.get(periodos, '1H'))\n",
    "        mask_consecut = chunk_varhmn['Delta_tiempo'] == expected_delta\n",
    "    \n",
    "        # Calcular la diferencia absoluta entre los valores consecutivos\n",
    "        chunk_varhmn['Delta'] = chunk_varhmn['Valor'].diff().abs()\n",
    "        chunk_varhmn['Delta'] = chunk_varhmn['Delta'].where(mask_consecut)\n",
    "    \n",
    "        # Aplicar la máscara de las horas soleadas después de crear las columnas temporales\n",
    "        mask_sunny2 = (chunk_varhmn['Fecha'].dt.hour >= 6) & (chunk_varhmn['Fecha'].dt.hour <= 18)\n",
    "        mask_sun2 = chunk_varhmn[mask_sunny2]\n",
    "    \n",
    "        if periodos == 'H':\n",
    "            # Máscara para identificar variaciones menores a 1.0\n",
    "            mask_varhmin = mask_sun2['Delta'] < 1.0\n",
    "        else:\n",
    "            # Agrupar por horas y verificar si alguna variación dentro de la hora excede 1.0\n",
    "            mask_sun2['Fecha_hora'] = mask_sun2['Fecha'].dt.floor('H')\n",
    "            hora_groups = mask_sun2.groupby('Fecha_hora')\n",
    "            mask_varhmin = hora_groups['Delta'].transform(lambda x: (x < 1.0).any())\n",
    "            \n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_varhmin & (mask_sun2['Estado'].isnull())\n",
    "        mask_sun2.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_varhmin & (mask_sun2['Estado'] == '0PSO0')\n",
    "        mask_sun2.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO1\n",
    "        # 0PSO2\n",
    "        condicion_0PSO2 = mask_varhmin & (mask_sun2['Estado'] == '0PSO1')\n",
    "        mask_sun2.loc[condicion_0PSO2, 'Estado'] = '0PSO2'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO2\n",
    "        # 0PSO2\n",
    "        condicion_0PSO3 = mask_varhmin & (mask_sun2['Estado'] == '0PSO2')\n",
    "        mask_sun2.loc[condicion_0PSO3, 'Estado'] = '0PSO3'\n",
    "    \n",
    "        # Copiar datos de chunk_jmp al chunk original\n",
    "        chunk.loc[mask_sun2.index] = mask_sun2\n",
    "    \n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        columns_to_drop = ['Delta', 'Fecha_anterior', 'Delta_tiempo', 'Fecha_hora']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in chunk.columns]\n",
    "        chunk.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "        return chunk, mask_varhmin\n",
    "\n",
    "    def procesar_archivos(self, funcion_evaluacion):\n",
    "        '''Este método procesa la lectura y guardado de los archivos para todas las pruebas'''\n",
    "        archivos = self.ruta_archivos\n",
    "\n",
    "        archivos_salida = []  # Lista para almacenar nombres de archivos de salida\n",
    "\n",
    "        # Se recorre cada archivo en la carpeta\n",
    "        for archivo in archivos:\n",
    "            if archivo.endswith('.csv'):\n",
    "                ruta_archivo = os.path.join(self.dir_files, archivo)\n",
    "\n",
    "                reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=self.chunk_size)#,dtype={7: 'str'}, low_memory=False)\n",
    "                resultados = []\n",
    "\n",
    "                for chunk in reader:\n",
    "                    try:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                    except ValueError:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    chunk['Station'] = chunk['Station'].astype('int64')\n",
    "\n",
    "                    try:\n",
    "                        chunk_resultado, _ = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    except ValueError:\n",
    "                        chunk_resultado = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    resultados.append(chunk_resultado)\n",
    "\n",
    "                if not resultados:  # Se verifica si la lista está vacía\n",
    "                    self.logger.warning('No hay resultados válidos para concatenar en el archivo %s. Continuando con el siguiente.', archivo)\n",
    "                    continue\n",
    "                    \n",
    "                resultados_consolidados = pd.concat(resultados)\n",
    "\n",
    "                # Genera el nombre del archivo de salida conservando los primeros 19 caracteres del nombre del archivo original\n",
    "                nombre_archivo_salida = archivo[:19] + '_qc.csv'\n",
    "\n",
    "                resultados_consolidados.to_csv(os.path.join(self.dir_files, nombre_archivo_salida), encoding='latin-1', index=False)\n",
    "\n",
    "                archivos_salida.append(nombre_archivo_salida)  # Agregar el nombre del archivo a la lista\n",
    "            \n",
    "        # Actualiza self.ruta_archivos para que la próxima prueba procese los resultados de esta prueba\n",
    "        self.ruta_archivos = archivos_salida\n",
    "        # Se fija el log de procesamiento completo de archivos\n",
    "        self.logger.info('Procesamiento completo de archivos de estaciones HR. Archivos generados: %s', archivos_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58f92b9d-0c3a-4652-aa4b-f41a66ab8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador = AutomatHREMA('Test_QC')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d4348e0-435a-4f34-ad20-e1d5d8da9827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_4036\\791192592.py:97: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '0PSO0' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  chunk.loc[chunk['Fecha'].dt.floor('H') == row['Fecha'].floor('H'), 'Estado'] = '0PSO0'\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_transm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d2d470b-c286-4289-90e5-8d9148c65a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_estruct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c17071-31e8-4484-9ae9-7ce1af648709",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_limrig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea10b68-a8cb-4b67-b977-65d83029a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_persist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b8d2a-8da3-4629-8884-bdc299e12d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_salto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcc6e5-8bee-4f94-9d04-14d4dd4d97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_horavmaxmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aca86b59-71b3-4471-87b3-150eb60eae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "43d3f1d7-4219-42d7-b574-e61723082392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [Fecha, Valor, mean_PF, std_PF, lim_inf, lim_sup]\n",
      "Index: []\n",
      "Empty DataFrame\n",
      "Columns: [Fecha, Valor, lim_inf, lim_sup]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3448\\3087793811.py:23: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  for index, row in chunk[mask].iterrows():#chunk[aligned_mask].iterrows():\n"
     ]
    },
    {
     "ename": "IndexingError",
     "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m procesador\u001b[38;5;241m.\u001b[39mprocesar_archivos(procesador\u001b[38;5;241m.\u001b[39mp_coherPFvals)\n",
      "Cell \u001b[1;32mIn[21], line 714\u001b[0m, in \u001b[0;36mAutomatHREMA.procesar_archivos\u001b[1;34m(self, funcion_evaluacion)\u001b[0m\n\u001b[0;32m    711\u001b[0m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStation\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m chunk[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStation\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mint64\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 714\u001b[0m     chunk_resultado, _ \u001b[38;5;241m=\u001b[39m funcion_evaluacion(chunk, archivo)  \u001b[38;5;66;03m# Desempaqueta solo el DataFrame\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m    716\u001b[0m     chunk_resultado \u001b[38;5;241m=\u001b[39m funcion_evaluacion(chunk, archivo)  \u001b[38;5;66;03m# Desempaqueta solo el DataFrame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m, in \u001b[0;36mlog_failures.<locals>.wrapper\u001b[1;34m(self, chunk, archivo)\u001b[0m\n\u001b[0;32m     17\u001b[0m result, mask \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, chunk, archivo)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m#    aligned_mask = mask.reindex(chunk.index, fill_value=False)  # Asegura que la máscara esté alineada con el índice del DataFrame\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m#except AttributeError:\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m#    aligned_mask = mask  # Si no se puede reindexar, usa la máscara tal como está\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index, row \u001b[38;5;129;01min\u001b[39;00m chunk[mask]\u001b[38;5;241m.\u001b[39miterrows():\u001b[38;5;66;03m#chunk[aligned_mask].iterrows():\u001b[39;00m\n\u001b[0;32m     24\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mArchivo: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m - Fila: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m - Valor fallido en \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, archivo, index, func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValor\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3884\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3882\u001b[0m \u001b[38;5;66;03m# Do we have a (boolean) 1d indexer?\u001b[39;00m\n\u001b[0;32m   3883\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m com\u001b[38;5;241m.\u001b[39mis_bool_indexer(key):\n\u001b[1;32m-> 3884\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_bool_array(key)\n\u001b[0;32m   3886\u001b[0m \u001b[38;5;66;03m# We are left with two options: a single key, and a collection of keys,\u001b[39;00m\n\u001b[0;32m   3887\u001b[0m \u001b[38;5;66;03m# We interpret tuples as collections only for non-MultiIndex\u001b[39;00m\n\u001b[0;32m   3888\u001b[0m is_single_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(key)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3940\u001b[0m, in \u001b[0;36mDataFrame._getitem_bool_array\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3934\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   3935\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mItem wrong length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(key)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3936\u001b[0m     )\n\u001b[0;32m   3938\u001b[0m \u001b[38;5;66;03m# check_bool_indexer will throw exception if Series key cannot\u001b[39;00m\n\u001b[0;32m   3939\u001b[0m \u001b[38;5;66;03m# be reindexed to match DataFrame rows\u001b[39;00m\n\u001b[1;32m-> 3940\u001b[0m key \u001b[38;5;241m=\u001b[39m check_bool_indexer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex, key)\n\u001b[0;32m   3942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key\u001b[38;5;241m.\u001b[39mall():\n\u001b[0;32m   3943\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexing.py:2575\u001b[0m, in \u001b[0;36mcheck_bool_indexer\u001b[1;34m(index, key)\u001b[0m\n\u001b[0;32m   2573\u001b[0m indexer \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mget_indexer_for(index)\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01min\u001b[39;00m indexer:\n\u001b[1;32m-> 2575\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m IndexingError(\n\u001b[0;32m   2576\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnalignable boolean Series provided as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2577\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindexer (index of the boolean Series and of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2578\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthe indexed object do not match).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2579\u001b[0m     )\n\u001b[0;32m   2581\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[0;32m   2583\u001b[0m \u001b[38;5;66;03m# fall through for boolean\u001b[39;00m\n",
      "\u001b[1;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_coherPFvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e1add052-3700-456b-b350-ffbb7610289f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_3448\\1782787598.py:661: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  mask_sun2['Fecha_hora'] = mask_sun2['Fecha'].dt.floor('H')\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_varminh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449b253-9dcb-4b22-896d-a898101a2cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffae449-bdf5-43d3-8a79-d968c7460a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0a4d5f-ddfc-4cd6-a108-e43da098013b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Crear datos de prueba\n",
    "def crear_datos_prueba(frecuencia, num_datos):\n",
    "    fechas = pd.date_range(start='2024-01-01', periods=num_datos, freq=frecuencia)\n",
    "    valores = np.random.normal(loc=100, scale=10, size=num_datos)\n",
    "    datos = {'Fecha': fechas, 'Valor': valores, 'Station': ['TestStation'] * num_datos}\n",
    "    df_prueba = pd.DataFrame(datos)\n",
    "    return df_prueba\n",
    "\n",
    "# Simular la clase y la función\n",
    "class Validador:\n",
    "    def __init__(self):\n",
    "        self.last_rows = None\n",
    "        self.current_file = None\n",
    "    \n",
    "    def p_coher5vals(self, chunk, archivo):\n",
    "        '''Esta prueba verifica que un valor tenga coherencia con los 5 anteriores y 5 posteriores según su desviación estándar y media'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = None    \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = None\n",
    "\n",
    "        ## Trabajo con frecuencias\n",
    "        # Asumimos un periodo fijo para simplificar\n",
    "        periodos = '5T'\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return chunk, None\n",
    "\n",
    "        ## Se asegura la revisión de 5 datos anteriores aún si cambia el chunk\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "\n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "\n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk['Fecha_anterior'] = chunk['Fecha'].shift(1)\n",
    "        chunk['Delta_tiempo'] = chunk['Fecha'] - chunk['Fecha_anterior']\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        if 'Estado' not in chunk.columns or chunk['Estado'].isnull().all():\n",
    "            chunk_5vals = chunk.copy()\n",
    "        else:\n",
    "            chunk_5vals = chunk[~chunk['Estado'].str.startswith(('0PER','0PAT'), na=False)].copy()\n",
    "\n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk_5vals['Delta_tiempo'] == pd.to_timedelta(periodos)\n",
    "        chunk_5vals['consec_group'] = (~mask_consecutivo).cumsum()\n",
    "\n",
    "        # Se establecen diferentes ventanas según frecuencias\n",
    "        windows = {'T': {'window': 240}, '5T': {'window': 72}, '10T': {'window': 48}, 'H': {'window': 11}}\n",
    "        window_size = windows[periodos]['window']\n",
    "        half_window = window_size // 2\n",
    "\n",
    "        # Filtrar grupos que tienen al menos el tamaño de ventana necesario\n",
    "        group_counts = chunk_5vals['consec_group'].value_counts()\n",
    "        valid_groups = group_counts[group_counts >= window_size].index\n",
    "        chnk_coh5vl = chunk_5vals[chunk_5vals['consec_group'].isin(valid_groups)]\n",
    "\n",
    "        # Verificar que los datos anteriores y posteriores sean consecutivos\n",
    "        valid_indices = []\n",
    "        for i in range(half_window, len(chnk_coh5vl) - half_window):\n",
    "            if all(mask_consecutivo[i-half_window:i+half_window]):\n",
    "                valid_indices.append(chnk_coh5vl.index[i])\n",
    "\n",
    "        chnk_coh5vl = chnk_coh5vl.loc[valid_indices]\n",
    "\n",
    "        # Calcular el promedio y desviación estándar de los registros anteriores y posteriores\n",
    "        chnk_coh5vl['mean_5'] = chnk_coh5vl['Valor'].rolling(window=window_size, center=True).mean()\n",
    "        chnk_coh5vl['std_5'] = chnk_coh5vl['Valor'].rolling(window=window_size, center=True).std()\n",
    "\n",
    "        # Calcular los límites superior e inferior\n",
    "        chnk_coh5vl['lim_inf'] = chnk_coh5vl['mean_5'] - (3 * chnk_coh5vl['std_5'])\n",
    "        chnk_coh5vl['lim_sup'] = chnk_coh5vl['mean_5'] + (3 * chnk_coh5vl['std_5'])\n",
    "\n",
    "        # Añadir mensajes de depuración para verificar los límites\n",
    "        print(chnk_coh5vl[['Fecha', 'Valor', 'mean_5', 'std_5', 'lim_inf', 'lim_sup']].head(20))\n",
    "\n",
    "        # Máscara para identificar valores fuera de los límites\n",
    "        mask_var5prev = (chnk_coh5vl['Valor'] < chnk_coh5vl['lim_inf']) | (chnk_coh5vl['Valor'] > chnk_coh5vl['lim_sup'])\n",
    "        \n",
    "        # Añadir mensajes de depuración para verificar los valores fuera de los límites\n",
    "        print(chnk_coh5vl[mask_var5prev][['Fecha', 'Valor', 'lim_inf', 'lim_sup']])\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_var5prev & (chnk_coh5vl['Estado'].isnull())\n",
    "        chnk_coh5vl.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_var5prev = mask_var5prev & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_var5prev & (chnk_coh5vl['Estado'] == '0PSO0')\n",
    "        chnk_coh5vl.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "        mask_var5prev = mask_var5prev & ~condicion_0PSO1\n",
    "        # 0PSO2\n",
    "        condicion_0PSO2 = mask_var5prev & (chnk_coh5vl['Estado'] == '0PSO1')\n",
    "        chnk_coh5vl.loc[condicion_0PSO2, 'Estado'] = '0PSO2'\n",
    "\n",
    "        # Se asegura la verificación de los valores anteriores si hubo cambio de chunk\n",
    "        self.last_rows = chnk_coh5vl.tail(5)\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        if 'Fecha_anterior' in chunk.columns:\n",
    "            chunk.drop(columns=['Fecha_anterior','Delta_tiempo'], axis=1, inplace=True)\n",
    "\n",
    "        # Copiar datos de chunk_coher al chunk original\n",
    "        chunk.loc[chnk_coh5vl.index] = chnk_coh5vl\n",
    "        # Continuar eliminando filas\n",
    "        if 'mean_5' in chunk.columns:\n",
    "            chunk.drop(columns=['mean_5','std_5','lim_inf','lim_sup'], axis=1, inplace=True)\n",
    "\n",
    "        return chunk, mask_var5prev\n",
    "\n",
    "# Probar la función\n",
    "def probar_p_coher5vals():\n",
    "    # Crear un conjunto de datos de prueba\n",
    "    df_prueba = crear_datos_prueba('5T', 8000)  # Cambia '5T' y el número de datos según sea necesario\n",
    "\n",
    "    # Crear instancia de la clase Validador\n",
    "    validador = Validador()\n",
    "    \n",
    "    # Introducir algunos valores fuera de los límites para probar el etiquetado\n",
    "    df_prueba.loc[10:15, 'Valor'] = 200  # Valores anómalos que deberían ser etiquetados\n",
    "\n",
    "    # Aplicar la función\n",
    "    resultado_chunk, mask_var5prev = validador.p_coher5vals(df_prueba, 'archivo_prueba.csv')\n",
    "\n",
    "    # Guardar el resultado en un CSV para revisión manual\n",
    "    resultado_chunk.to_csv('resultado_prueba.csv', index=False)\n",
    "    \n",
    "    # Verificar si el resultado parece correcto\n",
    "    print(\"Revisar el archivo 'resultado_prueba.csv' para verificar los resultados.\")\n",
    "\n",
    "# Ejecutar la prueba\n",
    "probar_p_coher5vals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a62f93-ae2d-463a-b163-54b3ca3d3193",
   "metadata": {},
   "outputs": [],
   "source": [
    "!jupyter --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf873bc6-9ad6-4eef-aca7-86b415803ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### V1 - Procesar archivos:\n",
    "    def procesar_archivos(self, funcion_evaluacion):\n",
    "        '''Este método procesa la lectura y guardado de los archivos para todas las pruebas'''\n",
    "        archivos = self.ruta_archivos\n",
    "\n",
    "        archivos_salida = []  # Lista para almacenar nombres de archivos de salida\n",
    "\n",
    "        # Se recorre cada archivo en la carpeta\n",
    "        for archivo in archivos:\n",
    "            if archivo.endswith('.csv'):\n",
    "                ruta_archivo = os.path.join(self.dir_files, archivo)\n",
    "\n",
    "                reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=self.chunk_size)#,dtype={7: 'str'}, low_memory=False)\n",
    "                resultados = []\n",
    "\n",
    "                for chunk in reader:\n",
    "                    try:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                    except ValueError:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    chunk['Station'] = chunk['Station'].astype('int64')\n",
    "\n",
    "                    try:\n",
    "                        chunk_resultado, _ = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    except ValueError:\n",
    "                        chunk_resultado = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    resultados.append(chunk_resultado)\n",
    "\n",
    "                if not resultados:  # Se verifica si la lista está vacía\n",
    "                    self.logger.warning('No hay resultados válidos para concatenar en el archivo %s. Continuando con el siguiente.', archivo)\n",
    "                    continue\n",
    "                    \n",
    "                resultados_consolidados = pd.concat(resultados)\n",
    "\n",
    "                # Genera el nombre del archivo de salida conservando los primeros 19 caracteres del nombre del archivo original\n",
    "                nombre_archivo_salida = archivo[:19] + '_qc.csv'\n",
    "\n",
    "                resultados_consolidados.to_csv(os.path.join(self.dir_files, nombre_archivo_salida), encoding='latin-1', index=False)\n",
    "\n",
    "                archivos_salida.append(nombre_archivo_salida)  # Agregar el nombre del archivo a la lista\n",
    "            \n",
    "        # Actualiza self.ruta_archivos para que la próxima prueba procese los resultados de esta prueba\n",
    "        self.ruta_archivos = archivos_salida\n",
    "        # Se fija el log de procesamiento completo de archivos\n",
    "        self.logger.info('Procesamiento completo de archivos de estaciones HR. Archivos generados: %s', archivos_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1910ba-dde6-4667-9c5b-cac66bfc65bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "### V1 logging\n",
    "\n",
    "# Configuración del logger para guardar en el directorio de archivos y sobrescribir cada vez\n",
    "def setup_logger(log_file_path):\n",
    "    logger = logging.getLogger('Test_QC')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='a')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger\n",
    "\n",
    "def log_failures(func):\n",
    "    @wraps(func)\n",
    "    def wrapper(self, chunk, archivo):\n",
    "        try:\n",
    "            result, mask = func(self, chunk, archivo)\n",
    "            if mask is not None:\n",
    "                try:\n",
    "                    aligned_mask = mask.reindex(chunk.index, fill_value=False)  # Asegura que la máscara esté alineada con el índice del DataFrame\n",
    "                except AttributeError:\n",
    "                    aligned_mask = mask  # Si no se puede reindexar, usa la máscara tal como está\n",
    "                for index, row in chunk[aligned_mask].iterrows():\n",
    "                    self.logger.info('Archivo: %s - Fila: %s - Valor fallido en %s: %s', archivo, index, func.__name__, row['Valor'])\n",
    "            return result\n",
    "        except ValueError as e:\n",
    "            self.logger.error('Error procesando el archivo %s: %s', archivo, str(e))\n",
    "            return chunk  # Devuelve una máscara falsa para manejar el error\n",
    "    return wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77019629",
   "metadata": {},
   "outputs": [],
   "source": [
    "## V1 prueba de transmisión\n",
    "    \n",
    "    def p_transm(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si existe al menos el 70% de datos esperados por día y hora\n",
    "        en la serie de datos; aquellos que no superen la prueba, son marcados como sospechosos'''\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk,_\n",
    "        \n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "        \n",
    "        # Obtener las cantidades esperadas de acuerdo a la frecuencia\n",
    "        cant_esperd_h = frecuencias['cant_esperd_h']\n",
    "        cant_esperd_d = frecuencias['cant_esperd_d']\n",
    "    \n",
    "        # Se establecen los aceptables\n",
    "        cant_aceptab_hora = 0.7 * cant_esperd_h\n",
    "        cant_aceptab_dia = 0.7 * cant_esperd_d\n",
    "    \n",
    "        # Agregar columna de etiquetas al dataframe original\n",
    "        chunk['Estado'] = np.nan\n",
    "        \n",
    "        # Definir función para asignar etiquetas\n",
    "        def asignar_etiqueta(row):\n",
    "            if row['count'] < cant_aceptab_hora:\n",
    "                chunk.loc[chunk['Fecha'].dt.floor('H') == row['Fecha'].floor('H'), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        # Evaluar por cada grupo de datos por hora y asignar la etiqueta\n",
    "        canthora = chunk.groupby(chunk['Fecha'].dt.floor('H')).size().reset_index(name='count')\n",
    "        canthora.apply(asignar_etiqueta, axis=1)\n",
    "        \n",
    "        # Definir función para asignar etiquetas de acumulado diario\n",
    "        def asignar_etiqueta_diaria(row):\n",
    "            if row['count'] < cant_aceptab_dia:\n",
    "                chunk.loc[chunk['Fecha'].dt.floor('D') == row['Fecha'].floor('D'), 'Estado'] = '0PSO0'\n",
    "        \n",
    "        # Evaluar por cada grupo de datos por día y asignar la etiqueta\n",
    "        cantdia = chunk.groupby(chunk['Fecha'].dt.floor('D')).size().reset_index(name='count')\n",
    "        cantdia.apply(asignar_etiqueta_diaria, axis=1)\n",
    "        \n",
    "        return chunk, _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f87fe54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13f5559",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2576f65f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
