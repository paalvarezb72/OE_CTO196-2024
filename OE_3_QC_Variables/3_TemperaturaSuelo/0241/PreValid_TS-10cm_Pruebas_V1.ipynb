{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74f2b16b-912e-4a9b-98c0-5a393e545013",
   "metadata": {},
   "source": [
    "# Pruebas automatizadas datos humedad relativa - Clase"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4885f6f6",
   "metadata": {},
   "source": [
    "> Elaborado por Paola Álvarez, profesional contratista IDEAM, contrato 196 de 2024. Comentarios o inquietudes, remitir a *palvarez@ideam.gov.co* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883ebabc-ed05-4660-a06e-7c242cb0ffeb",
   "metadata": {},
   "source": [
    "**Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28398810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "from functools import wraps\n",
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be14c1a4",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b7267c",
   "metadata": {},
   "source": [
    "A continuación, se encuentran las pruebas de pre-validación de datos de EMA para verificar su capacidad de detección de datos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89e3c8ea",
   "metadata": {},
   "source": [
    "## Clase con métodos de aplicación de QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dab107-ab7e-48ae-939b-7b0a6b24ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración del logger para guardar en el directorio de archivos y sobrescribir cada vez\n",
    "def setup_logger(log_file_path):\n",
    "    logger = logging.getLogger('RawUnmodified_TS-10cm')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # Clear existing handlers to avoid duplicate logs\n",
    "    if logger.hasHandlers():\n",
    "        logger.handlers.clear()\n",
    "    file_handler = logging.FileHandler(log_file_path, mode='a')\n",
    "    file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "    logger.addHandler(file_handler)\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ddb424af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutomatTS10cmEMA:\n",
    "    \n",
    "    def __init__(self, dir_files, chunk_size=540000):\n",
    "        self.dir_files = dir_files\n",
    "        self.ruta_archivos = os.listdir(dir_files)\n",
    "        self.chunk_size = chunk_size\n",
    "        self.last_rows = None\n",
    "        self.current_file = None\n",
    "        # Sección configuración de logs\n",
    "        log_file_path = os.path.join(dir_files, 'QC_TS-10cm.log')\n",
    "        self.logger = setup_logger(log_file_path)\n",
    "        self.logger.info('Inicialización de PreValidTS10EMA en directorio: %s', dir_files)\n",
    "\n",
    "    def process_freqs(self, chunk, archivo):\n",
    "        '''Esta función procesa las frecuencias para abreviar variedad de métodos en adelante'''\n",
    "        # Convertir la columna de fecha a datetime si aún no lo es\n",
    "        if not pd.api.types.is_datetime64_any_dtype(chunk['Fecha']):\n",
    "            chunk['Fecha'] = pd.to_datetime(chunk['Fecha'])\n",
    "            \n",
    "        # Cargar el archivo de frecuencias\n",
    "        freqinst200b = pd.read_csv('EMATS10_LatLonEntFreq.csv', encoding='latin-1') #, sep=';')\n",
    "    \n",
    "        # Definir el diccionario de frecuencias y cantidades esperadas\n",
    "        frecuencias = {\n",
    "            'min': {'cant_esperd_h': 60, 'cant_esperd_d': 1440, 'cant_esperd_m': 43200, \n",
    "                  'cant_esperd_a': 518400, 'minutos': 1, 'shiftnum': 92, 'jumpnum':15},\n",
    "            '5min': {'cant_esperd_h': 12, 'cant_esperd_d': 288, 'cant_esperd_m': 8640, \n",
    "                   'cant_esperd_a': 103680, 'minutos': 5, 'shiftnum': 27, 'jumpnum':20},\n",
    "            '10min': {'cant_esperd_h': 6, 'cant_esperd_d': 144, 'cant_esperd_m': 4320, \n",
    "                    'cant_esperd_a': 51840, 'minutos': 10, 'shiftnum': 19, 'jumpnum':25},\n",
    "            'h': {'cant_esperd_h': 1, 'cant_esperd_d': 24, 'cant_esperd_m': 720, \n",
    "                  'cant_esperd_a': 8640, 'shiftnum':12, 'jumpnum':35} ## Cambiar para adaptarse a directriz GGD\n",
    "        }\n",
    "    \n",
    "        # Obtener el valor de la estación\n",
    "        station_value = chunk['Station'].values[0]\n",
    "        if pd.isna(station_value):\n",
    "            print(f'La estación {station_value} no se encuentra en el análisis de frecuencias')\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "        else:\n",
    "            freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == station_value]\n",
    "            if freqinst200b_station.empty:\n",
    "                print(f\"No se encontró la estación {station_value} en freqinst200b\")\n",
    "                return {'periodos': None, 'frecuencias': None}\n",
    "            else:\n",
    "                freq_inf_value = freqinst200b_station['FreqInf'].values[0]\n",
    "    \n",
    "            if pd.isna(freq_inf_value):\n",
    "                try:\n",
    "                    periodos = pd.infer_freq(chunk['Fecha'][-25:])\n",
    "                    print(periodos)\n",
    "                    if periodos is None:\n",
    "                        print(f\"Frecuencia inferida es None para el archivo {archivo}\")\n",
    "                        return {'periodos': None, 'frecuencias': None}\n",
    "                except ValueError as e:\n",
    "                    print(f'Error al inferir la frecuencia en el archivo {archivo}: {str(e)}')\n",
    "                    return {'periodos': None, 'frecuencias': None}\n",
    "            else:\n",
    "                periodos = freq_inf_value\n",
    "    \n",
    "        if periodos is None:\n",
    "            print(f\"Periodo es None para el archivo {archivo}\")\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "    \n",
    "        if periodos in frecuencias:\n",
    "            return {'periodos': periodos, 'frecuencias': frecuencias[periodos]}\n",
    "        else:\n",
    "            print(f\"Periodo {periodos} no es reconocido en el diccionario de frecuencias\")\n",
    "            return {'periodos': None, 'frecuencias': None}\n",
    "\n",
    "    def p_transm(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si existe al menos el 70% de datos esperados por día y hora\n",
    "        en la serie de datos; aquellos que no superen la prueba, son marcados como sospechosos'''\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "        \n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "\n",
    "        # Obtener las cantidades esperadas de acuerdo a la frecuencia\n",
    "        cant_esperd_h = frecuencias['cant_esperd_h']\n",
    "        cant_esperd_d = frecuencias['cant_esperd_d']\n",
    "    \n",
    "        # Se establecen los aceptables\n",
    "        cant_aceptab_hora = 0.7 * cant_esperd_h\n",
    "        cant_aceptab_dia = 0.7 * cant_esperd_d\n",
    "    \n",
    "        # Agregar columna de etiquetas al dataframe original\n",
    "        chunk['Estado'] = ''\n",
    "        \n",
    "        # Definir función para asignar etiquetas y llenar archivo log\n",
    "        def asignar_etiqueta(row):\n",
    "            if row['count'] < cant_aceptab_hora:\n",
    "                filas_fallidas = chunk.loc[chunk['Fecha'].dt.floor('h') == row['Fecha'].floor('h')]\n",
    "                chunk.loc[filas_fallidas.index, 'Estado'] = '0PSO0'\n",
    "                # Registrar en el log las filas que fallaron\n",
    "                for index, fila in filas_fallidas.iterrows():\n",
    "                    self.logger.info('File %s - Row %s - failed hour p_transm: %s', archivo, index, fila['Fecha'])\n",
    "\n",
    "        # Evaluar por cada grupo de datos por hora y asignar la etiqueta\n",
    "        canthora = chunk.groupby(chunk['Fecha'].dt.floor('h')).size().reset_index(name='count')\n",
    "        canthora.apply(asignar_etiqueta, axis=1)\n",
    "        \n",
    "        # Definir función para asignar etiquetas de acumulado diario y llenar archivo log\n",
    "        def asignar_etiqueta_diaria(row):\n",
    "            if row['count'] < cant_aceptab_dia:\n",
    "                filas_fallidas_dia = chunk.loc[chunk['Fecha'].dt.floor('D') == row['Fecha'].floor('D')]\n",
    "                chunk.loc[filas_fallidas_dia.index, 'Estado'] = '0PSO0'\n",
    "                # Registrar en el log las filas que fallaron\n",
    "                for index, fila in filas_fallidas_dia.iterrows():\n",
    "                    self.logger.info('File %s - Row %s - failed day p_transm: %s', archivo, index, fila['Fecha'])\n",
    "\n",
    "        # Evaluar por cada grupo de datos por día y asignar la etiqueta\n",
    "        cantdia = chunk.groupby(chunk['Fecha'].dt.floor('D')).size().reset_index(name='count')\n",
    "        cantdia.apply(asignar_etiqueta_diaria, axis=1)\n",
    "        \n",
    "        return chunk\n",
    "\n",
    "    def p_estruc(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los datos fueron transmitidos en horas y minutos exactos al ser el\n",
    "        comportamiento esperado'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        # Se verifica si 'freq_info' is None\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "        periodos =  freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "\n",
    "        # Generar la operación para observar si la estructura es exacta en minutos\n",
    "        fecha = chunk['Fecha']\n",
    "    \n",
    "        # Se vectoriza la evaluación de la estructura por minuto para cada chunk:\n",
    "        if periodos == 'min':\n",
    "            mask_estr = fecha.dt.second != 0\n",
    "        elif periodos == 'h':\n",
    "            mask_estr = (fecha.dt.minute != 0) | (fecha.dt.second != 0)\n",
    "        else:\n",
    "            # Se obtiene num_para_modulo\n",
    "            num_para_modulo = frecuencias['minutos']\n",
    "            mask_estr = fecha.dt.minute % num_para_modulo != 0\n",
    "    \n",
    "        # Se registran los errores en el log\n",
    "        if mask_estr.any():\n",
    "           aligned_mask = mask_estr.reindex(chunk.index, fill_value=False)\n",
    "           for index, row in chunk[aligned_mask].iterrows():\n",
    "               self.logger.info('File %s - Row %s - failed time p_estruc: %s', archivo, index, row['Fecha'])\n",
    "        else:\n",
    "           self.logger.info('File: %s - No se encontraron fallos en p_estruc', archivo)\n",
    "        \n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_estr & (chunk['Estado']=='')\n",
    "        chunk.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_estr = mask_estr & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_estr & (chunk['Estado'] == '0PSO0')\n",
    "        chunk.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "\n",
    "        return chunk\n",
    "\n",
    "    def p_limrig(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si los datos crudos se encuentran fuera del umbral físico inferior o superior'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna de estado anterior\n",
    "        chunk['Estado_Anterior'] = ''\n",
    "        \n",
    "        # Se establecen los umbrales físicos/rígidos a datos crudos en nuevas colummnas para vectorizar\n",
    "        chunk['umbr_crud_inf'] = -15.0\n",
    "        chunk['umbr_crud_sup'] = 40.0\n",
    "\n",
    "        # Compara el dato con umbrales inferiores y superiores \n",
    "        mask_outbounds = (chunk['Valor'] < chunk['umbr_crud_inf']) | (chunk['Valor'] > chunk['umbr_crud_sup'])\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_outbounds.any():\n",
    "            aligned_mask_lr = mask_outbounds.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask_lr].iterrows():\n",
    "                self.logger.info('File %s - Row %s - failed val p_limrig: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_limrig', archivo)\n",
    "\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado_Anterior'\n",
    "        condicion_0PSO0 = mask_outbounds & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "        # Lógica de etiquetado para 'Estado'\n",
    "        condicion_0PER0 = mask_outbounds & ((chunk['Estado']=='') | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        \n",
    "        # Se eliminan las columnas no deseadas\n",
    "        if 'umbr_crud_inf' in chunk.columns:\n",
    "            chunk.drop(columns=['umbr_crud_inf', 'umbr_crud_sup'], axis=1, inplace=True)\n",
    "                \n",
    "        return chunk\n",
    "\n",
    "    def p_perst(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los datos que se repiten por más de cuatro horas consecutivas'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''    \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "            \n",
    "        # Guardar referencia a las filas originales del chunk actual\n",
    "        original_chunk = chunk.copy()\n",
    "            \n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "\n",
    "        # Verificar y eliminar duplicados en el índice después de la concatenación\n",
    "        if not chunk.index.is_unique:\n",
    "            chunk = chunk.reset_index(drop=True)  # Crear un nuevo índice único\n",
    "\n",
    "        # Crear una etiqueta temporal para saber qué filas provienen del chunk anterior\n",
    "        if self.last_rows is not None:\n",
    "            chunk['from_previous_chunk'] = [True] * len(self.last_rows) + [False] * len(original_chunk)\n",
    "        else:\n",
    "            chunk['from_previous_chunk'] = [False] * len(original_chunk)\n",
    "\n",
    "        # Se crean máscaras para el intervalo del día con radiación solar que puede afectar la humedad\n",
    "        mask_sunny = (chunk['Fecha'].dt.hour >= 5) & (chunk['Fecha'].dt.hour <= 19)\n",
    "        # Se filtran los datos para esas horas\n",
    "        mask_sun = chunk[mask_sunny]\n",
    "\n",
    "        ## Se manejan las distintas frecuencias para verificar adecuadamente las persistencias\n",
    "        # Se instancia el método 'process_freqs' para obtener las frecuencias \n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        # Se verifica si 'freq_info' is None\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return original_chunk\n",
    "        \n",
    "        # Se accede a las claves del diccionario\n",
    "        periodos =  freq_info['periodos']\n",
    "        # Se accede a los datos del diccionario\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "        # Se asignan los shiftnums\n",
    "        cantshifts = frecuencias['shiftnum']\n",
    "        \n",
    "        # Crear una lista de máscaras usando una lista por comprensión\n",
    "        masks = [(mask_sun['Valor'] == mask_sun['Valor'].shift(i)) for i in range(1, cantshifts + 1)]\n",
    "\n",
    "        # Combinar todas las máscaras usando reduce y operador &\n",
    "        from functools import reduce\n",
    "        mask_persdatos = reduce(lambda x, y: x & y, masks)\n",
    "        \n",
    "        #chunk.reset_index(drop=True)\n",
    "        # Se registran los errores en el log\n",
    "        if mask_persdatos.any():\n",
    "            aligned_mask_pers = mask_persdatos.reindex(chunk.index, fill_value=False)\n",
    "            for index, row in chunk[aligned_mask_pers].iterrows():\n",
    "                self.logger.info('File %s - Row %s - failed val p_perst: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_perst', archivo)\n",
    "\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        # Etiquetado de valores, se inicia con el Estado Anterior\n",
    "        condicion_0PSO0 = mask_persdatos & chunk['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Lógica de etiquetado para 'Estado'\n",
    "        condicion_0PER0 = mask_persdatos & ((chunk['Estado']=='') | chunk['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk.loc[condicion_0PER0, 'Estado'] = '0PER0'\n",
    "        mask_persdatos = mask_persdatos & ~condicion_0PER0\n",
    "\n",
    "        condicion_0PER1 = mask_persdatos & (chunk['Estado'] == '0PER0')\n",
    "        chunk.loc[condicion_0PER1, 'Estado'] = '0PER1'\n",
    "\n",
    "        # Eliminar solo las filas del chunk anterior que fueron evaluadas\n",
    "        chunk = chunk[~chunk['from_previous_chunk']].copy()\n",
    "\n",
    "        # Eliminar la columna temporal\n",
    "        chunk.drop(columns=['from_previous_chunk'], inplace=True)\n",
    "\n",
    "        # Actualizar las filas de self.last_rows para el próximo chunk\n",
    "        self.last_rows = chunk.tail(cantshifts).copy()\n",
    "\n",
    "        return chunk\n",
    "\n",
    "    def p_jump(self, chunk, archivo):\n",
    "        '''Esta prueba verifica si la variación entre valores consecutivos excede 45.0 %'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Se hace una instancia del método de 'process_freqs' para obtener las frecuencias\n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "\n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "        \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "        \n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_jmp = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, se copia el chunk\n",
    "            chunk_jmp = chunk.copy()\n",
    "        \n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk_jmp['Fecha_anterior'] = chunk_jmp['Fecha'].shift(1)\n",
    "        chunk_jmp['Delta_tiempo'] = chunk_jmp['Fecha'] - chunk_jmp['Fecha_anterior']\n",
    "        \n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk_jmp['Delta_tiempo'] == pd.to_timedelta(periodos)\n",
    "        \n",
    "        # Calcular la diferencia absoluta entre los valores consecutivos\n",
    "        chunk_jmp['Delta'] = chunk_jmp['Valor'].diff().abs()\n",
    "        chunk_jmp['Delta'] = chunk_jmp['Delta'].where(mask_consecutivo)\n",
    "\n",
    "        # Se determina número de salto\n",
    "        jumpnum = frecuencias['jumpnum']\n",
    "        \n",
    "        # Máscara para identificar variaciones mayores al número determinado para cada frecuencia\n",
    "        mask_variacion = chunk_jmp['Delta'] > jumpnum\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_variacion.any():\n",
    "            aligned_mask_salto = mask_variacion.reindex(chunk_jmp.index, fill_value=False)\n",
    "            for index, row in chunk_jmp[aligned_mask_salto].iterrows():\n",
    "                self.logger.info('File %s - Row %s - failed val p_jump: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_jump', archivo)\n",
    "      \n",
    "        # Etiquetado de valores, se inicia con el Estado Anterior\n",
    "        condicion_0PSO0 = mask_variacion & chunk_jmp['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_jmp.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_jmp.loc[condicion_0PSO0, 'Estado']\n",
    "        \n",
    "        chunk_jmp['Estado'] = chunk_jmp['Estado'].fillna('')\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = mask_variacion & chunk_jmp['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_jmp.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_jmp.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos\n",
    "        condicion_0PAT0 = mask_variacion & ((chunk_jmp['Estado'] == '') | chunk_jmp['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk_jmp.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        chunk_jmp.drop(columns=['Delta', 'Fecha_anterior', 'Delta_tiempo'], axis=1, inplace=True)\n",
    "        \n",
    "        # Se copia al chunk original\n",
    "        chunk.loc[chunk_jmp.index] = chunk_jmp\n",
    "        return chunk\n",
    "\n",
    "    def p_valvmin(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los datos que son máximos y mínimos en horarios distintos a los posibles por la temperatura máxima en el\n",
    "        día según la radiación solar'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_hm = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, se copia el chunk\n",
    "            chunk_hm = chunk.copy()\n",
    "            \n",
    "        # Se crean máscaras para los intervalos de tiempo conocidos para valores mínimos\n",
    "        mask_min_afternoon = (chunk_hm['Fecha'].dt.hour >= 9) & (chunk_hm['Fecha'].dt.hour <= 17)\n",
    "        # Se filtran los datos para esas horas\n",
    "        min_validdata = chunk_hm[mask_min_afternoon]\n",
    "\n",
    "        # Encontrar dos valores mínimos por día\n",
    "        min_values = chunk_hm.groupby(chunk_hm['Fecha'].dt.date).apply(lambda x: x.nsmallest(1, 'Valor')).reset_index(level=0, drop=True)\n",
    "\n",
    "        # Verificar los mínimos y obtener las horas correspondientes\n",
    "        notvalid_min_values = min_values[~min_values.index.isin(min_validdata.index)]\n",
    "\n",
    "        # Crear una máscara para identificar los índices de los valores no válidos\n",
    "        notval_min = chunk_hm.index.isin(notvalid_min_values.index) #notval_maxmin = chunk_hm.index.isin(notvalid_values.index)\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if notval_min.any():\n",
    "            for index, row in chunk_hm[notval_min].iterrows():\n",
    "                self.logger.info('File %s - Row %s - failed val p_valvmin: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_valvmin', archivo)\n",
    "        \n",
    "        chunk_hm['Estado'] = chunk_hm['Estado'].fillna('')\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = notval_min & chunk_hm['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_hm.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_hm.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos\n",
    "        condicion_0PAT0 = notval_min & ((chunk_hm['Estado'] == '') | chunk_hm['Estado'].isin(['0PSO0', '0PSO1'])) #notval_maxmin\n",
    "        chunk_hm.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "        notval_min = notval_min & ~condicion_0PAT0\n",
    "        \n",
    "        condicion_0PAT1 = notval_min & (chunk_hm['Estado'] == '0PAT0')\n",
    "        chunk_hm.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "\n",
    "        # Se copia al chunk original\n",
    "        chunk.loc[chunk_hm.index] = chunk_hm\n",
    "        return chunk\n",
    "\n",
    "    def p_sigmRP(self, chunk, archivo):\n",
    "        '''Esta prueba calcula, con los datos no etiquetados como erróneos, 4sigmas +- la media -como líms sup e inf- para detectar\n",
    "        datos atípicos en cada región pluviométrica homogénea obtenida por análisis de componentes principales (Ruíz, Cadena)'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_sgmRP = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, simplemente copia el chunk\n",
    "            chunk_sgmRP = chunk.copy()\n",
    "\n",
    "        # Cargar el archivo de frecuencias\n",
    "        RefRPLimInf = pd.read_csv('EMAHR_Allinfo_Replcble1.csv', encoding='latin-1', sep=';')\n",
    "        \n",
    "        # Se obtienen los valores de umbrales inferiores por estcación\n",
    "        for station in chunk['Station'].unique():\n",
    "            Sttn_LimRP =  RefRPLimInf[RefRPLimInf['Station'] == station]['RP_ACP'].values\n",
    "\n",
    "            # Verificar si Sttn_LimRP es NaN, con el fin de alertar y continuar el análisis\n",
    "            if np.isnan(Sttn_LimRP).all():\n",
    "                print(f'No hay valor de LimInfRP para la estación {station}. Se continúa con la siguiente estación')\n",
    "                continue\n",
    "            \n",
    "            Sttn_LimRP = Sttn_LimRP[0]\n",
    "            LimInfRP = RefRPLimInf.loc[RefRPLimInf['RP_ACP'] == Sttn_LimRP, 'LimInfRP'].iloc[0]\n",
    "\n",
    "            # Se crea la columna de límites inferiores para agilizar el proceso\n",
    "            chunk_sgmRP['LimInfRP'] = LimInfRP\n",
    "            chunk_sgmRP['LimInfRP'] = chunk_sgmRP['LimInfRP'].astype('float64')\n",
    "            \n",
    "            # Se comprueba si los datos son inferiores al límite establecido LimInfRP\n",
    "            mask_out_liminfRP = (chunk_sgmRP['Valor'] < chunk_sgmRP['LimInfRP'])\n",
    "\n",
    "            # Se registran los errores en el log             \n",
    "            if mask_out_liminfRP.any():\n",
    "                aligned_mask_sigmRP = mask_out_liminfRP.reindex(chunk_sgmRP.index, fill_value=False)\n",
    "                for index, row in chunk_sgmRP[aligned_mask_sigmRP].iterrows():\n",
    "                    self.logger.info('File %s - Row %s - failed val p_sgmRP: %s', archivo, index, row['Valor'])\n",
    "            else:\n",
    "                self.logger.info('File: %s - No se encontraron fallos en p_sgmRP', archivo)\n",
    "           \n",
    "            # Actualización de la columna de Estado\n",
    "            chunk_sgmRP['Estado'] = chunk_sgmRP['Estado'].fillna('')\n",
    "            # Condición llenado de 'Estado_Anterior', si aplica\n",
    "            condicion_0PSO0 = mask_out_liminfRP & chunk_sgmRP['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "            chunk_sgmRP.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_sgmRP.loc[condicion_0PSO0, 'Estado']\n",
    "    \n",
    "            # Se etiquetan los atípicos - '0PAT0'\n",
    "            condicion_0PAT0 = mask_out_liminfRP & ((chunk_sgmRP['Estado'] == '') | chunk_sgmRP['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "            chunk_sgmRP.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "            mask_out_liminfRP = mask_out_liminfRP & ~condicion_0PAT0\n",
    "            # 0PAT1\n",
    "            condicion_0PAT1 = mask_out_liminfRP & (chunk_sgmRP['Estado'] == '0PAT0')\n",
    "            chunk_sgmRP.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "            mask_out_liminfRP = mask_out_liminfRP & ~condicion_0PAT1\n",
    "            # 0PAT2\n",
    "            condicion_0PAT2 = mask_out_liminfRP & (chunk_sgmRP['Estado'] == '0PAT1')\n",
    "            chunk_sgmRP.loc[condicion_0PAT2, 'Estado'] = '0PAT2'\n",
    "            \n",
    "        # Se copia al chunk original\n",
    "        chunk.loc[chunk_sgmRP.index] = chunk_sgmRP\n",
    "        return chunk\n",
    "        \n",
    "    def p_sigma(self, chunk, archivo):\n",
    "        '''Esta prueba calcula, con los datos no etiquetados como erróneos, 4sigmas +- la media -como líms sup e inf- para detectar\n",
    "        datos atípicos en cada estación'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        if chunk['Estado'].notna().all(): # Se verifica que no hayan valores nulos en tal columna\n",
    "            chunk_sgm = chunk[~chunk['Estado'].str.startswith('0PER', na=False)].copy()\n",
    "        else:\n",
    "            # Si todos los valores son NaN, simplemente copia el chunk\n",
    "            chunk_sgm = chunk.copy()\n",
    "\n",
    "        # Se calculan los estadísticos para sigma\n",
    "        mean = chunk_sgm['Valor'].mean()\n",
    "        std = chunk_sgm['Valor'].std()\n",
    "        # Con ellos, se establecen los límites superior e inferior\n",
    "        chunk_sgm['LimSup_Sigma'] = (mean + (4 * std))\n",
    "        chunk_sgm['LimInf_Sigma'] = (mean - (4 * std))\n",
    "\n",
    "        # Se etiquetan los valores que sobrepasen el límite\n",
    "        mask_outbsigma = (chunk_sgm['Valor'] < chunk_sgm['LimInf_Sigma']) | (chunk_sgm['Valor'] > chunk_sgm['LimSup_Sigma'])\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_outbsigma.any():\n",
    "            aligned_mask_sigma = mask_outbsigma.reindex(chunk_sgm.index, fill_value=False)\n",
    "            for index, row in chunk_sgm[aligned_mask_sigma].iterrows():\n",
    "                self.logger.info('File %s - Row %s - failed val p_sigma: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_sigma', archivo)\n",
    "        \n",
    "        chunk_sgm['Estado'] = chunk_sgm['Estado'].fillna('')\n",
    "        ## Actualización de estado\n",
    "        # Condición llenado de 'Estado_Anterior', si aplica\n",
    "        condicion_0PSO0 = mask_outbsigma & chunk_sgm['Estado'].isin(['0PSO0', '0PSO1'])\n",
    "        chunk_sgm.loc[condicion_0PSO0, 'Estado_Anterior'] = chunk_sgm.loc[condicion_0PSO0, 'Estado']\n",
    "\n",
    "        # Se etiquetan los atípicos - '0PAT0'\n",
    "        condicion_0PAT0 = mask_outbsigma & ((chunk_sgm['Estado'] == '') | chunk_sgm['Estado'].isin(['0PSO0', '0PSO1']))\n",
    "        chunk_sgm.loc[condicion_0PAT0, 'Estado'] = '0PAT0'\n",
    "        mask_outbsigma = mask_outbsigma & ~condicion_0PAT0\n",
    "        # 0PAT1\n",
    "        condicion_0PAT1 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT0')\n",
    "        chunk_sgm.loc[condicion_0PAT1, 'Estado'] = '0PAT1'\n",
    "        mask_outbsigma = mask_outbsigma & ~condicion_0PAT1\n",
    "        # 0PAT2\n",
    "        condicion_0PAT2 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT1')\n",
    "        chunk_sgm.loc[condicion_0PAT2, 'Estado'] = '0PAT2'\n",
    "        mask_outbsigma = mask_outbsigma & ~condicion_0PAT2\n",
    "        # 0PAT3\n",
    "        condicion_0PAT3 = mask_outbsigma & (chunk_sgm['Estado'] == '0PAT2')\n",
    "        chunk_sgm.loc[condicion_0PAT3, 'Estado'] = '0PAT3'\n",
    "                    \n",
    "        # Se eliminan las columnas no deseadas\n",
    "        if 'LimSup_Sigma' in chunk.columns:\n",
    "            chunk.drop(columns=['LimSup_Sigma', 'LimInf_Sigma'], axis=1, inplace=True)\n",
    "    \n",
    "        chunk.loc[chunk_sgm.index] = chunk_sgm\n",
    "        return chunk\n",
    "\n",
    "    def p_coherPNv(self, chunk, archivo):\n",
    "        '''Esta prueba verifica que un valor tenga coherencia con los 5 valores anteriores y los 5 posteriores\n",
    "        según su desviación estándar y media'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''    \n",
    "        # Se genera la columna 'Estado_anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "            \n",
    "        ## Trabajo con frecuencias\n",
    "        # Se hace una instancia del método de 'process_freqs' para obtener las frecuencias\n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "\n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "\n",
    "        ## Se asegura la revisión de 5 datos anteriores aún si cambia el chunk\n",
    "        # Verificar si el archivo ha cambiado\n",
    "        if self.current_file != archivo:\n",
    "            # Si el archivo cambió, resetea self.last_rows y actualiza self.current_file\n",
    "            self.last_rows = None\n",
    "            self.current_file = archivo\n",
    "        # Usar self.last_rows para concatenar con el chunk actual\n",
    "        if self.last_rows is not None:\n",
    "            chunk = pd.concat([self.last_rows, chunk])\n",
    "            chunk.reset_index(drop=True)\n",
    "    \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "\n",
    "        # Crear una columna de diferencia temporal\n",
    "        chunk['Fecha_anterior'] = chunk['Fecha'].shift(1)\n",
    "        chunk['Delta_tiempo'] = chunk['Fecha'] - chunk['Fecha_anterior']\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos o atípicos\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        if 'Estado' not in chunk.columns or (chunk['Estado'] == '').all():\n",
    "            chunk_PFvals = chunk.copy()\n",
    "        else:\n",
    "            chunk_PFvals = chunk[~chunk['Estado'].str.startswith(('0PER','0PAT'), na=False)].copy()\n",
    "\n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        mask_consecutivo = chunk_PFvals['Delta_tiempo'] == pd.to_timedelta(periodos)\n",
    "        chunk_PFvals['consec_group'] = (~mask_consecutivo).cumsum()\n",
    "\n",
    "        # Se establecen diferentes ventanas según frecuencias\n",
    "        windows = {'1T': {'window': 240}, '5T': {'window': 72}, '10T': {'window': 48}, '1H': {'window': 11}}\n",
    "        window_size = windows[periodos]['window']\n",
    "        half_window = window_size // 2\n",
    "\n",
    "        # Filtrar grupos que tienen al menos el tamaño de ventana necesario\n",
    "        group_counts = chunk_PFvals['consec_group'].value_counts()\n",
    "        valid_groups = group_counts[group_counts >= window_size].index\n",
    "        chnk_cohPFvl = chunk_PFvals[chunk_PFvals['consec_group'].isin(valid_groups)]\n",
    "\n",
    "        # Verificar que los datos anteriores y posteriores sean consecutivos\n",
    "        valid_indices = []\n",
    "        for i in range(half_window, len(chnk_cohPFvl) - half_window):\n",
    "            if all(mask_consecutivo[i-half_window:i+half_window]):\n",
    "                valid_indices.append(chnk_cohPFvl.index[i])\n",
    "\n",
    "        chnk_cohPFvl = chnk_cohPFvl.loc[valid_indices]\n",
    "\n",
    "        # Calcular el promedio y desviación estándar de los registros anteriores y posteriores\n",
    "        chnk_cohPFvl['mean_PF'] = chnk_cohPFvl['Valor'].rolling(window=window_size, center=True).mean()\n",
    "        chnk_cohPFvl['std_PF'] = chnk_cohPFvl['Valor'].rolling(window=window_size, center=True).std()\n",
    "\n",
    "        # Calcular los límites superior e inferior\n",
    "        chnk_cohPFvl['lim_inf'] = chnk_cohPFvl['mean_PF'] - (3 * chnk_cohPFvl['std_PF'])\n",
    "        chnk_cohPFvl['lim_sup'] = chnk_cohPFvl['mean_PF'] + (3 * chnk_cohPFvl['std_PF'])\n",
    "\n",
    "        # Máscara para identificar valores fuera de los límites\n",
    "        mask_varPF = (chnk_cohPFvl['Valor'] < chnk_cohPFvl['lim_inf']) | (chnk_cohPFvl['Valor'] > chnk_cohPFvl['lim_sup'])\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_varPF.any():\n",
    "            aligned_mask_sigma = mask_varPF.reindex(chnk_cohPFvl.index, fill_value=False)\n",
    "            for index, row in chnk_cohPFvl[aligned_mask_sigma].iterrows():\n",
    "                self.logger.info('File %s - Row %s - failed val p_coherPNv %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_coherPNv', archivo)\n",
    "        \n",
    "        chnk_cohPFvl['Estado'] = chnk_cohPFvl['Estado'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_varPF & ((chnk_cohPFvl['Estado'] == ''))\n",
    "        chnk_cohPFvl.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_varPF = mask_varPF & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_varPF & (chnk_cohPFvl['Estado'] == '0PSO0')\n",
    "        chnk_cohPFvl.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "        mask_varPF = mask_varPF & ~condicion_0PSO1\n",
    "        # 0PSO2\n",
    "        condicion_0PSO2 = mask_varPF & (chnk_cohPFvl['Estado'] == '0PSO1')\n",
    "        chnk_cohPFvl.loc[condicion_0PSO2, 'Estado'] = '0PSO2'\n",
    "    \n",
    "        # Se asegura la verificación de los valores anteriores si hubo cambio de chunk\n",
    "        self.last_rows = chnk_cohPFvl.tail(5)\n",
    "\n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        if 'Fecha_anterior' in chunk.columns:\n",
    "            chunk.drop(columns=['Fecha_anterior','Delta_tiempo'], axis=1, inplace=True)\n",
    "            \n",
    "        # Copiar datos de chunk_coher al chunk original\n",
    "        chunk.loc[chnk_cohPFvl.index] = chnk_cohPFvl\n",
    "        # Continuar eliminando filas\n",
    "        if 'mean_5' in chunk.columns:\n",
    "            chunk.drop(columns=['mean_5','std_5','lim_inf','lim_sup'], axis=1, inplace=True)\n",
    "        \n",
    "        return chunk\n",
    "\n",
    "    def p_varmin(self, chunk, archivo):\n",
    "        '''Esta prueba detecta los valores horarios que no varían en 1.0 % durante la hora'''\n",
    "        # Se crea la columna 'Estado' si no existe\n",
    "        if 'Estado' not in chunk.columns:\n",
    "            chunk['Estado'] = ''\n",
    "        # Se genera la columna 'Estado_Anterior' si no existe\n",
    "        if 'Estado_Anterior' not in chunk.columns:\n",
    "            chunk['Estado_Anterior'] = ''\n",
    "        ## Trabajo con frecuencias\n",
    "        # Se hace una instancia del método de 'process_freqs' para obtener las frecuencias\n",
    "        freq_info = self.process_freqs(chunk, archivo)\n",
    "        if freq_info is None or freq_info['periodos'] is None or freq_info['frecuencias'] is None:\n",
    "            print(f\"No se pudo obtener información de frecuencia para el archivo {archivo}\")\n",
    "            return chunk\n",
    "\n",
    "        periodos = freq_info['periodos']\n",
    "        frecuencias = freq_info['frecuencias']\n",
    "    \n",
    "        # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "        if periodos.isalpha():\n",
    "            periodos = '1' + periodos\n",
    "    \n",
    "        # Ordenar el chunk por la columna 'Fecha'\n",
    "        chunk = chunk.sort_values('Fecha').reset_index(drop=True)\n",
    "\n",
    "        # Se genera filtro para no considerar datos ya catalogados como erróneos o atípicos\n",
    "        chunk['Estado'] = chunk['Estado'].fillna('')\n",
    "        if 'Estado' in chunk.columns and chunk['Estado'].notna().all():\n",
    "            chunk_varhmn = chunk[~chunk['Estado'].str.startswith(('0PER', '0PAT'), na=False)].copy()\n",
    "        else:\n",
    "            chunk_varhmn = chunk.copy()\n",
    "\n",
    "        # Crear una columna de diferencia temporal y otras columnas temporales\n",
    "        chunk_varhmn['Fecha_anterior'] = chunk_varhmn['Fecha'].shift(1)\n",
    "        chunk_varhmn['Delta_tiempo'] = chunk_varhmn['Fecha'] - chunk_varhmn['Fecha_anterior']\n",
    "    \n",
    "        # Crear una máscara para identificar filas consecutivas según la frecuencia esperada\n",
    "        freq_map = {'H': '1H', 'T': '1T', '5T': '5T', '10T': '10T'}\n",
    "        expected_delta = pd.to_timedelta(freq_map.get(periodos, '1H'))\n",
    "        mask_consecut = chunk_varhmn['Delta_tiempo'] == expected_delta\n",
    "    \n",
    "        # Calcular la diferencia absoluta entre los valores consecutivos\n",
    "        chunk_varhmn['Delta'] = chunk_varhmn['Valor'].diff().abs()\n",
    "        chunk_varhmn['Delta'] = chunk_varhmn['Delta'].where(mask_consecut)\n",
    "    \n",
    "        # Aplicar la máscara de las horas soleadas después de crear las columnas temporales\n",
    "        mask_sunny2 = (chunk_varhmn['Fecha'].dt.hour >= 6) & (chunk_varhmn['Fecha'].dt.hour <= 18)\n",
    "        mask_sun2 = chunk_varhmn[mask_sunny2].copy()\n",
    "    \n",
    "        if periodos == 'H':\n",
    "            # Máscara para identificar variaciones menores a 0.1 o 0.01\n",
    "            mask_varhmin = mask_sun2['Delta'] < 0.1\n",
    "        else:\n",
    "            # Agrupar por horas y verificar si alguna variación dentro de la hora excede 1.0\n",
    "            mask_sun2.loc[:, 'Fecha_hora'] = mask_sun2['Fecha'].dt.floor('h')\n",
    "            hora_groups = mask_sun2.groupby('Fecha_hora')\n",
    "            mask_varhmin = hora_groups['Delta'].transform(lambda x: (x < 0.01).any())\n",
    "\n",
    "        # Se registran los errores en el log\n",
    "        if mask_varhmin.any():\n",
    "            aligned_mask_varmin = mask_varhmin.reindex(mask_sun2.index, fill_value=False)\n",
    "            for index, row in mask_sun2[aligned_mask_varmin].iterrows():\n",
    "                self.logger.info('File %s - Row %s - failed val p_varmin: %s', archivo, index, row['Valor'])\n",
    "        else:\n",
    "            self.logger.info('File: %s - No se encontraron fallos en p_varmin', archivo)\n",
    "        \n",
    "        mask_sun2.loc[:, 'Estado'] = mask_sun2['Estado'].fillna('')\n",
    "        # Lógica de etiquetado para 'Estado', sospechoso, '0PSO0'\n",
    "        condicion_0PSO0 = mask_varhmin & ((mask_sun2['Estado'] == ''))\n",
    "        mask_sun2.loc[condicion_0PSO0, 'Estado'] = '0PSO0'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO0\n",
    "        # 0PSO1\n",
    "        condicion_0PSO1 = mask_varhmin & (mask_sun2['Estado'] == '0PSO0')\n",
    "        mask_sun2.loc[condicion_0PSO1, 'Estado'] = '0PSO1'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO1\n",
    "        # 0PSO2\n",
    "        condicion_0PSO2 = mask_varhmin & (mask_sun2['Estado'] == '0PSO1')\n",
    "        mask_sun2.loc[condicion_0PSO2, 'Estado'] = '0PSO2'\n",
    "        mask_varhmin = mask_varhmin & ~condicion_0PSO2\n",
    "        # 0PSO2\n",
    "        condicion_0PSO3 = mask_varhmin & (mask_sun2['Estado'] == '0PSO2')\n",
    "        mask_sun2.loc[condicion_0PSO3, 'Estado'] = '0PSO3'\n",
    "    \n",
    "        # Copiar datos de chunk_jmp al chunk original\n",
    "        chunk.loc[mask_sun2.index] = mask_sun2\n",
    "    \n",
    "        # Eliminar las columnas temporales antes de devolver el chunk\n",
    "        columns_to_drop = ['Delta', 'Fecha_anterior', 'Delta_tiempo', 'Fecha_hora']\n",
    "        columns_to_drop = [col for col in columns_to_drop if col in chunk.columns]\n",
    "        chunk.drop(columns=columns_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "        return chunk\n",
    "\n",
    "    def procesar_archivos(self, funcion_evaluacion):\n",
    "        '''Este método procesa la lectura y guardado de los archivos para todas las pruebas'''\n",
    "        archivos = self.ruta_archivos\n",
    "\n",
    "        archivos_salida = []  # Lista para almacenar nombres de archivos de salida\n",
    "\n",
    "        # Se recorre cada archivo en la carpeta\n",
    "        for archivo in archivos:\n",
    "            if archivo.endswith('.csv'):\n",
    "                ruta_archivo = os.path.join(self.dir_files, archivo)\n",
    "\n",
    "                reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=self.chunk_size)#,dtype={7: 'str'}, low_memory=False)\n",
    "                resultados = []\n",
    "\n",
    "                for chunk in reader:\n",
    "                    try:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                    except ValueError:\n",
    "                        chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                    chunk['Station'] = chunk['Station'].astype('int64')\n",
    "\n",
    "                    # try:\n",
    "                    #     chunk_resultado, _ = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    # except ValueError:\n",
    "                    chunk_resultado = funcion_evaluacion(chunk, archivo)  # Desempaqueta solo el DataFrame\n",
    "                    resultados.append(chunk_resultado)\n",
    "\n",
    "                if not resultados:  # Se verifica si la lista está vacía\n",
    "                    self.logger.warning('No hay resultados válidos para concatenar en el archivo %s. Continuando con el siguiente.', archivo)\n",
    "                    continue\n",
    "                    \n",
    "                resultados_consolidados = pd.concat(resultados)\n",
    "\n",
    "                # Genera el nombre del archivo de salida conservando los primeros 19 caracteres del nombre del archivo original\n",
    "                nombre_archivo_salida = archivo[:19] + '_qc.csv'\n",
    "\n",
    "                resultados_consolidados.to_csv(os.path.join(self.dir_files, nombre_archivo_salida), encoding='latin-1', index=False)\n",
    "\n",
    "                archivos_salida.append(nombre_archivo_salida)  # Agregar el nombre del archivo a la lista\n",
    "            \n",
    "        # Actualiza self.ruta_archivos para que la próxima prueba procese los resultados de esta prueba\n",
    "        self.ruta_archivos = archivos_salida\n",
    "        # Se fija el log de procesamiento completo de archivos\n",
    "        self.logger.info('Procesamiento completo de archivos de estaciones HR. Archivos generados: %s', archivos_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "58f92b9d-0c3a-4652-aa4b-f41a66ab8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador = AutomatTS10cmEMA('RawUnmodified_TS-10cm') #Test_QC/T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4348e0-435a-4f34-ad20-e1d5d8da9827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_transm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d2d470b-c286-4289-90e5-8d9148c65a78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_13452\\3458943399.py:795: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_estruc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7c17071-31e8-4484-9ae9-7ce1af648709",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_13452\\3458943399.py:795: DtypeWarning: Columns (6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    }
   ],
   "source": [
    "procesador.procesar_archivos(procesador.p_limrig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ea10b68-a8cb-4b67-b977-65d83029a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_perst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "5423c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Analizar presencia de '0PSO0'\n",
    "# Cambia este directorio al lugar donde tengas tu carpeta 'ReadytoCassandraFiles'\n",
    "directorio = 'RawUnmodified_TS-10cm' #r'Test_QC/10T'\n",
    "# Lista para almacenar los nombres de los archivos que contienen '0PSO1' en la columna 'Estado'\n",
    "cantdatos_0PER = []\n",
    "\n",
    "# Itera sobre cada archivo en el directorio\n",
    "for archivo in os.listdir(directorio):\n",
    "    if archivo.endswith('_qc.csv'):\n",
    "        # Construye la ruta completa al archivo\n",
    "        ruta_archivo = os.path.join(directorio, archivo)\n",
    "        # Especifica los tipos de dato para las columnas deseadas\n",
    "        tipos_de_dato = {'Estado': str, 'Estado_Anterior': str}\n",
    "        # Lee el archivo CSV en un DataFrame de pandas\n",
    "        df = pd.read_csv(ruta_archivo, encoding='latin-1',dtype=tipos_de_dato)\n",
    "        # Checa cuántos '0PER0' hay en la columna 'Estado'\n",
    "        count_0PER0 = df['Estado'].value_counts().get('0PER0', 0)\n",
    "        # Guarda el nombre del archivo y la cantidad de datos con '0PER0'\n",
    "        cantdatos_0PER.append((archivo, count_0PER0))\n",
    "\n",
    "# Imprime la cantidad de datos con '0PER0' en cada archivo\n",
    "#for archivo, count in cantdatos_0PER:\n",
    "    #print(f\"Archivo: {archivo}, Datos con '0PER0': {count}\")\n",
    "\n",
    "# Crear un DataFrame a partir de los resultados\n",
    "df_resultados = pd.DataFrame(cantdatos_0PER, columns=['Archivo', 'Cantidad_0PER0'])\n",
    "# Guardar resultados\n",
    "prueba = 'perst'\n",
    "df_resultados.to_csv(f'cant_{prueba}_8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a4b8d2a-8da3-4629-8884-bdc299e12d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_jump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcc6e5-8bee-4f94-9d04-14d4dd4d97e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_valvmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca86b59-71b3-4471-87b3-150eb60eae51",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_sigmRP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec77df-8447-4158-bbe9-c98ad4e7e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d3f1d7-4219-42d7-b574-e61723082392",
   "metadata": {},
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_coherPNv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1add052-3700-456b-b350-ffbb7610289f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "procesador.procesar_archivos(procesador.p_varmin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f449b253-9dcb-4b22-896d-a898101a2cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffae449-bdf5-43d3-8a79-d968c7460a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9559f9e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
