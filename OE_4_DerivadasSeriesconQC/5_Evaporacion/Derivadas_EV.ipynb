{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad644f7",
   "metadata": {},
   "source": [
    "# Cálculo de derivadas evaporación y graficación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a268314",
   "metadata": {},
   "source": [
    "> Elaborado por Paola Álvarez, profesional contratista IDEAM, contrato 196 de 2024. Comentarios o inquietudes, remitir a *palvarez@ideam.gov.co* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc197bfa",
   "metadata": {},
   "source": [
    "**Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a06ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import statistics\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import gc\n",
    "import calendar\n",
    "from collections import deque\n",
    "from datetime import timedelta\n",
    "from scipy import stats\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.chart import LineChart, Reference\n",
    "from openpyxl.chart import ScatterChart, Reference, Series\n",
    "from openpyxl.chart import BarChart, Reference\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5dbca0",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0b13d-32b9-4249-8f67-b9579629c775",
   "metadata": {},
   "source": [
    "### Pruebas unitarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27ed7b",
   "metadata": {},
   "source": [
    "#### Datos con QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8cdb286-46d3-49de-8050-56a678c42f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar frecuencias\n",
    "def process_frequencies(df, columna_fecha, freq_csv_path, porc_min=0.7):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "    # Cargar el archivo de frecuencias\n",
    "    freqinst200b = pd.read_csv(freq_csv_path, encoding='latin-1', sep=';')\n",
    "\n",
    "    # Definir el diccionario de frecuencias y cantidades esperadas\n",
    "    frecuencias = {\n",
    "        'min': {'cant_esperd_h': 60, 'cant_esperd_d': 1440, 'cant_esperd_m': 43200, 'cant_esperd_a': 518400, 'minutos': 1},\n",
    "        '2min': {'cant_esperd_h': 30, 'cant_esperd_d': 720, 'cant_esperd_m': 21600, 'cant_esperd_a': 259200, 'minutos': 2},        \n",
    "        '5min': {'cant_esperd_h': 12, 'cant_esperd_d': 288, 'cant_esperd_m': 8640, 'cant_esperd_a': 103680, 'minutos': 5},\n",
    "        '10min': {'cant_esperd_h': 6, 'cant_esperd_d': 144, 'cant_esperd_m': 4320, 'cant_esperd_a': 51840, 'minutos': 10},\n",
    "        'h': {'cant_esperd_h': 1, 'cant_esperd_d': 24, 'cant_esperd_m': 720, 'cant_esperd_a': 8640}\n",
    "    }\n",
    "\n",
    "    # Obtener el valor de la estación\n",
    "    station_value = df['Station'].values[0]\n",
    "    freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == station_value]\n",
    "    periodos = freqinst200b_station['FreqInf'].values[0]\n",
    "    print(periodos)\n",
    "\n",
    "    if pd.isna(periodos):\n",
    "        try:\n",
    "            periodos = pd.infer_freq(df[columna_fecha][-25:])\n",
    "            print(periodos)\n",
    "            if periodos is None:\n",
    "                project_value = freqinst100b_station['Instituc'].values[0]\n",
    "                periodos = {'CENICAFE': '5min', 'IDEAM': '10min', 'CAR': 'h', 'IDIGER': 'min'}.get(project_value, 'min')\n",
    "                print(f\"Frecuencia inferida para {df['Station']} es None. Se determina según entidad {project_value}\")\n",
    "                \n",
    "                #return None, None, None, None, None, None\n",
    "        except ValueError as e:\n",
    "            print(f'Error al inferir la frecuencia en el archivo {df}: {str(e)}')\n",
    "            return None, None, None, None, None, None\n",
    "\n",
    "    # Obtener las cantidades esperadas y el offset en minutos\n",
    "    cant_esperd_h = frecuencias[periodos]['cant_esperd_h']\n",
    "    cant_esperd_d = frecuencias[periodos]['cant_esperd_d']\n",
    "    cant_esperd_m = frecuencias[periodos]['cant_esperd_m']\n",
    "    cant_esperd_a = frecuencias[periodos]['cant_esperd_a']\n",
    "    if periodos == 'h':\n",
    "        pass\n",
    "    else:\n",
    "        minutoffset = frecuencias[periodos]['minutos']\n",
    "\n",
    "    # Ajustar la hora de cada registro\n",
    "    df_c = df.copy()\n",
    "    if periodos == 'h':\n",
    "        df_c[columna_fecha] = df_c[columna_fecha] #- pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df_c[columna_fecha] = df_c[columna_fecha] - pd.Timedelta(minutes=minutoffset)\n",
    "\n",
    "    # Establecer la columna de fecha como índice\n",
    "    df_c.set_index(columna_fecha, inplace=True)\n",
    "\n",
    "    # Función para verificar si un día, mes o año tiene suficientes datos\n",
    "    def complet_hora(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_h * porc_min\n",
    "    \n",
    "    def complet_dia(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_d * porc_min\n",
    "\n",
    "    def complet_mes(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_m * porc_min\n",
    "\n",
    "    def complet_anio(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_a * porc_min\n",
    "\n",
    "    return (df_c, periodos, cant_esperd_h, cant_esperd_d, cant_esperd_m, cant_esperd_a,\n",
    "           complet_hora, complet_dia, complet_mes, complet_anio)\n",
    "    #complet_hora, complet_dia, complet_mes, complet_anio\n",
    "\n",
    "# Uso funciones de frecuencias\n",
    "df_example = pd.read_csv('../../OE_3_QC_Variables/5_Evaporacion/RawUnmodified_EV/Estacion_0021205970.csv', encoding='latin-1')#, dtype={'Estado_Anterior':str})\n",
    "if 'Estado' in df_example.columns:\n",
    "    df_example = df_example[~df_example['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PSO','0PAT','0PER']]))]\n",
    "    \n",
    "columna_fecha = 'Fecha'\n",
    "freq_csv_path = '../../OE_3_QC_Variables/5_Evaporacion/EMAEV_LatLonEntFreq.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c74e85e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h\n",
      "h\n"
     ]
    }
   ],
   "source": [
    "## V1, primero, cálculo de evaporación instantánea, luego\n",
    "## Derivadas diarias, mensuales y anuales\n",
    "# Evaporación\n",
    "df_example_c = df_example.copy()\n",
    "\n",
    "def EV_I(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "    \"\"\"\n",
    "    Calcula la evaporación en un tanque a partir de datos de nivel, considerando solo las disminuciones de nivel.\n",
    "    \"\"\"\n",
    "    # Asegurarnos de que la columna de fecha sea datetime\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "\n",
    "    # Ordenar por fecha por seguridad\n",
    "    df = df.sort_values(by=columna_fecha).reset_index(drop=True)\n",
    "   \n",
    "    # Calcular la diferencia entre niveles consecutivos\n",
    "    df['Diferencia'] = df[columna_valor].diff()\n",
    "    \n",
    "    # Filtrar solo las disminuciones (diferencias negativas)\n",
    "    df['Evaporacion'] = df['Diferencia'].where(df['Diferencia'] < 0, 0).abs()\n",
    "    \n",
    "    return df[['Station','Fecha','Evaporacion']] # evaporacion_total,\n",
    "\n",
    "# Se llama el archivo original según si es dato crudo o con qc\n",
    "ev_i = EV_I(df_example_c)\n",
    "ev_i_c = ev_i.copy()\n",
    "\n",
    "def EV_TT_H(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, periodos, cant_esperd_h, _, _, _, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "    \n",
    "    if periodos == 'H':\n",
    "        return df[['Station','Fecha','Evaporacion']]\n",
    "\n",
    "    if df_c is None or complet_hora is None:\n",
    "        return None\n",
    "\n",
    "    # Se filtran los datos que tienen la complititud mínima\n",
    "    df_filtrado = df_c.groupby([df_c.index.hour]).filter(complet_hora)\n",
    "    ev_tt_h = df_filtrado[[columna_valor]].resample('h').sum().round(4)\n",
    "    ev_tt_h[columna_valor] = ev_tt_h[columna_valor].where(\n",
    "        df_filtrado[columna_valor].resample('h').count() >= cant_esperd_h * porc_min,\n",
    "        other=float('nan'))\n",
    "\n",
    "    return ev_tt_h\n",
    "\n",
    "# Ejemplo de uso de la función\n",
    "ev_tt_h = EV_TT_H(ev_i)\n",
    "\n",
    "\n",
    "ev_tt_h_c = ev_tt_h.copy()\n",
    "def EV_TT_D(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Revisar resultado anterior\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "\n",
    "    # Establecer la columna de fecha como índice\n",
    "    df.set_index(columna_fecha, inplace=True)\n",
    "    cant_esperd_d = 24\n",
    "    def complet_dia(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_d * porc_min\n",
    "\n",
    "    # Luego de establecer el índice, aplicar resample\n",
    "    df_filtrado = df.groupby([df.index.date]).filter(complet_dia)\n",
    "    # Calcular la suma diaria de evaporación\n",
    "    ev_tt_d = df_filtrado[[columna_valor]].resample('D').sum().round(4)\n",
    "    # Aplicar filtro de completitud diaria\n",
    "    ev_tt_d[columna_valor] = ev_tt_d[columna_valor].where(\n",
    "        df_filtrado[columna_valor].resample('D').count() >= cant_esperd_d * porc_min,\n",
    "        other=float('nan'))\n",
    "    \n",
    "    return ev_tt_d\n",
    "\n",
    "ev_tt_d = EV_TT_D(ev_tt_h_c)\n",
    "\n",
    "ev_tt_d_c = ev_tt_d.copy()\n",
    "def EV_TT_M(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Revisar resultado anterior\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "\n",
    "    # Establecer la columna de fecha como índice\n",
    "    df.set_index(columna_fecha, inplace=True)\n",
    "\n",
    "    # Cantidad días mes\n",
    "    days_in_month = df.index.to_series().dt.days_in_month\n",
    "    days_in_month = days_in_month.resample('ME').first()\n",
    "    \n",
    "    # Función para verificar si una hora específica tiene suficientes datos\n",
    "    def complet_mes(sub_df):\n",
    "        mes = sub_df.index[0].month\n",
    "        total_esperado = days_in_month[days_in_month.index.month == mes].iloc[0]\n",
    "        return len(sub_df) >= total_esperado * porc_min\n",
    "\n",
    "    # Luego de establecer el índice, aplicar resample\n",
    "    df_filtrado = df.groupby([df.index.year, df.index.month]).filter(complet_mes)\n",
    "    # Calcular la suma diaria de evaporación\n",
    "    ev_tt_m = df_filtrado[[columna_valor]].resample('ME').sum().round(4)\n",
    "\n",
    "    # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "    counts = df.groupby([df.index.year, df.index.month])[columna_valor].count()\n",
    "    counts.index = pd.to_datetime(['{}-{}'.format(i[0], i[1]) for i in counts.index], format='%Y-%m')\n",
    "    counts = counts.resample('ME').sum()\n",
    "    ev_tt_m[columna_valor] = ev_tt_m[columna_valor].where(counts >= days_in_month * porc_min, other=float('nan'))\n",
    "    \n",
    "    return ev_tt_m\n",
    "\n",
    "ev_tt_m = EV_TT_M(ev_tt_d_c)\n",
    "\n",
    "\n",
    "ev_tt_m_c = ev_tt_m.copy()\n",
    "def EV_TT_A(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Revisar resultado anterior\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "   \n",
    "    # Establecer la columna de fecha como índice\n",
    "    df.set_index(columna_fecha, inplace=True)\n",
    "    \n",
    "    cant_esperd_a = 12\n",
    "    def complet_anio(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_a * porc_min\n",
    "\n",
    "    # Luego de establecer el índice, aplicar resample\n",
    "    df_filtrado = df.groupby([df.index.year]).filter(complet_anio)\n",
    "    # Calcular la suma diaria de evaporación\n",
    "    ev_tt_a = df_filtrado[[columna_valor]].resample('YE').sum().round(4)\n",
    "\n",
    "    # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "    counts = df.groupby(df.index.year)[columna_valor].count()\n",
    "    counts.index = pd.to_datetime(['{}-01'.format(i) for i in counts.index], format='%Y-%m')\n",
    "    counts = counts.resample('YE').sum()\n",
    "    ev_tt_a[columna_valor] = ev_tt_a[columna_valor].where(counts >= 12 * porc_min, other=float('nan'))\n",
    "\n",
    "    return ev_tt_a\n",
    "\n",
    "ev_tt_a = EV_TT_A(ev_tt_m_c)\n",
    "\n",
    "## Derivadas medias horarias diarias, mensuales y anuales\n",
    "# Evaporación media horaria\n",
    "def EV_MEDIA_H(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, periodos, cant_esperd_h, _, _, _, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "    \n",
    "    if periodos == 'H':\n",
    "        return df_c[['Station','Fecha','Evaporacion']]\n",
    "\n",
    "    if df_c is None or complet_hora is None:\n",
    "        return None\n",
    "\n",
    "    # Se filtran los datos que tienen la complititud mínima\n",
    "    df_filtrado = df_c.groupby([df_c.index.hour]).filter(complet_hora) #dfC.groupby([dfC.index.date]).filter(complet_dia)\n",
    "    ev_media_h = df_filtrado[[columna_valor]].resample('h').mean().round(4)\n",
    "\n",
    "    return ev_media_h\n",
    "\n",
    "ev_media_h = EV_MEDIA_H(ev_i_c)\n",
    "\n",
    "# Evaporación media diaria\n",
    "ev_media_h_c = ev_media_h.copy()\n",
    "def EV_MEDIA_D(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Revisar resultado anterior\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "\n",
    "    # Establecer la columna de fecha como índice\n",
    "    df.set_index(columna_fecha, inplace=True)\n",
    "    cant_esperd_d = 24\n",
    "    def complet_dia(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_d * porc_min\n",
    "\n",
    "    # Luego de establecer el índice, aplicar resample\n",
    "    df_filtrado = df.groupby([df.index.date]).filter(complet_dia)\n",
    "    # Calcular la suma diaria de evaporación\n",
    "    ev_media_d = df_filtrado[[columna_valor]].resample('D').mean().round(4)\n",
    "    # Aplicar filtro de completitud diaria\n",
    "    ev_media_d[columna_valor] = ev_media_d[columna_valor].where(\n",
    "        df_filtrado[columna_valor].resample('D').count() >= cant_esperd_d * porc_min,\n",
    "        other=float('nan'))\n",
    "\n",
    "    return ev_media_d\n",
    "\n",
    "ev_media_d = EV_MEDIA_D(ev_media_h_c)\n",
    "\n",
    "# Evaporación media mensual\n",
    "ev_media_d_c = ev_media_d.copy()\n",
    "def EV_MEDIA_M(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Revisar resultado anterior\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "\n",
    "    # Establecer la columna de fecha como índice\n",
    "    df.set_index(columna_fecha, inplace=True)\n",
    "\n",
    "    # Cantidad días mes\n",
    "    days_in_month = df.index.to_series().dt.days_in_month\n",
    "    days_in_month = days_in_month.resample('ME').first()\n",
    "    \n",
    "    # Función para verificar si una hora específica tiene suficientes datos\n",
    "    def complet_mes(sub_df):\n",
    "        mes = sub_df.index[0].month\n",
    "        total_esperado = days_in_month[days_in_month.index.month == mes].iloc[0]\n",
    "        return len(sub_df) >= total_esperado * porc_min\n",
    "\n",
    "    # Luego de establecer el índice, aplicar resample\n",
    "    df_filtrado = df.groupby([df.index.year, df.index.month]).filter(complet_mes)\n",
    "    # Calcular la suma diaria de evaporación\n",
    "    ev_media_m = df_filtrado[[columna_valor]].resample('ME').mean().round(4)\n",
    "\n",
    "    # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "    counts = df.groupby([df.index.year, df.index.month])[columna_valor].count()\n",
    "    counts.index = pd.to_datetime(['{}-{}'.format(i[0], i[1]) for i in counts.index], format='%Y-%m')\n",
    "    counts = counts.resample('ME').sum()\n",
    "    ev_media_m[columna_valor] = ev_media_m[columna_valor].where(counts >= days_in_month * porc_min, other=float('nan'))\n",
    "    \n",
    "    return ev_media_m\n",
    "\n",
    "ev_media_m = EV_MEDIA_M(ev_media_d_c)\n",
    "\n",
    "# Evaporación media anual\n",
    "ev_media_m_c = ev_media_m.copy()\n",
    "def EV_MEDIA_A(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Revisar resultado anterior\n",
    "    if df.empty:\n",
    "        return df\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "   \n",
    "    # Establecer la columna de fecha como índice\n",
    "    df.set_index(columna_fecha, inplace=True)\n",
    "    \n",
    "    cant_esperd_a = 12\n",
    "    def complet_anio(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_a * porc_min\n",
    "\n",
    "    # Luego de establecer el índice, aplicar resample\n",
    "    df_filtrado = df.groupby([df.index.year]).filter(complet_anio)\n",
    "    # Calcular la suma diaria de evaporación\n",
    "    ev_media_a = df_filtrado[[columna_valor]].resample('YE').mean().round(4)\n",
    "\n",
    "    # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "    counts = df.groupby(df.index.year)[columna_valor].count()\n",
    "    counts.index = pd.to_datetime(['{}-01'.format(i) for i in counts.index], format='%Y-%m')\n",
    "    counts = counts.resample('YE').sum()\n",
    "    ev_media_a[columna_valor] = ev_media_a[columna_valor].where(counts >= 12 * porc_min, other=float('nan'))\n",
    "\n",
    "    return ev_media_a\n",
    "\n",
    "ev_media_a = EV_MEDIA_A(ev_media_m_c)\n",
    "\n",
    "# ###-- Derivados máximos y mínimos\n",
    "# #Humedad relativa del aire a 10 cm  mínima diaria\n",
    "# df_example_c = df_example.copy()\n",
    "# def EV_MEDIA_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "#     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "#         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "#     # Se llama la función de procesamiento de frecuencias\n",
    "#     df_c, complet_dia, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "#     if df_c is None or complet_dia is None:\n",
    "#         return None\n",
    "\n",
    "#     # Filtrar los días que tienen suficientes datos\n",
    "#     df_c['Fecha_temp'] = df_c.index\n",
    "#     dias_validos = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_dia).index\n",
    "    \n",
    "#     # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "#     dias_validos = pd.to_datetime(dias_validos).normalize()\n",
    "#     df_filtrado = df_c[df_c.index.normalize().isin(dias_validos)]\n",
    "    \n",
    "#     # Encontrar el valor mínimo por cada día válido\n",
    "#     idx_minimos_dia = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmin()\n",
    "#     mn_d = df_filtrado.loc[idx_minimos_dia]\n",
    "    \n",
    "#     # Eliminar la columna temporal antes de retornar el resultado\n",
    "#     if 'Fecha_temp' in mn_d.columns:\n",
    "#         mn_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "    \n",
    "#     return mn_d[[columna_valor]]\n",
    "\n",
    "# df_mn_d = HRS30_MN_D(df_example_c)\n",
    "\n",
    "# # Humedad relativa del aire a 10 cm  máxima diaria\n",
    "# df_example_c = df_example.copy()\n",
    "# def HRS30_MX_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "#     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "#         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "#     # Se llama la función de procesamiento de frecuencias\n",
    "#     df_c, complet_dia, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "#     if df_c is None or complet_dia is None:\n",
    "#         return None\n",
    "\n",
    "#     # Filtrar los días que tienen suficientes datos\n",
    "#     df_c['Fecha_temp'] = df_c.index\n",
    "#     dias_validos = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_dia).index\n",
    "#     # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "#     dias_validos = pd.to_datetime(dias_validos).normalize()\n",
    "#     df_filtrado = df_c[df_c.index.normalize().isin(dias_validos)]\n",
    "#     # Encontrar el valor mínimo por cada día válido\n",
    "#     idx_maximos_dia = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmax()\n",
    "#     mx_d = df_filtrado.loc[idx_maximos_dia]\n",
    "    \n",
    "#     # Eliminar la columna temporal antes de retornar el resultado\n",
    "#     if 'Fecha_temp' in mx_d.columns:\n",
    "#         mx_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "    \n",
    "#     return mx_d[[columna_valor]]\n",
    "\n",
    "# df_mx_d = HRS30_MX_D(df_example_c)\n",
    "\n",
    "# # Humedad relativa del aire a 10 cm  mínima mensual\n",
    "# df_example_c = df_example.copy()\n",
    "# def HRS30_MN_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "#     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "#         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "#     # Se llama la función de procesamiento de frecuencias\n",
    "#     df_c, _, complet_mes, _ = process_frequencies(df, columna_fecha, freq_csv_path,porc_min)\n",
    "\n",
    "#     if df_c is None or complet_mes is None:\n",
    "#         return None\n",
    "\n",
    "#     # Filtrar los meses que tienen suficientes datos\n",
    "#     df_c['Fecha_temp'] = df_c.index\n",
    "#     # Eliminar duplicados en el índice\n",
    "#     df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "#     meses_validos = df_c.groupby(df_c.index.to_period('M')).filter(complet_mes).index.to_period('M')\n",
    "\n",
    "#     # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "#     df_filtrado = df_c[df_c.index.to_period('M').isin(meses_validos)]\n",
    "\n",
    "#     # Encontrar el valor mínimo por cada mes válido\n",
    "#     idx_minimos_mes = df_filtrado.groupby(df_filtrado.index.to_period('M'))[columna_valor].idxmin()\n",
    "#     min_m = df_filtrado.loc[idx_minimos_mes]\n",
    "\n",
    "#     # Eliminar las columnas temporales antes de retornar el resultado\n",
    "#     if 'Fecha_temp' in min_m.columns:\n",
    "#         min_m.drop(columns=['Fecha_temp'], inplace=True)\n",
    "\n",
    "#     return min_m[[columna_valor]]\n",
    "    \n",
    "# df_mn_m = HRS30_MN_M(df_example_c)\n",
    "\n",
    "# # Humedad relativa del aire a 10 cm  máxima mensual\n",
    "# df_example_c = df_example.copy()\n",
    "# def HRS30_MX_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "#     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "#         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "    \n",
    "#     # Se llama la función de procesamiento de frecuencias\n",
    "#     df_c, _, complet_mes, _ = process_frequencies(df, columna_fecha, freq_csv_path,porc_min)\n",
    "\n",
    "#     if df_c is None or complet_mes is None:\n",
    "#         return None\n",
    "\n",
    "#     # Filtrar los meses que tienen suficientes datos\n",
    "#     df_c['Fecha_temp'] = df_c.index\n",
    "#     # Eliminar duplicados en el índice\n",
    "#     df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "#     meses_validos = df_c.groupby(df_c.index.to_period('M')).filter(complet_mes).index.to_period('M')\n",
    "\n",
    "#     # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "#     df_filtrado = df_c[df_c.index.to_period('M').isin(meses_validos)]\n",
    "\n",
    "#     # Encontrar el valor mínimo por cada mes válido\n",
    "#     idx_maximos_mes = df_filtrado.groupby(df_filtrado.index.to_period('M'))[columna_valor].idxmax()\n",
    "#     max_m = df_filtrado.loc[idx_maximos_mes]\n",
    "\n",
    "#     # Eliminar las columnas temporales antes de retornar el resultado\n",
    "#     if 'Fecha_temp' in max_m.columns:\n",
    "#         max_m.drop(columns=['Fecha_temp'], inplace=True)\n",
    "\n",
    "#     return max_m[[columna_valor]]\n",
    "    \n",
    "# df_mx_m = HRS30_MX_M(df_example_c)\n",
    "\n",
    "# # Humedad relativa del aire a 10 cm mínima anual\n",
    "# df_example_c = df_example.copy()\n",
    "# def HRS30_MN_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "#     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "#         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "#     # Se llama la función de procesamiento de frecuencias\n",
    "#     df_c, _, _, complet_anio = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "#     if df_c is None or complet_anio is None:\n",
    "#         return None\n",
    "\n",
    "#     # Filtrar los meses que tienen suficientes datos\n",
    "#     df_c['Fecha_temp'] = df_c.index\n",
    "#     # Eliminar duplicados en el índice\n",
    "#     df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "#     anios_validos = df_c.groupby(df_c.index.to_period('A')).filter(complet_anio).index.to_period('A')\n",
    "\n",
    "#     # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "#     df_filtrado = df_c[df_c.index.to_period('A').isin(anios_validos)]\n",
    "\n",
    "#     # Encontrar el valor mínimo por cada mes válido\n",
    "#     idx_minimos_anio = df_filtrado.groupby(df_filtrado.index.to_period('A'))[columna_valor].idxmin()\n",
    "#     min_a = df_filtrado.loc[idx_minimos_anio]\n",
    "\n",
    "#     # Eliminar las columnas temporales antes de retornar el resultado\n",
    "#     if 'Fecha_temp' in min_a.columns:\n",
    "#         min_a.drop(columns=['Fecha_temp'], inplace=True)\n",
    "\n",
    "#     return min_a[[columna_valor]]\n",
    "    \n",
    "# df_mn_a = HRS30_MN_A(df_example_c)\n",
    "\n",
    "# # Humedad relativa del aire a 10 cm  máxima anual\n",
    "# df_example_c = df_example.copy()\n",
    "# def HRS30_MX_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "#     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "#     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "#         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "#     # Se llama la función de procesamiento de frecuencias\n",
    "#     df_c, _, _, complet_anio = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "#     if df_c is None or complet_anio is None:\n",
    "#         return None\n",
    "\n",
    "#     # Filtrar los meses que tienen suficientes datos\n",
    "#     df_c['Fecha_temp'] = df_c.index\n",
    "#     # Eliminar duplicados en el índice\n",
    "#     df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "#     anios_validos = df_c.groupby(df_c.index.to_period('A')).filter(complet_anio).index.to_period('A')\n",
    "\n",
    "#     # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "#     df_filtrado = df_c[df_c.index.to_period('A').isin(anios_validos)]\n",
    "\n",
    "#     # Encontrar el valor mínimo por cada mes válido\n",
    "#     idx_minimos_anio = df_filtrado.groupby(df_filtrado.index.to_period('A'))[columna_valor].idxmax()\n",
    "#     max_a = df_filtrado.loc[idx_minimos_anio]\n",
    "\n",
    "#     # Eliminar las columnas temporales antes de retornar el resultado\n",
    "#     if 'Fecha_temp' in max_a.columns:\n",
    "#         max_a.drop(columns=['Fecha_temp'], inplace=True)\n",
    "\n",
    "#     return max_a[[columna_valor]]\n",
    "    \n",
    "# df_mx_a = HRS30_MN_A(df_example_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d69c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Gráficas y export en excel\n",
    "# Se crea un nuevo archivo Excel con openpyxl\n",
    "wb = Workbook()\n",
    "sheets = {\n",
    "    'EV_I': ev_i, 'EV_TT_H': ev_tt_h,\n",
    "    'EV_TT_D': ev_tt_d, 'EV_TT_M': ev_tt_m,\n",
    "    'EV_TT_A': ev_tt_a, 'EV_MEDIA_H': ev_media_h,\n",
    "    'EV_MEDIA_D': ev_media_d, 'EV_MEDIA_M': ev_media_m,\n",
    "    'EV_MEDIA_A': ev_media_a,\n",
    "    #'HRS30_MN_M': df_mn_m,\n",
    "    #'HRS30_MX_M': df_mx_m, 'HRS30_MN_A': df_mn_a,\n",
    "    #'HRS30_MX_A': df_mx_a \n",
    "}\n",
    "\n",
    "# Si el workbook todavía tiene la hoja por defecto, se elimina\n",
    "if \"Sheet\" in wb.sheetnames:\n",
    "    del wb[\"Sheet\"]\n",
    "\n",
    "for sheet_name, data in sheets.items():\n",
    "    ws = wb.create_sheet(title=sheet_name)\n",
    "    \n",
    "    # Agregamos los datos al Excel\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(data, index=True, header=True), 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "    \n",
    "    # Crear una gráfica\n",
    "    chart = LineChart()\n",
    "    chart.title = sheet_name\n",
    "    chart.style = 5\n",
    "    chart.y_axis.title = 'Evaporación (mm)'\n",
    "    chart.x_axis.title = 'Fecha'\n",
    "    \n",
    "    # Establecer datos para la gráfica\n",
    "    max_row = ws.max_row\n",
    "    values = Reference(ws, min_col=2, min_row=2, max_col=2, max_row=max_row)\n",
    "    dates = Reference(ws, min_col=1, min_row=3, max_col=1, max_row=max_row)\n",
    "    chart.add_data(values, titles_from_data=True)\n",
    "    chart.set_categories(dates)\n",
    "    \n",
    "    # Quitar la leyenda\n",
    "    chart.legend = None\n",
    "\n",
    "    # Cambiar el grosor de la línea a 0.5 puntos (equivalente a 50 centésimas de punto)\n",
    "    for series in chart.series:\n",
    "        series.graphicalProperties.line.width = 50\n",
    "        series.graphicalProperties.line.solidFill = \"3498db\"  # Marrón\n",
    "        \n",
    "    # Posicionar la gráfica en el Excel\n",
    "    ws.add_chart(chart, \"E3\")\n",
    "\n",
    "# Guardar el archivo Excel\n",
    "wb.save(\"Agreg_graf_EV_AUT.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4c130",
   "metadata": {},
   "source": [
    "## Función para cálculo masivo de derivadas y generación de gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfd6f3d-f60d-4d27-b653-7eb769b203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar frecuencias\n",
    "def process_frequencies(df, columna_fecha, freq_csv_path, porc_min=0.7):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "    # Cargar el archivo de frecuencias\n",
    "    freqinst200b = pd.read_csv(freq_csv_path, encoding='latin-1', sep=';')\n",
    "\n",
    "    # Definir el diccionario de frecuencias y cantidades esperadas\n",
    "    frecuencias = {\n",
    "        'min': {'cant_esperd_h': 60, 'cant_esperd_d': 1440, 'cant_esperd_m': 43200, 'cant_esperd_a': 518400, 'minutos': 1},\n",
    "        '2min': {'cant_esperd_h': 30, 'cant_esperd_d': 720, 'cant_esperd_m': 21600, 'cant_esperd_a': 259200, 'minutos': 2},\n",
    "        '5min': {'cant_esperd_h': 12, 'cant_esperd_d': 288, 'cant_esperd_m': 8640, 'cant_esperd_a': 103680, 'minutos': 5},\n",
    "        '10min': {'cant_esperd_h': 6, 'cant_esperd_d': 144, 'cant_esperd_m': 4320, 'cant_esperd_a': 51840, 'minutos': 10},\n",
    "        'h': {'cant_esperd_h': 1, 'cant_esperd_d': 24, 'cant_esperd_m': 720, 'cant_esperd_a': 8640}\n",
    "    }\n",
    "\n",
    "    # Obtener el valor de la estación\n",
    "    station_value = df['station'].values[0] # Station\n",
    "    freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == station_value]\n",
    "    periodos = freqinst200b_station['FreqInf'].values[0]\n",
    "\n",
    "    if pd.isna(periodos):\n",
    "        try:\n",
    "            periodos = pd.infer_freq(df[columna_fecha][-25:])\n",
    "            print(periodos)\n",
    "            if periodos is None:\n",
    "                project_value = freqinst100b_station['Instituc'].values[0]\n",
    "                periodos = {'CENICAFE': '5min', 'IDEAM': '10min', 'CAR': 'h', 'IDIGER': 'min'}.get(project_value, 'min')\n",
    "                print(f\"Frecuencia inferida para {df['station']} es None. Se determina según entidad {project_value}\") #Station\n",
    "                \n",
    "                #return None, None, None, None, None, None\n",
    "        except ValueError as e:\n",
    "            print(f'Error al inferir la frecuencia en el archivo {df}: {str(e)}')\n",
    "            return None, None, None, None, None, None\n",
    "\n",
    "    # Obtener las cantidades esperadas y el offset en minutos\n",
    "    cant_esperd_h = frecuencias[periodos]['cant_esperd_h']\n",
    "    cant_esperd_d = frecuencias[periodos]['cant_esperd_d']\n",
    "    cant_esperd_m = frecuencias[periodos]['cant_esperd_m']\n",
    "    cant_esperd_a = frecuencias[periodos]['cant_esperd_a']\n",
    "    if periodos == 'h':\n",
    "        pass\n",
    "    else:\n",
    "        minutoffset = frecuencias[periodos]['minutos']\n",
    "\n",
    "    # Ajustar la hora de cada registro\n",
    "    df_c = df.copy()\n",
    "    if periodos == 'h':\n",
    "        df_c[columna_fecha] = df_c[columna_fecha] #- pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df_c[columna_fecha] = df_c[columna_fecha] - pd.Timedelta(minutes=minutoffset)\n",
    "\n",
    "    # Establecer la columna de fecha como índice\n",
    "    df_c.set_index(columna_fecha, inplace=True)\n",
    "\n",
    "    # Función para verificar si un día, mes o año tiene suficientes datos\n",
    "    def complet_hora(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_h * porc_min\n",
    "    \n",
    "    def complet_dia(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_d * porc_min\n",
    "\n",
    "    def complet_mes(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_m * porc_min\n",
    "\n",
    "    def complet_anio(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_a * porc_min\n",
    "\n",
    "    return df_c, periodos, cant_esperd_h, cant_esperd_d, cant_esperd_m, cant_esperd_a, complet_hora, complet_dia, complet_mes, complet_anio\n",
    "\n",
    "# # Uso funciones de frecuencias\n",
    "# df_example = pd.read_csv('../../OE_3_QC_Variables/2_HumedadRelativa/Test_QC/Estacion_0011115501.csv', encoding='latin-1')#, dtype={'Estado_Anterior':str})\n",
    "# if 'Estado' in df_example.columns:\n",
    "#     df_example = df_example[df_example['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PC']]))]    \n",
    "columna_fecha = 'event_time' #Fecha\n",
    "freq_csv_path = '../../OE_3_QC_Variables/5_Evaporacion/EMAEV_LatLonEntFreq.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb23a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cálculo de derivadas de varios archivos en una sola carpeta\n",
    "def calc_deriv_EV(carpeta, chunk_size=540000):\n",
    "    archivos = os.listdir(carpeta)\n",
    "\n",
    "    # Se recorre cada archivo en la carpeta\n",
    "    for archivo in archivos:\n",
    "        if archivo.endswith('.csv'):\n",
    "            ruta_archivo = os.path.join(carpeta, archivo)\n",
    "        \n",
    "            # Se procesan los archivos csv por fragmentos\n",
    "            reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=chunk_size)\n",
    "            \n",
    "            for chunk in reader:\n",
    "                # Se generan dataframes analizados\n",
    "                # De cada chunk se transforma a datetime la serie/columna 'Fecha'\n",
    "                try:\n",
    "                    chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                except ValueError:\n",
    "                    chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    try:\n",
    "                        dfC = chunk[~chunk['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PSO','0PAT','0PER']]))]\n",
    "                        dfC_c = dfC.copy()\n",
    "                        station_value = dfC_c['Station'].values[0]\n",
    "                    except IndexError:\n",
    "                        print(f\"Error en el archivo {archivo}: dfC está vacío. Saltando al siguiente archivo.\")\n",
    "                        continue  # Sale del bucle de chunks y continúa con el siguiente archivo\n",
    "                else:\n",
    "                    chunk_c = chunk.copy()\n",
    "                    station_value = chunk_c['Station'].values[0]\n",
    "\n",
    "                # Cálculo de la evaporación instantánea\n",
    "                def EV_I(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                    \"\"\"\n",
    "                    Calcula la evaporación en un tanque a partir de datos de nivel, considerando solo las disminuciones de nivel.\n",
    "                    \"\"\"\n",
    "                    # Asegurarnos de que la columna de fecha sea datetime\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodos, cant_esperd_h, _, _, _, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "                    # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "                    if periodos.isalpha():\n",
    "                        periodos = '1' + periodos\n",
    "                        \n",
    "                    # Ordenar por fecha por seguridad\n",
    "                    df = df.sort_values(by=columna_fecha).reset_index(drop=True)\n",
    "                   \n",
    "                    # Calcular la diferencia entre niveles consecutivos\n",
    "                    df['Diferencia'] = df[columna_valor].diff()\n",
    "\n",
    "                    # Calcular el delta de tiempo entre fechas consecutivas\n",
    "                    df['Delta_Tiempo'] = df[columna_fecha].diff()\n",
    "                \n",
    "                    # Convertir la frecuencia esperada a un timedelta\n",
    "                    freq_timedelta = pd.to_timedelta(periodos)\n",
    "                    \n",
    "                    # Identificar valores no consecutivos\n",
    "                    df['Es_Consecutivo'] = df['Delta_Tiempo'] <= freq_timedelta\n",
    "                    \n",
    "                                        \n",
    "                    # Calcular evaporación solo para registros consecutivos con disminuciones\n",
    "                    df['Evaporacion'] = np.where(\n",
    "                        df['Es_Consecutivo'] & (df['Diferencia'] < 0),\n",
    "                        df['Diferencia'].abs(),\n",
    "                        np.nan\n",
    "                    )\n",
    "\n",
    "                    # Se filtran columnas relevantes para el resultado de mhmma según si es raw o qc\n",
    "                    if 'Estado' in df.columns:\n",
    "                        ev_i = df[['Station','Fecha','Evaporacion','Estado']]\n",
    "                    else:\n",
    "                        ev_i = df[['Station','Fecha','Evaporacion']]\n",
    "                    sttn = df['Station'][0]\n",
    "                    ev_i.to_csv(f'ev_i_{sttn}.csv')\n",
    "\n",
    "                    return df[['Station','Fecha','Evaporacion']] # evaporacion_total,\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    ev_i = EV_I(dfC_c)\n",
    "                else:\n",
    "                    ev_i = EV_I(chunk_c)\n",
    "\n",
    "                # Evaporación total horaria\n",
    "                ev_i_c = ev_i.copy()\n",
    "                def EV_TT_H(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodos, cant_esperd_h, _, _, _, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                    \n",
    "                    if periodos == 'H':\n",
    "                        return df_c[['Station','Fecha','Evaporacion']]\n",
    "                \n",
    "                    if df_c is None or complet_hora is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Se filtran los datos que tienen la complititud mínima\n",
    "                    df_filtrado = df_c.groupby([df_c.index.hour]).filter(complet_hora) #dfC.groupby([dfC.index.date]).filter(complet_dia)\n",
    "                    ev_tt_h = df_filtrado[[columna_valor]].resample('h').sum()\n",
    "                    ev_tt_h[columna_valor] = ev_tt_h[columna_valor].where(\n",
    "                        df_filtrado[columna_valor].resample('h').count() >= cant_esperd_h * porc_min,\n",
    "                        other=float('nan'))\n",
    "\n",
    "                    return ev_tt_h\n",
    "                \n",
    "                # Ejemplo de uso de la función\n",
    "                ev_tt_h = EV_TT_H(ev_i_c)\n",
    "                \n",
    "                # Evaporación total diaria\n",
    "                ev_tt_h_c = ev_tt_h.copy()\n",
    "                def EV_TT_D(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Revisar resultado anterior\n",
    "                    if df.empty:\n",
    "                        return df\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                \n",
    "                    # Establecer la columna de fecha como índice\n",
    "                    df.set_index(columna_fecha, inplace=True)\n",
    "                    cant_esperd_d = 24\n",
    "                    def complet_dia(sub_df):\n",
    "                        return len(sub_df) >= cant_esperd_d * porc_min\n",
    "                \n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df.groupby([df.index.date]).filter(complet_dia)\n",
    "                    # Calcular la suma diaria de evaporación\n",
    "                    ev_tt_d = df_filtrado[[columna_valor]].resample('D').sum().round(4)\n",
    "                    # Aplicar filtro de completitud diaria\n",
    "                    ev_tt_d[columna_valor] = ev_tt_d[columna_valor].where(\n",
    "                        df_filtrado[columna_valor].resample('D').count() >= cant_esperd_d * porc_min,\n",
    "                        other=float('nan'))\n",
    "                    \n",
    "                    return ev_tt_d\n",
    "                \n",
    "                ev_tt_d = EV_TT_D(ev_tt_h_c)\n",
    "\n",
    "                # Evaporación total mensual\n",
    "                ev_tt_d_c = ev_tt_d.copy()\n",
    "                def EV_TT_M(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Revisar resultado anterior\n",
    "                    if df.empty:\n",
    "                        return df\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                \n",
    "                    # Establecer la columna de fecha como índice\n",
    "                    df.set_index(columna_fecha, inplace=True)\n",
    "                \n",
    "                    # Cantidad días mes\n",
    "                    days_in_month = df.index.to_series().dt.days_in_month\n",
    "                    days_in_month = days_in_month.resample('ME').first()\n",
    "                    \n",
    "                    # Función para verificar si una hora específica tiene suficientes datos\n",
    "                    def complet_mes(sub_df):\n",
    "                        mes = sub_df.index[0].month\n",
    "                        total_esperado = days_in_month[days_in_month.index.month == mes].iloc[0]\n",
    "                        return len(sub_df) >= total_esperado * porc_min\n",
    "                \n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df.groupby([df.index.year, df.index.month]).filter(complet_mes)\n",
    "                    # Calcular la suma diaria de evaporación\n",
    "                    ev_tt_m = df_filtrado[[columna_valor]].resample('ME').sum().round(4)\n",
    "                \n",
    "                    # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "                    counts = df.groupby([df.index.year, df.index.month])[columna_valor].count()\n",
    "                    counts.index = pd.to_datetime(['{}-{}'.format(i[0], i[1]) for i in counts.index], format='%Y-%m')\n",
    "                    counts = counts.resample('ME').sum()\n",
    "                    ev_tt_m[columna_valor] = ev_tt_m[columna_valor].where(counts >= days_in_month * porc_min, other=float('nan'))\n",
    "                    \n",
    "                    return ev_tt_m\n",
    "                \n",
    "                ev_tt_m = EV_TT_M(ev_tt_d_c)\n",
    "                \n",
    "                # Evaporación total anual\n",
    "                ev_tt_m_c = ev_tt_m.copy()\n",
    "                def EV_TT_A(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Revisar resultado anterior\n",
    "                    if df.empty:\n",
    "                        return df\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                   \n",
    "                    # Establecer la columna de fecha como índice\n",
    "                    df.set_index(columna_fecha, inplace=True)\n",
    "                    \n",
    "                    cant_esperd_a = 12\n",
    "                    def complet_anio(sub_df):\n",
    "                        return len(sub_df) >= cant_esperd_a * porc_min\n",
    "                \n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df.groupby([df.index.year]).filter(complet_anio)\n",
    "                    # Calcular la suma diaria de evaporación\n",
    "                    ev_tt_a = df_filtrado[[columna_valor]].resample('YE').sum().round(4)\n",
    "                \n",
    "                    # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "                    counts = df.groupby(df.index.year)[columna_valor].count()\n",
    "                    counts.index = pd.to_datetime(['{}-01'.format(i) for i in counts.index], format='%Y-%m')\n",
    "                    counts = counts.resample('YE').sum()\n",
    "                    ev_tt_a[columna_valor] = ev_tt_a[columna_valor].where(counts >= 12 * porc_min, other=float('nan'))\n",
    "                \n",
    "                    return ev_tt_a\n",
    "                \n",
    "                ev_tt_a = EV_TT_A(ev_tt_m_c)\n",
    "                \n",
    "                ## Derivadas medias horarias diarias, mensuales y anuales\n",
    "                # Evaporación media horaria\n",
    "                def EV_MEDIA_H(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodos, cant_esperd_h, _, _, _, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                    \n",
    "                    if periodos == 'H':\n",
    "                        return df_c[['Station','Fecha','Evaporacion']]\n",
    "                \n",
    "                    if df_c is None or complet_hora is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Se filtran los datos que tienen la complititud mínima\n",
    "                    df_filtrado = df_c.groupby([df_c.index.hour]).filter(complet_hora) #dfC.groupby([dfC.index.date]).filter(complet_dia)\n",
    "                    ev_media_h = df_filtrado[[columna_valor]].resample('h').mean().round(4)\n",
    "                \n",
    "                    return ev_media_h\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    ev_media_h = EV_MEDIA_H(ev_i)\n",
    "                else:\n",
    "                    ev_media_h = EV_MEDIA_H(ev_i)\n",
    "                \n",
    "                # Evaporación media diaria\n",
    "                ev_media_h_c = ev_media_h.copy()\n",
    "                def EV_MEDIA_D(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Revisar resultado anterior\n",
    "                    if df.empty:\n",
    "                        return df\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                \n",
    "                    # Establecer la columna de fecha como índice\n",
    "                    df.set_index(columna_fecha, inplace=True)\n",
    "                    cant_esperd_d = 24\n",
    "                    def complet_dia(sub_df):\n",
    "                        return len(sub_df) >= cant_esperd_d * porc_min\n",
    "                \n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df.groupby([df.index.date]).filter(complet_dia)\n",
    "                    # Calcular la suma diaria de evaporación\n",
    "                    ev_media_d = df_filtrado[[columna_valor]].resample('D').mean().round(4)\n",
    "                    # Aplicar filtro de completitud diaria\n",
    "                    ev_media_d[columna_valor] = ev_media_d[columna_valor].where(\n",
    "                        df_filtrado[columna_valor].resample('D').count() >= cant_esperd_d * porc_min,\n",
    "                        other=float('nan'))\n",
    "                \n",
    "                    return ev_media_d\n",
    "                \n",
    "                ev_media_d = EV_MEDIA_D(ev_media_h_c)\n",
    "                \n",
    "                # Evaporación media mensual\n",
    "                ev_media_d_c = ev_media_d.copy()\n",
    "                def EV_MEDIA_M(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Revisar resultado anterior\n",
    "                    if df.empty:\n",
    "                        return df\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                \n",
    "                    # Establecer la columna de fecha como índice\n",
    "                    df.set_index(columna_fecha, inplace=True)\n",
    "                \n",
    "                    # Cantidad días mes\n",
    "                    days_in_month = df.index.to_series().dt.days_in_month\n",
    "                    days_in_month = days_in_month.resample('ME').first()\n",
    "                    \n",
    "                    # Función para verificar si una hora específica tiene suficientes datos\n",
    "                    def complet_mes(sub_df):\n",
    "                        mes = sub_df.index[0].month\n",
    "                        total_esperado = days_in_month[days_in_month.index.month == mes].iloc[0]\n",
    "                        return len(sub_df) >= total_esperado * porc_min\n",
    "                \n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df.groupby([df.index.year, df.index.month]).filter(complet_mes)\n",
    "                    # Calcular la suma diaria de evaporación\n",
    "                    ev_media_m = df_filtrado[[columna_valor]].resample('ME').mean().round(4)\n",
    "                \n",
    "                    # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "                    counts = df.groupby([df.index.year, df.index.month])[columna_valor].count()\n",
    "                    counts.index = pd.to_datetime(['{}-{}'.format(i[0], i[1]) for i in counts.index], format='%Y-%m')\n",
    "                    counts = counts.resample('ME').sum()\n",
    "                    ev_media_m[columna_valor] = ev_media_m[columna_valor].where(counts >= days_in_month * porc_min, other=float('nan'))\n",
    "                    \n",
    "                    return ev_media_m\n",
    "                \n",
    "                ev_media_m = EV_MEDIA_M(ev_media_d_c)\n",
    "                \n",
    "                # Evaporación media anual\n",
    "                ev_media_m_c = ev_media_m.copy()\n",
    "                def EV_MEDIA_A(df, columna_fecha='Fecha', columna_valor='Evaporacion', porc_min=0.7):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Revisar resultado anterior\n",
    "                    if df.empty:\n",
    "                        return df\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                   \n",
    "                    # Establecer la columna de fecha como índice\n",
    "                    df.set_index(columna_fecha, inplace=True)\n",
    "                    \n",
    "                    cant_esperd_a = 12\n",
    "                    def complet_anio(sub_df):\n",
    "                        return len(sub_df) >= cant_esperd_a * porc_min\n",
    "                \n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df.groupby([df.index.year]).filter(complet_anio)\n",
    "                    # Calcular la suma diaria de evaporación\n",
    "                    ev_media_a = df_filtrado[[columna_valor]].resample('YE').mean().round(4)\n",
    "                \n",
    "                    # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "                    counts = df.groupby(df.index.year)[columna_valor].count()\n",
    "                    counts.index = pd.to_datetime(['{}-01'.format(i) for i in counts.index], format='%Y-%m')\n",
    "                    counts = counts.resample('YE').sum()\n",
    "                    ev_media_a[columna_valor] = ev_media_a[columna_valor].where(counts >= 12 * porc_min, other=float('nan'))\n",
    "                \n",
    "                    return ev_media_a\n",
    "                \n",
    "                ev_media_a = EV_MEDIA_A(ev_media_m_c)\n",
    "                \n",
    "                # ###-- Derivados máximos y mínimos\n",
    "                # #Humedad relativa del aire a 10 cm  mínima diaria\n",
    "                # def HRS30_MN_H(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                #     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                #     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                #         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                #     # Se llama la función de procesamiento de frecuencias\n",
    "                #     df_c, periodo, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                #     if df_c is None or complet_hora is None:\n",
    "                #         return None\n",
    "                \n",
    "                #     # Filtrar los días que tienen suficientes datos\n",
    "                #     df_c['Fecha_temp'] = df_c.index\n",
    "                #     horas_validas = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_hora).index\n",
    "                    \n",
    "                #     # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "                #     horas_validas = pd.to_datetime(horas_validas).normalize()\n",
    "                #     df_filtrado = df_c[df_c.index.normalize().isin(horas_validas)]\n",
    "                    \n",
    "                #     # Encontrar el valor mínimo por cada día válido\n",
    "                #     idx_minimos_hora = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmin()\n",
    "                #     mn_d = df_filtrado.loc[idx_minimos_hora]\n",
    "                    \n",
    "                #     # Eliminar la columna temporal antes de retornar el resultado\n",
    "                #     if 'Fecha_temp' in mn_d.columns:\n",
    "                #         mn_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                    \n",
    "                #     return mn_d[[columna_valor]]\n",
    "\n",
    "                # # Se llama el archivo original según si es dato crudo o con qc\n",
    "                # if 'Estado' in chunk.columns:\n",
    "                #     df_mn_h = HRS30_MN_H(dfC_c)\n",
    "                # else:\n",
    "                #     df_mn_h = HRS30_MN_H(chunk_c)\n",
    "                    \n",
    "                # #Humedad relativa del aire a 10 cm  mínima diaria\n",
    "                # def HRS30_MX_H(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                #     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                #     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                #         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                #     # Se llama la función de procesamiento de frecuencias\n",
    "                #     df_c, periodo, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                #     if df_c is None or complet_hora is None:\n",
    "                #         return None\n",
    "                \n",
    "                #     # Filtrar los días que tienen suficientes datos\n",
    "                #     df_c['Fecha_temp'] = df_c.index\n",
    "                #     horas_validas = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_hora).index\n",
    "                    \n",
    "                #     # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "                #     horas_validas = pd.to_datetime(horas_validas).normalize()\n",
    "                #     df_filtrado = df_c[df_c.index.normalize().isin(horas_validas)]\n",
    "                    \n",
    "                #     # Encontrar el valor mínimo por cada día válido\n",
    "                #     idx_maximos_hora = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmax()\n",
    "                #     mx_h = df_filtrado.loc[idx_maximos_hora]\n",
    "                    \n",
    "                #     # Eliminar la columna temporal antes de retornar el resultado\n",
    "                #     if 'Fecha_temp' in mx_h.columns:\n",
    "                #         mx_h.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                    \n",
    "                #     return mx_h[[columna_valor]]\n",
    "\n",
    "                # # Se llama el archivo original según si es dato crudo o con qc\n",
    "                # if 'Estado' in chunk.columns:\n",
    "                #     df_mx_h = HRS30_MX_H(dfC_c)\n",
    "                # else:\n",
    "                #     df_mx_h = HRS30_MX_H(chunk_c)\n",
    "                \n",
    "                # def HRS30_MN_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                #     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                #     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                #         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                #     # Se llama la función de procesamiento de frecuencias\n",
    "                #     df_c, periodo,_,complet_dia, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                #     if df_c is None or complet_dia is None:\n",
    "                #         return None\n",
    "                \n",
    "                #     # Filtrar los días que tienen suficientes datos\n",
    "                #     df_c['Fecha_temp'] = df_c.index\n",
    "                #     dias_validos = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_dia).index\n",
    "                    \n",
    "                #     # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "                #     dias_validos = pd.to_datetime(dias_validos).normalize()\n",
    "                #     df_filtrado = df_c[df_c.index.normalize().isin(dias_validos)]\n",
    "                    \n",
    "                #     # Encontrar el valor mínimo por cada día válido\n",
    "                #     idx_minimos_dia = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmin()\n",
    "                #     mn_d = df_filtrado.loc[idx_minimos_dia]\n",
    "                    \n",
    "                #     # Eliminar la columna temporal antes de retornar el resultado\n",
    "                #     if 'Fecha_temp' in mn_d.columns:\n",
    "                #         mn_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                    \n",
    "                #     return mn_d[[columna_valor]]\n",
    "\n",
    "                # # Se llama el archivo original según si es dato crudo o con qc\n",
    "                # if 'Estado' in chunk.columns:\n",
    "                #     df_mn_d = HRS30_MN_D(dfC_c)\n",
    "                # else:\n",
    "                #     df_mn_d = HRS30_MN_D(chunk_c)\n",
    "                \n",
    "                # # Humedad relativa del aire a 10 cm  máxima diaria\n",
    "                # def HRS30_MX_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                #     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                #     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                #         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                #     # Se llama la función de procesamiento de frecuencias\n",
    "                #     df_c, periodo,_,complet_dia,_,_ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                #     if df_c is None or complet_dia is None:\n",
    "                #         return None\n",
    "                \n",
    "                #     # Filtrar los días que tienen suficientes datos\n",
    "                #     df_c['Fecha_temp'] = df_c.index\n",
    "                #     dias_validos = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_dia).index\n",
    "                #     # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "                #     dias_validos = pd.to_datetime(dias_validos).normalize()\n",
    "                #     df_filtrado = df_c[df_c.index.normalize().isin(dias_validos)]\n",
    "                #     # Encontrar el valor mínimo por cada día válido\n",
    "                #     idx_maximos_dia = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmax()\n",
    "                #     mx_d = df_filtrado.loc[idx_maximos_dia]\n",
    "                    \n",
    "                #     # Eliminar la columna temporal antes de retornar el resultado\n",
    "                #     if 'Fecha_temp' in mx_d.columns:\n",
    "                #         mx_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                    \n",
    "                #     return mx_d[[columna_valor]]\n",
    "                \n",
    "                # # Se llama el archivo original según si es dato crudo o con qc\n",
    "                # if 'Estado' in chunk.columns:\n",
    "                #     df_mx_d = HRS30_MX_D(dfC_c)\n",
    "                # else:\n",
    "                #     df_mx_d = HRS30_MX_D(chunk_c)\n",
    "                \n",
    "                # # Humedad relativa del aire a 10 cm  mínima mensual\n",
    "                # def HRS30_MN_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                #     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                #     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                #         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                #     # Se llama la función de procesamiento de frecuencias\n",
    "                #     df_c, periodo,_,_,complet_mes,_ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                #     if df_c is None or complet_mes is None:\n",
    "                #         return None\n",
    "                \n",
    "                #     # Filtrar los meses que tienen suficientes datos\n",
    "                #     df_c['Fecha_temp'] = df_c.index\n",
    "                #     # Eliminar duplicados en el índice\n",
    "                #     df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "                #     meses_validos = df_c.groupby(df_c.index.to_period('M')).filter(complet_mes).index.to_period('M')\n",
    "                \n",
    "                #     # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "                #     df_filtrado = df_c[df_c.index.to_period('M').isin(meses_validos)]\n",
    "                \n",
    "                #     # Encontrar el valor mínimo por cada mes válido\n",
    "                #     idx_minimos_mes = df_filtrado.groupby(df_filtrado.index.to_period('M'))[columna_valor].idxmin()\n",
    "                #     min_m = df_filtrado.loc[idx_minimos_mes]\n",
    "                \n",
    "                #     # Eliminar las columnas temporales antes de retornar el resultado\n",
    "                #     if 'Fecha_temp' in min_m.columns:\n",
    "                #         min_m.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                \n",
    "                #     return min_m[[columna_valor]]\n",
    "\n",
    "                # # Se llama el archivo original según si es dato crudo o con qc\n",
    "                # if 'Estado' in chunk.columns:\n",
    "                #     df_mn_m = HRS30_MN_M(dfC_c)\n",
    "                # else:\n",
    "                #     df_mn_m = HRS30_MN_M(chunk_c)\n",
    "                \n",
    "                # # Humedad relativa del aire a 10 cm  máxima mensual\n",
    "                # def HRS30_MX_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                #     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                #     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                #         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                #     # Se llama la función de procesamiento de frecuencias\n",
    "                #     df_c, periodo,_,_,complet_mes,_ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                #     if df_c is None or complet_mes is None:\n",
    "                #         return None\n",
    "                \n",
    "                #     # Filtrar los meses que tienen suficientes datos\n",
    "                #     df_c['Fecha_temp'] = df_c.index\n",
    "                #     # Eliminar duplicados en el índice\n",
    "                #     df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "                #     meses_validos = df_c.groupby(df_c.index.to_period('M')).filter(complet_mes).index.to_period('M')\n",
    "                \n",
    "                #     # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "                #     df_filtrado = df_c[df_c.index.to_period('M').isin(meses_validos)]\n",
    "                \n",
    "                #     # Encontrar el valor mínimo por cada mes válido\n",
    "                #     idx_maximos_mes = df_filtrado.groupby(df_filtrado.index.to_period('M'))[columna_valor].idxmax()\n",
    "                #     max_m = df_filtrado.loc[idx_maximos_mes]\n",
    "                \n",
    "                #     # Eliminar las columnas temporales antes de retornar el resultado\n",
    "                #     if 'Fecha_temp' in max_m.columns:\n",
    "                #         max_m.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                \n",
    "                #     return max_m[[columna_valor]]\n",
    "\n",
    "                # # Se llama el archivo original según si es dato crudo o con qc\n",
    "                # if 'Estado' in chunk.columns:\n",
    "                #     df_mx_m = HRS30_MX_M(dfC_c)\n",
    "                # else:\n",
    "                #     df_mx_m = HRS30_MX_M(chunk_c)\n",
    "                \n",
    "                # # Humedad relativa del aire a 10 cm mínima anual\n",
    "                # def HRS30_MN_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                #     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                #     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                #         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                #     # Se llama la función de procesamiento de frecuencias\n",
    "                #     df_c, periodo,_,_,_,complet_anio = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                #     if df_c is None or complet_anio is None:\n",
    "                #         return None\n",
    "                \n",
    "                #     # Filtrar los meses que tienen suficientes datos\n",
    "                #     df_c['Fecha_temp'] = df_c.index\n",
    "                #     # Eliminar duplicados en el índice\n",
    "                #     df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "                #     anios_validos = df_c.groupby(df_c.index.to_period('Y')).filter(complet_anio).index.to_period('Y')\n",
    "                \n",
    "                #     # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "                #     df_filtrado = df_c[df_c.index.to_period('Y').isin(anios_validos)]\n",
    "                \n",
    "                #     # Encontrar el valor mínimo por cada mes válido\n",
    "                #     idx_minimos_anio = df_filtrado.groupby(df_filtrado.index.to_period('Y'))[columna_valor].idxmin()\n",
    "                #     min_a = df_filtrado.loc[idx_minimos_anio]\n",
    "                \n",
    "                #     # Eliminar las columnas temporales antes de retornar el resultado\n",
    "                #     if 'Fecha_temp' in min_a.columns:\n",
    "                #         min_a.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                \n",
    "                #     return min_a[[columna_valor]]\n",
    "\n",
    "                # # Se llama el archivo original según si es dato crudo o con qc\n",
    "                # if 'Estado' in chunk.columns:\n",
    "                #     df_mn_a = HRS30_MN_A(dfC_c)\n",
    "                # else:\n",
    "                #     df_mn_a = HRS30_MN_A(chunk_c)\n",
    "                \n",
    "                # # Humedad relativa del aire a 10 cm  máxima anual\n",
    "                # def HRS30_MX_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.7):\n",
    "                #     # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                #     if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                #         df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                #     # Se llama la función de procesamiento de frecuencias\n",
    "                #     df_c, periodo,_,_,_,complet_anio = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                #     if df_c is None or complet_anio is None:\n",
    "                #         return None\n",
    "                \n",
    "                #     # Filtrar los meses que tienen suficientes datos\n",
    "                #     df_c['Fecha_temp'] = df_c.index\n",
    "                #     # Eliminar duplicados en el índice\n",
    "                #     df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "                #     anios_validos = df_c.groupby(df_c.index.to_period('Y')).filter(complet_anio).index.to_period('Y')\n",
    "                \n",
    "                #     # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "                #     df_filtrado = df_c[df_c.index.to_period('Y').isin(anios_validos)]\n",
    "                \n",
    "                #     # Encontrar el valor mínimo por cada mes válido\n",
    "                #     idx_maximos_anio = df_filtrado.groupby(df_filtrado.index.to_period('Y'))[columna_valor].idxmax()\n",
    "                #     max_a = df_filtrado.loc[idx_maximos_anio]\n",
    "                \n",
    "                #     # Eliminar las columnas temporales antes de retornar el resultado\n",
    "                #     if 'Fecha_temp' in max_a.columns:\n",
    "                #         max_a.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                \n",
    "                #     return max_a[[columna_valor]]\n",
    "                    \n",
    "                # # Se llama el archivo original según si es dato crudo o con qc\n",
    "                # if 'Estado' in chunk.columns:\n",
    "                #     df_mx_a = HRS30_MX_A(dfC_c)\n",
    "                # else:\n",
    "                #     df_mx_a = HRS30_MX_A(chunk_c)\n",
    "        \n",
    "                ### Se crea un nuevo archivo Excel con openpyxl\n",
    "                wb = Workbook()\n",
    "                sheets = {\n",
    "                    #'EV_I': ev_i,\n",
    "                    'EV_TT_H': ev_tt_h, 'EV_TT_D': ev_tt_d, \n",
    "                    'EV_TT_M': ev_tt_m, 'EV_TT_A': ev_tt_a, \n",
    "                    'EV_MEDIA_H': ev_media_h, 'EV_MEDIA_D': ev_media_d, \n",
    "                    'EV_MEDIA_M': ev_media_m, 'EV_MEDIA_A': ev_media_a,\n",
    "                    # 'HRS30_MEDIA_H': dfhrs30_med_h, 'HRS30_MEDIA_D': dfhrs30_med_d,\n",
    "                    # 'HRS30_MEDIA_M': dfhrs30_med_m, 'HRS30_MEDIA_A': dfhrs30_med_a, \n",
    "                    # 'HRS30_MN_H': df_mn_h,'HRS30_MX_H': df_mx_h,\n",
    "                    # 'HRS30_MN_D': df_mn_d,'HRS30_MX_D': df_mx_d, \n",
    "                    # 'HRS30_MN_M': df_mn_m,'HRS30_MX_M': df_mx_m, \n",
    "                    # 'HRS30_MN_A': df_mn_a,'HRS30_MX_A': df_mx_a \n",
    "                }\n",
    "                \n",
    "                # Si el workbook todavía tiene la hoja por defecto, se elimina\n",
    "                if \"Sheet\" in wb.sheetnames:\n",
    "                    del wb[\"Sheet\"]\n",
    "                \n",
    "                for sheet_name, data in sheets.items():\n",
    "                    ws = wb.create_sheet(title=sheet_name)\n",
    "                    \n",
    "                    # Agregamos los datos al Excel\n",
    "                    if data is None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for r_idx, row in enumerate(dataframe_to_rows(data, index=True, header=True), 1):\n",
    "                            for c_idx, value in enumerate(row, 1):\n",
    "                                ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                    \n",
    "                    # Crear una gráfica\n",
    "                    chart = LineChart()\n",
    "                    chart.title = sheet_name\n",
    "                    chart.style = 5\n",
    "                    chart.y_axis.title = 'Evaporación (mm)'\n",
    "                    chart.x_axis.title = 'Fecha'\n",
    "                    \n",
    "                    # Establecer datos para la gráfica\n",
    "                    max_row = ws.max_row\n",
    "                    values = Reference(ws, min_col=2, min_row=2, max_col=2, max_row=max_row)\n",
    "                    dates = Reference(ws, min_col=1, min_row=3, max_col=1, max_row=max_row)\n",
    "                    chart.add_data(values, titles_from_data=True)\n",
    "                    chart.set_categories(dates)\n",
    "                    \n",
    "                    # Quitar la leyenda\n",
    "                    chart.legend = None+\n",
    "                    \n",
    "                    # Cambiar el grosor de la línea a 0.5 puntos (equivalente a 50 centésimas de punto)\n",
    "                    for series in chart.series:\n",
    "                        series.graphicalProperties.line.width = 50\n",
    "                        series.graphicalProperties.line.solidFill = \"3498db\" \n",
    "                        \n",
    "                    # Posicionar la gráfica en el Excel\n",
    "                    ws.add_chart(chart, \"E3\")\n",
    "                \n",
    "                # Nombres archivos\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    nombre_archivo_salida = os.path.join(carpeta, archivo[:22] + '_deriv.xlsx')\n",
    "                else:\n",
    "                    nombre_archivo_salida = os.path.join(carpeta, archivo[:19] + '_deriv.xlsx')\n",
    "                #nombre_archivo_salida = os.path.join(carpeta, archivo[:22] + '_deriv.xlsx') #archivo[:19] el original #archivo[:22] con qc\n",
    "                \n",
    "                # Guardar el archivo Excel\n",
    "                wb.save(nombre_archivo_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57f4beec-b086-4a76-b100-f3c3224f2a2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_16420\\4222087669.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en el archivo Estacion_0023195040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0024025050_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0026255030_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0027015320_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0035215020_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0046015030_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0052055160_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0054017040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n"
     ]
    }
   ],
   "source": [
    "calc_deriv_EV('../../OE_3_QC_Variables/5_Evaporacion/QCResult_EV/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c0262efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_deriv_HRS30('../../../OE_3_QC_Variables/4_HumedadSuelo/HRS30/RawUnmodified_HRS30/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df022d20-772d-4041-a291-da25abf644e6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0bb96-7e66-4509-90a3-e1a97bf517ec",
   "metadata": {},
   "source": [
    "## Función cálculo masivo de derivadas y alistamiento Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eac274a3-3b32-403e-a606-0004f3c97c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deriv_EV_QC(carpeta):#, chunk_size=540000):\n",
    "    archivos = os.listdir(carpeta)\n",
    "\n",
    "    # Se crean carpetas para cada tipo de DataFrame si no existen\n",
    "    carpetas_salidas = ['EV_TT_H_QC', 'EV_TT_D_QC', 'EV_TT_M_QC', 'EV_TT_A_QC']#,\n",
    "                        #'EV_MN_H_QC', 'EV_MX_H_QC', 'EV_MN_D_QC', 'EV_MX_D_QC',\n",
    "                        #'EV_MN_M_QC', 'EV_MX_M_QC', 'EV_MN_A_QC', 'EV_MX_A_QC']\n",
    "\n",
    "    for cs in carpetas_salidas:\n",
    "        os.makedirs(os.path.join(carpeta, cs), exist_ok=True)\n",
    "    \n",
    "    # Procesar cada archivo en la carpeta\n",
    "    for archivo in archivos:\n",
    "        if archivo.endswith('.csv'):\n",
    "            ruta_archivo = os.path.join(carpeta, archivo)\n",
    "            df = pd.read_csv(ruta_archivo, encoding='latin-1')\n",
    "\n",
    "            # Procesar la fecha y filtrar según estado, como en el ejemplo original\n",
    "            try:\n",
    "                df['event_time'] = pd.to_datetime(df['event_time'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "            except ValueError:\n",
    "                df['event_time'] = pd.to_datetime(df['event_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "                  \n",
    "            try:\n",
    "                dfC = df[df['state'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PC']]))] # Se comenta si es de crudos\n",
    "                station_value = dfC['station'].values[0] #df['station'].values[0]\n",
    "            except IndexError:\n",
    "                print(f\"Error en el archivo {archivo}: dfC está vacío. Saltando al siguiente archivo.\")\n",
    "                continue  # Sale del bucle de chunks y continúa con el siguiente archivo\n",
    "\n",
    "            dfC_c = dfC.copy()\n",
    "            # Cálculo de la evaporación instantánea\n",
    "            def EV_I(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.7):\n",
    "                \"\"\"\n",
    "                Calcula la evaporación en un tanque a partir de datos de nivel, considerando solo las disminuciones de nivel.\n",
    "                \"\"\"\n",
    "                # Asegurarnos de que la columna de fecha sea datetime\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                    df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "\n",
    "                # Se llama la función de procesamiento de frecuencias\n",
    "                df_c, periodos, cant_esperd_h, _, _, _, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "                # Asegurarse de que 'periodos' tenga un número antes de la unidad\n",
    "                if periodos.isalpha():\n",
    "                    periodos = '1' + periodos\n",
    "                    \n",
    "                # Ordenar por fecha por seguridad\n",
    "                df = df.sort_values(by=columna_fecha).reset_index(drop=True)\n",
    "               \n",
    "                # Calcular la diferencia entre niveles consecutivos\n",
    "                df['Diferencia'] = df[columna_valor].diff()\n",
    "\n",
    "                # Calcular el delta de tiempo entre fechas consecutivas\n",
    "                df['Delta_Tiempo'] = df[columna_fecha].diff()\n",
    "            \n",
    "                # Convertir la frecuencia esperada a un timedelta\n",
    "                freq_timedelta = pd.to_timedelta(periodos)\n",
    "                \n",
    "                # Identificar valores no consecutivos\n",
    "                df['Es_Consecutivo'] = df['Delta_Tiempo'] <= freq_timedelta\n",
    "                                   \n",
    "                # Calcular evaporación solo para registros consecutivos con disminuciones\n",
    "                df[columna_valor] = np.where(\n",
    "                    df['Es_Consecutivo'] & (df['Diferencia'] < 0),\n",
    "                    df['Diferencia'].abs(),\n",
    "                    np.nan\n",
    "                )\n",
    "\n",
    "                # Se filtran columnas relevantes para el resultado de mhmma según si es raw o qc\n",
    "                ev_i = df[['station','event_time',columna_valor]]\n",
    "                \n",
    "                \n",
    "                return ev_i # evaporacion_total,\n",
    "\n",
    "            # Se llama el archivo original según si es dato crudo o con qc\n",
    "            ev_i = EV_I(dfC_c)\n",
    "            \n",
    "            # Evaporación total horaria\n",
    "            def EV_TT_H_QC(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.7):  \n",
    "                # Se llama la función de procesamiento de frecuencias\n",
    "                df_c, periodos, cant_esperd_h, _, _, _, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "                if periodos == 'h':\n",
    "                    df_c.reset_index(inplace=True)\n",
    "                    df_c['label'] = 'EV_TT_H_QC'\n",
    "                    return df_c[['station','label','event_time','event_value']]\n",
    "            \n",
    "                if df_c is None or complet_hora is None:\n",
    "                    return None\n",
    "            \n",
    "                # Se filtran los datos que tienen la complititud mínima\n",
    "                df_filtrado = df_c.groupby([df_c.index.hour]).filter(complet_hora) #dfC.groupby([dfC.index.date]).filter(complet_dia)\n",
    "                EV_60_tt_h = df_filtrado[[columna_valor]].resample('h').sum()\n",
    "                EV_60_tt_h[columna_valor] = EV_60_tt_h[columna_valor].where(\n",
    "                    df_filtrado[columna_valor].resample('h').count() >= cant_esperd_h * porc_min,\n",
    "                    other=float('nan'))\n",
    "\n",
    "                # Cambio de contenido de columnas\n",
    "                EV_60_tt_h['station'] = df_c['station'].iloc[0]\n",
    "                EV_60_tt_h['station'] = EV_60_tt_h['station'].astype('int64')\n",
    "                EV_60_tt_h['label'] = 'EV_TT_H_QC'\n",
    "                \n",
    "                # Reset index\n",
    "                EV_60_tt_h.reset_index(inplace=True)\n",
    "                \n",
    "                # Se reordenan las columnas\n",
    "                nuevo_orden = ['station', 'label', 'event_time', 'event_value']\n",
    "                # Reordenar las columnas usando el nuevo orden\n",
    "                EV_60_tt_h = EV_60_tt_h[nuevo_orden]\n",
    "            \n",
    "                # Convertir 'event_time' de nuevo a datetime para uniformidad\n",
    "                EV_60_tt_h['event_time'] = pd.to_datetime(EV_60_tt_h['event_time'])\n",
    "            \n",
    "                return EV_60_tt_h\n",
    "\n",
    "            dfev_tt_h = EV_TT_H_QC(ev_i)\n",
    "            \n",
    "            # Evaporación total diaria\n",
    "            def EV_TT_D_QC(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.7):\n",
    "                if df[columna_valor].empty:\n",
    "                    return None\n",
    "                df.reset_index(inplace=True)\n",
    "                # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                    df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                # Ajustar la hora de cada registro para que corresponda al rango deseado\n",
    "                df[columna_fecha] = df[columna_fecha] - pd.Timedelta(hours=1)\n",
    "                # Establecer la columna de fecha como índice\n",
    "                df.set_index(columna_fecha, inplace=True)\n",
    "                \n",
    "                # Calcular el total de registros esperados por día pluviométrico\n",
    "                total_esperado_por_dia = 24\n",
    "                # Función para verificar si un día pluviométrico tiene suficientes datos\n",
    "                def complet_dia(sub_df):\n",
    "                    return len(sub_df) >= total_esperado_por_dia * porc_min\n",
    "            \n",
    "                # Filtrar los días con suficientes datos y calcular el promedio diario\n",
    "                df_filtrado = df.groupby([df.index.date]).filter(complet_dia)\n",
    "                # Verificar si el DataFrame filtrado está vacío\n",
    "                if df_filtrado.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por días válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return None\n",
    "                    \n",
    "                # Se calcula la media\n",
    "                EV_60_tt_d = df_filtrado[[columna_valor]].resample('D').sum()\n",
    "\n",
    "                # Llenado de vacíos cuando no superan pruebas\n",
    "                EV_60_tt_d[columna_valor] = EV_60_tt_d[columna_valor].where(\n",
    "                    df_filtrado[columna_valor].resample('D').count() >= total_esperado_por_dia * porc_min,\n",
    "                    other=float('nan'))\n",
    "\n",
    "                if EV_60_tt_d.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por días válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return None\n",
    "\n",
    "                # Cambio de contenido de columnas\n",
    "                EV_60_tt_d['station'] = df['station'].iloc[0]\n",
    "                EV_60_tt_d['station'] = EV_60_tt_d['station'].astype('int64')\n",
    "                EV_60_tt_d['label'] = 'EV_TT_D_QC'\n",
    "                \n",
    "                # Reset index\n",
    "                EV_60_tt_d.reset_index(inplace=True)\n",
    "                \n",
    "                # Se reordenan las columnas\n",
    "                nuevo_orden = ['station', 'label', 'event_time', 'event_value']\n",
    "                # Reordenar las columnas usando el nuevo orden\n",
    "                EV_60_tt_d = EV_60_tt_d[nuevo_orden]\n",
    "            \n",
    "                # Convertir 'event_time' de nuevo a datetime para uniformidad\n",
    "                EV_60_tt_d['event_time'] = pd.to_datetime(EV_60_tt_d['event_time'])\n",
    "            \n",
    "                return EV_60_tt_d\n",
    "\n",
    "            dfev_tt_d = EV_TT_D_QC(dfev_tt_h)\n",
    "            \n",
    "            # Evaporación total mensual\n",
    "            dfev_tt_d_c = dfev_tt_d.copy()\n",
    "            def EV_TT_M_QC(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.7):    \n",
    "                if df[columna_valor].empty:\n",
    "                    return None\n",
    "                df.reset_index(inplace=True)\n",
    "                # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                    df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                \n",
    "                # Establecer la columna 'event_time' como índice\n",
    "                df.set_index(columna_fecha, inplace=True)\n",
    "                \n",
    "                days_in_month = df.index.to_series().dt.days_in_month\n",
    "                days_in_month = days_in_month.resample('ME').first()\n",
    "                \n",
    "                # Función para verificar si una hora específica tiene suficientes datos\n",
    "                def complet_mes(sub_df):\n",
    "                    mes = sub_df.index[0].month\n",
    "                    total_esperado = days_in_month[days_in_month.index.month == mes].iloc[0]\n",
    "                    return len(sub_df) >= total_esperado * porc_min\n",
    "            \n",
    "                # Luego de establecer el índice, aplicar resample\n",
    "                df_filtrado = df.groupby([df.index.year, df.index.month]).filter(complet_mes)\n",
    "                # Verificar si el DataFrame filtrado está vacío\n",
    "                if df_filtrado.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por meses válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return None\n",
    "                    \n",
    "                # Se calcula la media \n",
    "                EV_60_tt_m = df_filtrado[['event_value']].resample('ME').sum()\n",
    "\n",
    "                # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "                counts = df.groupby([df.index.year, df.index.month])[columna_valor].count()\n",
    "                counts.index = pd.to_datetime(['{}-{}'.format(i[0], i[1]) for i in counts.index], format='%Y-%m')\n",
    "                counts = counts.resample('ME').sum()\n",
    "                EV_60_tt_m[columna_valor] = EV_60_tt_m[columna_valor].where(counts >= days_in_month * porc_min, other=float('nan'))\n",
    "                if EV_60_tt_m.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por meses válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return None\n",
    "                \n",
    "                # Cambio de contenido de columnas\n",
    "                EV_60_tt_m['station'] = df['station'].iloc[0]\n",
    "                EV_60_tt_m['station'] = EV_60_tt_m['station'].astype('int64')\n",
    "                EV_60_tt_m['label'] = 'EV_TT_M_QC'\n",
    "                \n",
    "                # Reset index\n",
    "                EV_60_tt_m.reset_index(inplace=True)\n",
    "                \n",
    "                # Se reordenan las columnas\n",
    "                nuevo_orden = ['station', 'label', 'event_time', 'event_value']\n",
    "                # Reordenar las columnas usando el nuevo orden\n",
    "                EV_60_tt_m = EV_60_tt_m[nuevo_orden]\n",
    "            \n",
    "                # Convertir 'event_time' de nuevo a datetime para uniformidad\n",
    "                EV_60_tt_m['event_time'] = pd.to_datetime(EV_60_tt_m['event_time'])\n",
    "                \n",
    "                return EV_60_tt_m\n",
    "            \n",
    "            dfev_tt_m = EV_TT_M_QC(dfev_tt_d_c)\n",
    "            \n",
    "            # Evaporación total anual\n",
    "            dfev_tt_m_c = dfev_tt_m.copy()\n",
    "            def EV_TT_A_QC(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.7):\n",
    "                if df[columna_valor].empty:\n",
    "                    return None\n",
    "                df.reset_index(inplace=True)\n",
    "                # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                    df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                \n",
    "                # Función para verificar si una hora específica tiene suficientes datos\n",
    "                def complet_anio(sub_df):\n",
    "                    return len(sub_df) >= 12 * porc_min\n",
    "                   \n",
    "                # Antes de resample, establecer la columna 'event_time' como índice\n",
    "                df.set_index(columna_fecha, inplace=True)\n",
    "            \n",
    "                # Luego de establecer el índice, aplicar resample\n",
    "                df_filtrado = df.groupby([df.index.year]).filter(complet_anio)\n",
    "                # Verificar si el DataFrame filtrado está vacío\n",
    "                if df_filtrado.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por años válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return None\n",
    "                    \n",
    "                # Se calcula la media    \n",
    "                EV_60_tt_a = df_filtrado[['event_value']].resample('YE').sum()\n",
    "\n",
    "                # Reemplazar sumas de 0 con NaN si no cumplen con el porcentaje mínimo\n",
    "                counts = df.groupby(df.index.year)[columna_valor].count()\n",
    "                counts.index = pd.to_datetime(['{}-01'.format(i) for i in counts.index], format='%Y-%m')\n",
    "                counts = counts.resample('YE').sum()\n",
    "                EV_60_tt_a[columna_valor] = EV_60_tt_a[columna_valor].where(counts >= 12 * porc_min, other=float('nan'))\n",
    "                if EV_60_tt_a.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por años válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return None\n",
    "\n",
    "                # Cambio de contenido de columnas\n",
    "                EV_60_tt_a['station'] = df['station'].iloc[0]\n",
    "                EV_60_tt_a['station'] = EV_60_tt_a['station'].astype('int64')\n",
    "                EV_60_tt_a['label'] = 'EV_TT_A_QC'\n",
    "                \n",
    "                # Reset index\n",
    "                EV_60_tt_a.reset_index(inplace=True)\n",
    "                \n",
    "                # Se reordenan las columnas\n",
    "                nuevo_orden = ['station', 'label', 'event_time', 'event_value']\n",
    "                # Reordenar las columnas usando el nuevo orden\n",
    "                EV_60_tt_a = EV_60_tt_a[nuevo_orden]\n",
    "            \n",
    "                # Convertir 'event_time' de nuevo a datetime para uniformidad\n",
    "                EV_60_tt_a['event_time'] = pd.to_datetime(EV_60_tt_a['event_time'])\n",
    "                \n",
    "                return EV_60_tt_a\n",
    "            \n",
    "            dfev_tt_a = EV_TT_A_QC(dfev_tt_m_c)\n",
    "\n",
    "            if not dfev_tt_h.empty:\n",
    "                dfev_tt_h.to_csv(os.path.join(carpeta, 'EV_TT_H_QC', f'{archivo[:19]}.csv'), index=False)\n",
    "            if not dfev_tt_d.empty:\n",
    "                dfev_tt_d.to_csv(os.path.join(carpeta, 'EV_TT_D_QC', f'{archivo[:19]}.csv'), index=False)\n",
    "            if not dfev_tt_m.empty:\n",
    "                dfev_tt_m.to_csv(os.path.join(carpeta, 'EV_TT_M_QC', f'{archivo[:19]}.csv'), date_format='%Y-%m', index=False)\n",
    "            if not dfev_tt_a.empty:\n",
    "                dfev_tt_a.to_csv(os.path.join(carpeta, 'EV_TT_A_QC', f'{archivo[:19]}.csv'), date_format='%Y',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "90943a6b-2791-482e-b411-611884b55457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en el archivo Estacion_0023195040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "DataFrame vacío después de filtrar por días válidos. Regresando Estacion_0024015513 vacío.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'copy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[75], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m calc_deriv_EV_QC(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../OE_3_QC_Variables/5_Evaporacion/ReadyToCassandraFiles_EV/\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[73], line 178\u001b[0m, in \u001b[0;36mcalc_deriv_EV_QC\u001b[1;34m(carpeta)\u001b[0m\n\u001b[0;32m    175\u001b[0m dfev_tt_d \u001b[38;5;241m=\u001b[39m EV_TT_D_QC(dfev_tt_h)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;66;03m# Evaporación total mensual\u001b[39;00m\n\u001b[1;32m--> 178\u001b[0m dfev_tt_d_c \u001b[38;5;241m=\u001b[39m dfev_tt_d\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mEV_TT_M_QC\u001b[39m(df, columna_fecha\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_time\u001b[39m\u001b[38;5;124m'\u001b[39m, columna_valor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevent_value\u001b[39m\u001b[38;5;124m'\u001b[39m, porc_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m):    \n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m df[columna_valor]\u001b[38;5;241m.\u001b[39mempty:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'copy'"
     ]
    }
   ],
   "source": [
    "calc_deriv_EV_QC('../../OE_3_QC_Variables/5_Evaporacion/ReadyToCassandraFiles_EV/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f8d21-7fc5-42ff-88af-84a73cb7cb5a",
   "metadata": {},
   "source": [
    "### Cálculo promedios horarios mensuales multianuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "15a30dc0-69e9-4016-8d4d-3033f49e5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hmMa_EV(carpeta, chunk_size=540000):\n",
    "    archivos = os.listdir(carpeta)\n",
    "\n",
    "    # Se recorre cada archivo en la carpeta\n",
    "    for archivo in archivos:\n",
    "        if archivo.endswith('.csv'):\n",
    "            ruta_archivo = os.path.join(carpeta, archivo)\n",
    "        \n",
    "            # Se procesan los archivos csv por fragmentos\n",
    "            reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=chunk_size)\n",
    "            \n",
    "            for chunk in reader:\n",
    "                # Se generan dataframes analizados\n",
    "                # De cada chunk se transforma a datetime la serie/columna 'Fecha'\n",
    "                try:\n",
    "                    chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                    #chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "                except ValueError:\n",
    "                    chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "                    chunk = chunk[~chunk['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PSO0','0PAT','0PER']]))]\n",
    "\n",
    "                # Se hace la agrupación para cálculo de medias horarias mensuales multianuales\n",
    "                #hym_ma = chunk['Valor'].groupby(by =[chunk[\"Fecha\"].dt.month, chunk[\"Fecha\"].dt.hour]).mean().unstack(level=0)\n",
    "                hym_ma = chunk['Evaporacion'].groupby(by =[chunk[\"Fecha\"].dt.month, chunk[\"Fecha\"].dt.hour]).mean().unstack(level=0)\n",
    "\n",
    "                # Crear un archivo Excel y agregar los datos\n",
    "                wb = Workbook()\n",
    "                ws = wb.active\n",
    "                ws.title = \"Datos\"\n",
    "                \n",
    "                # Agregar datos al archivo Excel\n",
    "                for r in dataframe_to_rows(hym_ma.reset_index(), index=False, header=True):\n",
    "                    ws.append(r)\n",
    "                \n",
    "                # Crear la gráfica de dispersión\n",
    "                chart = ScatterChart()\n",
    "                chart.title = \"Valores Promedio por Hora y Mes\"\n",
    "                chart.style = 13\n",
    "                chart.x_axis.title = 'Hora del día'\n",
    "                chart.y_axis.title = 'Valor'\n",
    "                \n",
    "                # Aumentar el tamaño del gráfico\n",
    "                chart.width = 20  # Anchura del gráfico (pulgadas)\n",
    "                chart.height = 12  # Altura del gráfico (pulgadas)\n",
    "                \n",
    "                # Fijar el máximo valor del eje x\n",
    "                chart.x_axis.scaling.max = 23\n",
    "                chart.x_axis.scaling.min = 0\n",
    "                chart.x_axis.majorUnit = 1\n",
    "                \n",
    "                # Agregar series a la gráfica\n",
    "                colors = ['1F77B4', 'FF7F0E', '2CA02C', 'D62728', '9467BD', '8C564B', 'E377C2', '7F7F7F', 'BCBD22', '17BECF', 'AEC7E8', 'FFBB78']\n",
    "                for i in range(2, 14):  # Columnas B a M (meses 1 a 12)\n",
    "                    xvalues = Reference(ws, min_col=1, min_row=2, max_row=25)\n",
    "                    yvalues = Reference(ws, min_col=i, min_row=1, max_row=25)\n",
    "                    series = Series(yvalues, xvalues, title_from_data=True)\n",
    "                    series.graphicalProperties.line.solidFill = colors[i % len(colors)]  # Asignar colores a las líneas\n",
    "                    series.graphicalProperties.line.width = 30000  # Ajustar el grosor de las líneas\n",
    "                    series.marker.symbol = 'circle'  # Cambiar el marcador a círculo\n",
    "                    series.marker.size = 5\n",
    "                    series.marker.graphicalProperties.solidFill = colors[i % len(colors)]  # Cambiar el color del marcador\n",
    "                    chart.series.append(series)\n",
    "                \n",
    "                # Insertar la gráfica en la hoja de cálculo\n",
    "                ws.add_chart(chart, \"O2\")\n",
    "\n",
    "                # Nombres archivos\n",
    "                nombre_archivo_salida = os.path.join(carpeta, archivo[:13] + '_hm_ma.xlsx') #archivo[:22] el de qc\n",
    "                # Guardar el archivo Excel\n",
    "                wb.save(nombre_archivo_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e1c99242-480a-4d33-afb9-c9b9bf75ad91",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_hmMa_EV('EV_i_qc')\n",
    "#calc_hmMa_EV('EV_i')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecbf0a5-3966-40d9-b88c-a5a409a03823",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_hmMa_EV('../../OE_3_QC_Variables/5_Evaporacion/RawUnmodified_EV/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb1faf3-35ad-4626-8cc6-20c8da15bc55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
