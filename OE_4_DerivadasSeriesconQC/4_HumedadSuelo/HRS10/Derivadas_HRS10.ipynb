{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad644f7",
   "metadata": {},
   "source": [
    "# Cálculo de derivadas temperatura del suelo -10cm y graficación"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a268314",
   "metadata": {},
   "source": [
    "> Elaborado por Paola Álvarez, profesional contratista IDEAM, contrato 196 de 2024. Comentarios o inquietudes, remitir a *palvarez@ideam.gov.co* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc197bfa",
   "metadata": {},
   "source": [
    "**Librerías**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a06ee5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime\n",
    "import statistics\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import gc\n",
    "import calendar\n",
    "from collections import deque\n",
    "from datetime import timedelta\n",
    "from scipy import stats\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.chart import LineChart, Reference\n",
    "from openpyxl.chart import ScatterChart, Reference, Series\n",
    "from openpyxl.chart import BarChart, Reference\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5dbca0",
   "metadata": {},
   "source": [
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c0b13d-32b9-4249-8f67-b9579629c775",
   "metadata": {},
   "source": [
    "### Pruebas unitarias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc27ed7b",
   "metadata": {},
   "source": [
    "#### Datos con QC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8cdb286-46d3-49de-8050-56a678c42f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar frecuencias\n",
    "def process_frequencies(df, columna_fecha, freq_csv_path, porc_min=0.67):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "    # Cargar el archivo de frecuencias\n",
    "    freqinst200b = pd.read_csv(freq_csv_path, encoding='latin-1')\n",
    "\n",
    "    # Definir el diccionario de frecuencias y cantidades esperadas\n",
    "    frecuencias = {\n",
    "        'T': {'cant_esperd_h': 60, 'cant_esperd_d': 1440, 'cant_esperd_m': 43200, 'cant_esperd_a': 518400, 'minutos': 1},\n",
    "        '5T': {'cant_esperd_h': 12, 'cant_esperd_d': 288, 'cant_esperd_m': 8640, 'cant_esperd_a': 103680, 'minutos': 5},\n",
    "        '10T': {'cant_esperd_h': 6, 'cant_esperd_d': 144, 'cant_esperd_m': 4320, 'cant_esperd_a': 51840, 'minutos': 10},\n",
    "        'h': {'cant_esperd_h': 1, 'cant_esperd_d': 24, 'cant_esperd_m': 720, 'cant_esperd_a': 8640}\n",
    "    }\n",
    "\n",
    "    # Obtener el valor de la estación\n",
    "    station_value = df['Station'].values[0]\n",
    "    freqinst200b_station = freqinst200b.loc[freqinst200b['Station'] == station_value]\n",
    "    periodos = freqinst200b_station['FreqInf'].values[0]\n",
    "\n",
    "    if pd.isna(periodos):\n",
    "        try:\n",
    "            periodos = pd.infer_freq(df[columna_fecha][-25:])\n",
    "            print(periodos)\n",
    "            if periodos is None:\n",
    "                print(f\"Frecuencia inferida es None para el archivo {df}\")\n",
    "                return None, None, None, None\n",
    "        except ValueError as e:\n",
    "            print(f'Error al inferir la frecuencia en el archivo {df}: {str(e)}')\n",
    "            return None, None, None, None\n",
    "\n",
    "    # Obtener las cantidades esperadas y el offset en minutos\n",
    "    cant_esperd_h = frecuencias[periodos]['cant_esperd_h']\n",
    "    cant_esperd_d = frecuencias[periodos]['cant_esperd_d']\n",
    "    cant_esperd_m = frecuencias[periodos]['cant_esperd_m']\n",
    "    cant_esperd_a = frecuencias[periodos]['cant_esperd_a']\n",
    "    if periodos == 'h':\n",
    "        pass\n",
    "    else:\n",
    "        minutoffset = frecuencias[periodos]['minutos']\n",
    "\n",
    "    # Ajustar la hora de cada registro\n",
    "    df_c = df.copy()\n",
    "    if periodos == 'h':\n",
    "        df_c[columna_fecha] = df_c[columna_fecha] - pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df_c[columna_fecha] = df_c[columna_fecha] - pd.Timedelta(minutes=minutoffset)\n",
    "\n",
    "    # Establecer la columna de fecha como índice\n",
    "    df_c.set_index(columna_fecha, inplace=True)\n",
    "\n",
    "    # Función para verificar si un día, mes o año tiene suficientes datos\n",
    "    def complet_dia(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_d * porc_min\n",
    "\n",
    "    def complet_mes(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_m * porc_min\n",
    "\n",
    "    def complet_anio(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_a * porc_min\n",
    "\n",
    "    return df_c, complet_dia, complet_mes, complet_anio\n",
    "\n",
    "# Uso funciones de frecuencias\n",
    "df_example = pd.read_csv('../../OE_3_QC_Variables/2_HumedadRelativa/Test_QC/Estacion_0011025501_qc.csv', encoding='latin-1')#, dtype={'Estado_Anterior':str})\n",
    "if 'Estado' in df_example.columns:\n",
    "    df_example = df_example[~df_example['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PSO','0PAT','0PER']]))]\n",
    "    \n",
    "columna_fecha = 'Fecha'\n",
    "freq_csv_path = '../../OE_3_QC_Variables/2_HumedadRelativa/EMAhrs10_LatLonEntFreq2.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c74e85e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V2_22/07/2024, cambio de procesamiento de frecuencias\n",
    "## Derivadas diarias, mensuales y anuales\n",
    "# Humedad relativa del aire a 10 cm media diaria\n",
    "df_example_c = df_example.copy()\n",
    "def HRS10_MEDIA_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "     \n",
    "    # Se filtran los que hayan superaddo las pruebas\n",
    "    #dfC = df[df['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PC']]))]\n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, complet_dia, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "    if df_c is None or complet_dia is None:\n",
    "        return None\n",
    "\n",
    "    # Se filtran los datos que tienen la complititud mínima\n",
    "    df_filtrado = df_c.groupby([df_c.index.date]).filter(complet_dia) #dfC.groupby([dfC.index.date]).filter(complet_dia)\n",
    "    hrs10_med_d = df_filtrado[[columna_valor]].resample('D').mean()\n",
    "\n",
    "    return hrs10_med_d\n",
    "\n",
    "# Ejemplo de uso de la función\n",
    "dfhrs10_med_d = HRS10_MEDIA_D(df_example_c)\n",
    "\n",
    "# Presión atmosférica media mensual\n",
    "def HRS10_MEDIA_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):    \n",
    "    df.reset_index(inplace=True)\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "    \n",
    "    # Establecer la columna 'Fecha' como índice\n",
    "    df.set_index(columna_fecha, inplace=True)\n",
    "    \n",
    "    days_in_month = df.index.to_series().dt.days_in_month\n",
    "    days_in_month = days_in_month.resample('ME').first()\n",
    "    \n",
    "    # Función para verificar si una hora específica tiene suficientes datos\n",
    "    def complet_mes(sub_df):\n",
    "        mes = sub_df.index[0].month\n",
    "        total_esperado = days_in_month[days_in_month.index.month == mes].iloc[0]\n",
    "        return len(sub_df) >= total_esperado * porc_min\n",
    "\n",
    "    # Luego de establecer el índice, aplicar resample\n",
    "    df_filtrado = df.groupby([df.index.year, df.index.month]).filter(complet_mes)\n",
    "    hrs10_med_m = df_filtrado[['Valor']].resample('ME').mean()\n",
    "    \n",
    "    return hrs10_med_m\n",
    "\n",
    "dfhrs10_med_m = HRS10_MEDIA_M(dfhrs10_med_d)\n",
    "\n",
    "# Presión atmosférica del Aire a 2 metros media anual\n",
    "def HRS10_MEDIA_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "    df.reset_index(inplace=True)\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "    \n",
    "    # Función para verificar si una hora específica tiene suficientes datos\n",
    "    def complet_anio(sub_df):\n",
    "        return len(sub_df) >= 12 * porc_min\n",
    "       \n",
    "    # Antes de resample, establecer la columna 'Fecha' como índice\n",
    "    df.set_index(columna_fecha, inplace=True)\n",
    "\n",
    "    # Luego de establecer el índice, aplicar resample\n",
    "    df_filtrado = df.groupby([df.index.year]).filter(complet_anio)\n",
    "    hrs10_med_a = df_filtrado[['Valor']].resample('YE').mean()\n",
    "    \n",
    "    return hrs10_med_a\n",
    "\n",
    "dfhrs10_med_a = HRS10_MEDIA_A(dfhrs10_med_m)\n",
    "\n",
    "###-- Derivados máximos y mínimos\n",
    "#Humedad relativa del aire a 10 cm  mínima diaria\n",
    "df_example_c = df_example.copy()\n",
    "def HRS10_MN_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, complet_dia, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "    if df_c is None or complet_dia is None:\n",
    "        return None\n",
    "\n",
    "    # Filtrar los días que tienen suficientes datos\n",
    "    df_c['Fecha_temp'] = df_c.index\n",
    "    dias_validos = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_dia).index\n",
    "    \n",
    "    # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "    dias_validos = pd.to_datetime(dias_validos).normalize()\n",
    "    df_filtrado = df_c[df_c.index.normalize().isin(dias_validos)]\n",
    "    \n",
    "    # Encontrar el valor mínimo por cada día válido\n",
    "    idx_minimos_dia = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmin()\n",
    "    mn_d = df_filtrado.loc[idx_minimos_dia]\n",
    "    \n",
    "    # Eliminar la columna temporal antes de retornar el resultado\n",
    "    if 'Fecha_temp' in mn_d.columns:\n",
    "        mn_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "    \n",
    "    return mn_d[[columna_valor]]\n",
    "\n",
    "df_mn_d = HRS10_MN_D(df_example_c)\n",
    "\n",
    "# Humedad relativa del aire a 10 cm  máxima diaria\n",
    "df_example_c = df_example.copy()\n",
    "def HRS10_MX_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, complet_dia, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "    if df_c is None or complet_dia is None:\n",
    "        return None\n",
    "\n",
    "    # Filtrar los días que tienen suficientes datos\n",
    "    df_c['Fecha_temp'] = df_c.index\n",
    "    dias_validos = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_dia).index\n",
    "    # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "    dias_validos = pd.to_datetime(dias_validos).normalize()\n",
    "    df_filtrado = df_c[df_c.index.normalize().isin(dias_validos)]\n",
    "    # Encontrar el valor mínimo por cada día válido\n",
    "    idx_maximos_dia = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmax()\n",
    "    mx_d = df_filtrado.loc[idx_maximos_dia]\n",
    "    \n",
    "    # Eliminar la columna temporal antes de retornar el resultado\n",
    "    if 'Fecha_temp' in mx_d.columns:\n",
    "        mx_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "    \n",
    "    return mx_d[[columna_valor]]\n",
    "\n",
    "df_mx_d = HRS10_MX_D(df_example_c)\n",
    "\n",
    "# Humedad relativa del aire a 10 cm  mínima mensual\n",
    "df_example_c = df_example.copy()\n",
    "def HRS10_MN_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, _, complet_mes, _ = process_frequencies(df, columna_fecha, freq_csv_path,porc_min)\n",
    "\n",
    "    if df_c is None or complet_mes is None:\n",
    "        return None\n",
    "\n",
    "    # Filtrar los meses que tienen suficientes datos\n",
    "    df_c['Fecha_temp'] = df_c.index\n",
    "    # Eliminar duplicados en el índice\n",
    "    df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "    meses_validos = df_c.groupby(df_c.index.to_period('M')).filter(complet_mes).index.to_period('M')\n",
    "\n",
    "    # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "    df_filtrado = df_c[df_c.index.to_period('M').isin(meses_validos)]\n",
    "\n",
    "    # Encontrar el valor mínimo por cada mes válido\n",
    "    idx_minimos_mes = df_filtrado.groupby(df_filtrado.index.to_period('M'))[columna_valor].idxmin()\n",
    "    min_m = df_filtrado.loc[idx_minimos_mes]\n",
    "\n",
    "    # Eliminar las columnas temporales antes de retornar el resultado\n",
    "    if 'Fecha_temp' in min_m.columns:\n",
    "        min_m.drop(columns=['Fecha_temp'], inplace=True)\n",
    "\n",
    "    return min_m[[columna_valor]]\n",
    "    \n",
    "df_mn_m = HRS10_MN_M(df_example_c)\n",
    "\n",
    "# Humedad relativa del aire a 10 cm  máxima mensual\n",
    "df_example_c = df_example.copy()\n",
    "def HRS10_MX_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "    \n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, _, complet_mes, _ = process_frequencies(df, columna_fecha, freq_csv_path,porc_min)\n",
    "\n",
    "    if df_c is None or complet_mes is None:\n",
    "        return None\n",
    "\n",
    "    # Filtrar los meses que tienen suficientes datos\n",
    "    df_c['Fecha_temp'] = df_c.index\n",
    "    # Eliminar duplicados en el índice\n",
    "    df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "    meses_validos = df_c.groupby(df_c.index.to_period('M')).filter(complet_mes).index.to_period('M')\n",
    "\n",
    "    # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "    df_filtrado = df_c[df_c.index.to_period('M').isin(meses_validos)]\n",
    "\n",
    "    # Encontrar el valor mínimo por cada mes válido\n",
    "    idx_maximos_mes = df_filtrado.groupby(df_filtrado.index.to_period('M'))[columna_valor].idxmax()\n",
    "    max_m = df_filtrado.loc[idx_maximos_mes]\n",
    "\n",
    "    # Eliminar las columnas temporales antes de retornar el resultado\n",
    "    if 'Fecha_temp' in max_m.columns:\n",
    "        max_m.drop(columns=['Fecha_temp'], inplace=True)\n",
    "\n",
    "    return max_m[[columna_valor]]\n",
    "    \n",
    "df_mx_m = HRS10_MX_M(df_example_c)\n",
    "\n",
    "# Humedad relativa del aire a 10 cm mínima anual\n",
    "df_example_c = df_example.copy()\n",
    "def HRS10_MN_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, _, _, complet_anio = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "    if df_c is None or complet_anio is None:\n",
    "        return None\n",
    "\n",
    "    # Filtrar los meses que tienen suficientes datos\n",
    "    df_c['Fecha_temp'] = df_c.index\n",
    "    # Eliminar duplicados en el índice\n",
    "    df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "    anios_validos = df_c.groupby(df_c.index.to_period('A')).filter(complet_anio).index.to_period('A')\n",
    "\n",
    "    # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "    df_filtrado = df_c[df_c.index.to_period('A').isin(anios_validos)]\n",
    "\n",
    "    # Encontrar el valor mínimo por cada mes válido\n",
    "    idx_minimos_anio = df_filtrado.groupby(df_filtrado.index.to_period('A'))[columna_valor].idxmin()\n",
    "    min_a = df_filtrado.loc[idx_minimos_anio]\n",
    "\n",
    "    # Eliminar las columnas temporales antes de retornar el resultado\n",
    "    if 'Fecha_temp' in min_a.columns:\n",
    "        min_a.drop(columns=['Fecha_temp'], inplace=True)\n",
    "\n",
    "    return min_a[[columna_valor]]\n",
    "    \n",
    "df_mn_a = HRS10_MN_A(df_example_c)\n",
    "\n",
    "# Humedad relativa del aire a 10 cm  máxima anual\n",
    "df_example_c = df_example.copy()\n",
    "def HRS10_MX_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "    # Se llama la función de procesamiento de frecuencias\n",
    "    df_c, _, _, complet_anio = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "    if df_c is None or complet_anio is None:\n",
    "        return None\n",
    "\n",
    "    # Filtrar los meses que tienen suficientes datos\n",
    "    df_c['Fecha_temp'] = df_c.index\n",
    "    # Eliminar duplicados en el índice\n",
    "    df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "    anios_validos = df_c.groupby(df_c.index.to_period('A')).filter(complet_anio).index.to_period('A')\n",
    "\n",
    "    # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "    df_filtrado = df_c[df_c.index.to_period('A').isin(anios_validos)]\n",
    "\n",
    "    # Encontrar el valor mínimo por cada mes válido\n",
    "    idx_minimos_anio = df_filtrado.groupby(df_filtrado.index.to_period('A'))[columna_valor].idxmax()\n",
    "    max_a = df_filtrado.loc[idx_minimos_anio]\n",
    "\n",
    "    # Eliminar las columnas temporales antes de retornar el resultado\n",
    "    if 'Fecha_temp' in max_a.columns:\n",
    "        max_a.drop(columns=['Fecha_temp'], inplace=True)\n",
    "\n",
    "    return max_a[[columna_valor]]\n",
    "    \n",
    "df_mx_a = HRS10_MN_A(df_example_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62d69c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- Gráficas y export en excel\n",
    "# Se crea un nuevo archivo Excel con openpyxl\n",
    "wb = Workbook()\n",
    "sheets = {\n",
    "    'HRS10_MEDIA_D': dfhrs10_med_d, 'HRS10_MEDIA_M': dfhrs10_med_m,\n",
    "    'HRS10_MEDIA_A': dfhrs10_med_a, 'HRS10_MN_D': df_mn_d,\n",
    "    'HRS10_MX_D': df_mx_d, 'HRS10_MN_M': df_mn_m,\n",
    "    'HRS10_MX_M': df_mx_m, 'HRS10_MN_A': df_mn_a,\n",
    "    'HRS10_MX_A': df_mx_a \n",
    "}\n",
    "\n",
    "# Si el workbook todavía tiene la hoja por defecto, se elimina\n",
    "if \"Sheet\" in wb.sheetnames:\n",
    "    del wb[\"Sheet\"]\n",
    "\n",
    "for sheet_name, data in sheets.items():\n",
    "    ws = wb.create_sheet(title=sheet_name)\n",
    "    \n",
    "    # Agregamos los datos al Excel\n",
    "    for r_idx, row in enumerate(dataframe_to_rows(data, index=True, header=True), 1):\n",
    "        for c_idx, value in enumerate(row, 1):\n",
    "            ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "    \n",
    "    # Crear una gráfica\n",
    "    chart = LineChart()\n",
    "    chart.title = sheet_name\n",
    "    chart.style = 5\n",
    "    chart.y_axis.title = 'Humedad relativa (%)'\n",
    "    chart.x_axis.title = 'Fecha'\n",
    "    \n",
    "    # Establecer datos para la gráfica\n",
    "    max_row = ws.max_row\n",
    "    values = Reference(ws, min_col=2, min_row=2, max_col=2, max_row=max_row)\n",
    "    dates = Reference(ws, min_col=1, min_row=3, max_col=1, max_row=max_row)\n",
    "    chart.add_data(values, titles_from_data=True)\n",
    "    chart.set_categories(dates)\n",
    "    \n",
    "    # Quitar la leyenda\n",
    "    chart.legend = None\n",
    "        \n",
    "    # Posicionar la gráfica en el Excel\n",
    "    ws.add_chart(chart, \"E3\")\n",
    "\n",
    "# Guardar el archivo Excel\n",
    "wb.save(\"Agreg_graf_HRS10_AUT_60_5T_cohr5vals.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db4c130",
   "metadata": {},
   "source": [
    "## Función para cálculo masivo de derivadas y generación de gráficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dfd6f3d-f60d-4d27-b653-7eb769b203f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para procesar frecuencias\n",
    "def process_frequencies(df, columna_fecha, freq_csv_path, porc_min=0.67):\n",
    "    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "        \n",
    "    # Cargar el archivo de frecuencias\n",
    "    freqinst200b = pd.read_csv(freq_csv_path, encoding='latin-1', sep=';')\n",
    "\n",
    "    # Definir el diccionario de frecuencias y cantidades esperadas\n",
    "    frecuencias = {\n",
    "        'min': {'cant_esperd_h': 60, 'cant_esperd_d': 1440, 'cant_esperd_m': 43200, 'cant_esperd_a': 518400, 'minutos': 1},\n",
    "        '5min': {'cant_esperd_h': 12, 'cant_esperd_d': 288, 'cant_esperd_m': 8640, 'cant_esperd_a': 103680, 'minutos': 5},\n",
    "        '10min': {'cant_esperd_h': 6, 'cant_esperd_d': 144, 'cant_esperd_m': 4320, 'cant_esperd_a': 51840, 'minutos': 10},\n",
    "        'h': {'cant_esperd_h': 1, 'cant_esperd_d': 24, 'cant_esperd_m': 720, 'cant_esperd_a': 8640}\n",
    "    }\n",
    "\n",
    "    # Obtener el valor de la estación\n",
    "    station_value = df['station'].values[0] #Station\n",
    "    freqinst200b_station = freqinst200b.loc[freqinst200b['station'] == station_value]  #Station\n",
    "    periodos = freqinst200b_station['FreqInf'].values[0]\n",
    "\n",
    "    if pd.isna(periodos):\n",
    "        try:\n",
    "            periodos = pd.infer_freq(df[columna_fecha][-25:])\n",
    "            print(periodos)\n",
    "            if periodos is None:\n",
    "                project_value = freqinst100b_station['Instituc'].values[0]\n",
    "                periodos = {'CENICAFE': '5min', 'IDEAM': '10min', 'CAR': 'h', 'IDIGER': 'min'}.get(project_value, 'min')\n",
    "                print(f\"Frecuencia inferida para {df['Station']} es None. Se determina según entidad {project_value}\")\n",
    "                \n",
    "                #return None, None, None, None, None, None\n",
    "        except ValueError as e:\n",
    "            print(f'Error al inferir la frecuencia en el archivo {df}: {str(e)}')\n",
    "            return None, None, None, None, None, None\n",
    "\n",
    "    # Obtener las cantidades esperadas y el offset en minutos\n",
    "    cant_esperd_h = frecuencias[periodos]['cant_esperd_h']\n",
    "    cant_esperd_d = frecuencias[periodos]['cant_esperd_d']\n",
    "    cant_esperd_m = frecuencias[periodos]['cant_esperd_m']\n",
    "    cant_esperd_a = frecuencias[periodos]['cant_esperd_a']\n",
    "    if periodos == 'h':\n",
    "        pass\n",
    "    else:\n",
    "        minutoffset = frecuencias[periodos]['minutos']\n",
    "\n",
    "    # Ajustar la hora de cada registro\n",
    "    df_c = df.copy()\n",
    "    if periodos == 'h':\n",
    "        df_c[columna_fecha] = df_c[columna_fecha] #- pd.Timedelta(hours=1)\n",
    "    else:\n",
    "        df_c[columna_fecha] = df_c[columna_fecha] - pd.Timedelta(minutes=minutoffset)\n",
    "\n",
    "    # Establecer la columna de fecha como índice\n",
    "    df_c.set_index(columna_fecha, inplace=True)\n",
    "\n",
    "    # Función para verificar si un día, mes o año tiene suficientes datos\n",
    "    def complet_hora(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_h * porc_min\n",
    "    \n",
    "    def complet_dia(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_d * porc_min\n",
    "\n",
    "    def complet_mes(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_m * porc_min\n",
    "\n",
    "    def complet_anio(sub_df):\n",
    "        return len(sub_df) >= cant_esperd_a * porc_min\n",
    "\n",
    "    return df_c, periodos, complet_hora, complet_dia, complet_mes, complet_anio\n",
    "\n",
    "# # Uso funciones de frecuencias\n",
    "# df_example = pd.read_csv('../../OE_3_QC_Variables/2_HumedadRelativa/Test_QC/Estacion_0011115501.csv', encoding='latin-1')#, dtype={'Estado_Anterior':str})\n",
    "# if 'Estado' in df_example.columns:\n",
    "#     df_example = df_example[df_example['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PC']]))]    \n",
    "columna_fecha = 'event_time' # Fecha\n",
    "freq_csv_path = '../../../OE_3_QC_Variables/4_HumedadSuelo/HRS10/EMAHRS10_LatLonEntFreq.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eb23a3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para cálculo de derivadas de varios archivos en una sola carpeta\n",
    "def calc_deriv_HRS10(carpeta, chunk_size=540000):\n",
    "    archivos = os.listdir(carpeta)\n",
    "\n",
    "    # Se recorre cada archivo en la carpeta\n",
    "    for archivo in archivos:\n",
    "        if archivo.endswith('.csv'):\n",
    "            ruta_archivo = os.path.join(carpeta, archivo)\n",
    "        \n",
    "            # Se procesan los archivos csv por fragmentos\n",
    "            reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=chunk_size)\n",
    "            \n",
    "            for chunk in reader:\n",
    "                # Se generan dataframes analizados\n",
    "                # De cada chunk se transforma a datetime la serie/columna 'Fecha'\n",
    "                try:\n",
    "                    chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                except ValueError:\n",
    "                    chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    try:\n",
    "                        dfC = chunk[~chunk['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PSO0','0PAT','0PER']]))]\n",
    "                        dfC_c = dfC.copy()\n",
    "                        station_value = dfC_c['Station'].values[0]\n",
    "                    except IndexError:\n",
    "                        print(f\"Error en el archivo {archivo}: dfC está vacío. Saltando al siguiente archivo.\")\n",
    "                        continue  # Sale del bucle de chunks y continúa con el siguiente archivo\n",
    "                else:\n",
    "                    chunk_c = chunk.copy()\n",
    "                    station_value = chunk_c['Station'].values[0]\n",
    "\n",
    "                # Humedad relativa media horaria\n",
    "                def HRS10_MEDIA_H(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):  \n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                        \n",
    "                    # Se filtran los que hayan superaddo las pruebas\n",
    "                    df_c, periodos, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df_c.groupby([df_c.index.date, df_c.index.hour]).filter(complet_hora)\n",
    "                    hrs10_med_h = df_filtrado[['Valor']].resample('h').mean()\n",
    "                \n",
    "                    return hrs10_med_h[['Valor']]\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    dfhrs10_med_h = HRS10_MEDIA_H(dfC_c)\n",
    "                else:\n",
    "                    dfhrs10_med_h = HRS10_MEDIA_H(chunk_c)\n",
    "                \n",
    "                ## Derivadas diarias, mensuales y anuales\n",
    "                # Humedad relativa del aire a 10 cm media diaria\n",
    "                def HRS10_MEDIA_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    df.reset_index(inplace=True)\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    \n",
    "                    # Cantidad esperada por día\n",
    "                    cant_esperd_d = 24\n",
    "                    # Función para verificar si una hora específica tiene suficientes datos\n",
    "                    def complet_dia(sub_df, total_esperado=cant_esperd_d, porc_min=porc_min):\n",
    "                        return len(sub_df) >= total_esperado * porc_min\n",
    "                    \n",
    "                    df.set_index('Fecha', inplace=True)\n",
    "                    # Se filtran los datos que tienen la complititud mínima\n",
    "                    df_filtrado = df.groupby([df.index.date]).filter(complet_dia) #dfC.groupby([dfC.index.date]).filter(complet_dia)\n",
    "                    hrs10_med_d = df_filtrado[[columna_valor]].resample('D').mean()\n",
    "                \n",
    "                    return hrs10_med_d\n",
    "                \n",
    "                dfhrs10_med_d = HRS10_MEDIA_D(dfhrs10_med_h)                \n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                #if 'Estado' in chunk.columns:\n",
    "                #    dfhrs10_med_d = HRS10_MEDIA_D(dfC_c)\n",
    "                #else:\n",
    "                #    dfhrs10_med_d = HRS10_MEDIA_D(chunk_c)\n",
    "                \n",
    "                # Presión atmosférica media mensual\n",
    "                def HRS10_MEDIA_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):    \n",
    "                    if df is None:\n",
    "                        return None                  \n",
    "                    days_in_month = df.index.to_series().dt.days_in_month\n",
    "                    days_in_month = days_in_month.resample('ME').first()\n",
    "                    \n",
    "                    # Función para verificar si una hora específica tiene suficientes datos\n",
    "                    def complet_mes(sub_df):\n",
    "                        mes = sub_df.index[0].month\n",
    "                        total_esperado = days_in_month[days_in_month.index.month == mes].iloc[0]\n",
    "                        return len(sub_df) >= total_esperado * porc_min\n",
    "                \n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df.groupby([df.index.year, df.index.month]).filter(complet_mes)\n",
    "                    hrs10_med_m = df_filtrado[['Valor']].resample('ME').mean()\n",
    "                    \n",
    "                    return hrs10_med_m\n",
    "                \n",
    "                dfhrs10_med_m = HRS10_MEDIA_M(dfhrs10_med_d)\n",
    "                \n",
    "                # Presión atmosférica del Aire a 2 metros media anual\n",
    "                def HRS10_MEDIA_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    if df is None:\n",
    "                        return None\n",
    "                    \n",
    "                    # Función para verificar si una hora específica tiene suficientes datos\n",
    "                    def complet_anio(sub_df):\n",
    "                        return len(sub_df) >= 12 * porc_min\n",
    "                \n",
    "                    # Luego de establecer el índice, aplicar resample\n",
    "                    df_filtrado = df.groupby([df.index.year]).filter(complet_anio)\n",
    "                    hrs10_med_a = df_filtrado[['Valor']].resample('YE').mean()\n",
    "                    \n",
    "                    return hrs10_med_a\n",
    "                \n",
    "                dfhrs10_med_a = HRS10_MEDIA_A(dfhrs10_med_m)\n",
    "                \n",
    "                ###-- Derivados máximos y mínimos\n",
    "                #Humedad relativa del aire a 10 cm  mínima diaria\n",
    "                def HRS10_MN_H(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodo, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                    if df_c is None or complet_hora is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Filtrar los días que tienen suficientes datos\n",
    "                    df_c['Fecha_temp'] = df_c.index\n",
    "                    horas_validas = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_hora).index\n",
    "                    \n",
    "                    # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "                    horas_validas = pd.to_datetime(horas_validas).normalize()\n",
    "                    df_filtrado = df_c[df_c.index.normalize().isin(horas_validas)]\n",
    "                    \n",
    "                    # Encontrar el valor mínimo por cada día válido\n",
    "                    idx_minimos_hora = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmin()\n",
    "                    mn_h = df_filtrado.loc[idx_minimos_hora]\n",
    "                    \n",
    "                    # Eliminar la columna temporal antes de retornar el resultado\n",
    "                    if 'Fecha_temp' in mn_h.columns:\n",
    "                        mn_h.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                    \n",
    "                    return mn_h[[columna_valor]]\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    df_mn_h = HRS10_MN_H(dfC_c)\n",
    "                else:\n",
    "                    df_mn_h = HRS10_MN_H(chunk_c)\n",
    "                    \n",
    "                #Humedad relativa del aire a 10 cm  mínima diaria\n",
    "                def HRS10_MX_H(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodo, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                    if df_c is None or complet_hora is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Filtrar los días que tienen suficientes datos\n",
    "                    df_c['Fecha_temp'] = df_c.index\n",
    "                    horas_validas = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_hora).index\n",
    "                    \n",
    "                    # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "                    horas_validas = pd.to_datetime(horas_validas).normalize()\n",
    "                    df_filtrado = df_c[df_c.index.normalize().isin(horas_validas)]\n",
    "                    \n",
    "                    # Encontrar el valor mínimo por cada día válido\n",
    "                    idx_maximos_hora = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmax()\n",
    "                    mx_h = df_filtrado.loc[idx_maximos_hora]\n",
    "                    \n",
    "                    # Eliminar la columna temporal antes de retornar el resultado\n",
    "                    if 'Fecha_temp' in mx_h.columns:\n",
    "                        mx_h.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                    \n",
    "                    return mx_h[[columna_valor]]\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    df_mx_h = HRS10_MX_H(dfC_c)\n",
    "                else:\n",
    "                    df_mx_h = HRS10_MX_H(chunk_c)\n",
    "                \n",
    "                def HRS10_MN_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodo,_,complet_dia, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                    if df_c is None or complet_dia is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Filtrar los días que tienen suficientes datos\n",
    "                    df_c['Fecha_temp'] = df_c.index\n",
    "                    dias_validos = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_dia).index\n",
    "                    \n",
    "                    # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "                    dias_validos = pd.to_datetime(dias_validos).normalize()\n",
    "                    df_filtrado = df_c[df_c.index.normalize().isin(dias_validos)]\n",
    "                    \n",
    "                    # Encontrar el valor mínimo por cada día válido\n",
    "                    idx_minimos_dia = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmin()\n",
    "                    mn_d = df_filtrado.loc[idx_minimos_dia]\n",
    "                    \n",
    "                    # Eliminar la columna temporal antes de retornar el resultado\n",
    "                    if 'Fecha_temp' in mn_d.columns:\n",
    "                        mn_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                    \n",
    "                    return mn_d[[columna_valor]]\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    df_mn_d = HRS10_MN_D(dfC_c)\n",
    "                else:\n",
    "                    df_mn_d = HRS10_MN_D(chunk_c)\n",
    "                \n",
    "                # Humedad relativa del aire a 10 cm  máxima diaria\n",
    "                def HRS10_MX_D(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodo,_,complet_dia,_,_ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                    if df_c is None or complet_dia is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Filtrar los días que tienen suficientes datos\n",
    "                    df_c['Fecha_temp'] = df_c.index\n",
    "                    dias_validos = df_c.groupby(df_c['Fecha_temp'].dt.date).filter(complet_dia).index\n",
    "                    # Filtrar el DataFrame original para incluir solo los días válidos\n",
    "                    dias_validos = pd.to_datetime(dias_validos).normalize()\n",
    "                    df_filtrado = df_c[df_c.index.normalize().isin(dias_validos)]\n",
    "                    # Encontrar el valor mínimo por cada día válido\n",
    "                    idx_maximos_dia = df_filtrado.groupby(df_filtrado.index.to_series().dt.date)[columna_valor].idxmax()\n",
    "                    mx_d = df_filtrado.loc[idx_maximos_dia]\n",
    "                    \n",
    "                    # Eliminar la columna temporal antes de retornar el resultado\n",
    "                    if 'Fecha_temp' in mx_d.columns:\n",
    "                        mx_d.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                    \n",
    "                    return mx_d[[columna_valor]]\n",
    "                \n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    df_mx_d = HRS10_MX_D(dfC_c)\n",
    "                else:\n",
    "                    df_mx_d = HRS10_MX_D(chunk_c)\n",
    "                \n",
    "                # Humedad relativa del aire a 10 cm  mínima mensual\n",
    "                def HRS10_MN_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodo,_,_,complet_mes,_ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                    if df_c is None or complet_mes is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Filtrar los meses que tienen suficientes datos\n",
    "                    df_c['Fecha_temp'] = df_c.index\n",
    "                    # Eliminar duplicados en el índice\n",
    "                    df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "                    meses_validos = df_c.groupby(df_c.index.to_period('M')).filter(complet_mes).index.to_period('M')\n",
    "                \n",
    "                    # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "                    df_filtrado = df_c[df_c.index.to_period('M').isin(meses_validos)]\n",
    "                \n",
    "                    # Encontrar el valor mínimo por cada mes válido\n",
    "                    idx_minimos_mes = df_filtrado.groupby(df_filtrado.index.to_period('M'))[columna_valor].idxmin()\n",
    "                    min_m = df_filtrado.loc[idx_minimos_mes]\n",
    "                \n",
    "                    # Eliminar las columnas temporales antes de retornar el resultado\n",
    "                    if 'Fecha_temp' in min_m.columns:\n",
    "                        min_m.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                \n",
    "                    return min_m[[columna_valor]]\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    df_mn_m = HRS10_MN_M(dfC_c)\n",
    "                else:\n",
    "                    df_mn_m = HRS10_MN_M(chunk_c)\n",
    "                \n",
    "                # Humedad relativa del aire a 10 cm  máxima mensual\n",
    "                def HRS10_MX_M(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodo,_,_,complet_mes,_ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                    if df_c is None or complet_mes is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Filtrar los meses que tienen suficientes datos\n",
    "                    df_c['Fecha_temp'] = df_c.index\n",
    "                    # Eliminar duplicados en el índice\n",
    "                    df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "                    meses_validos = df_c.groupby(df_c.index.to_period('M')).filter(complet_mes).index.to_period('M')\n",
    "                \n",
    "                    # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "                    df_filtrado = df_c[df_c.index.to_period('M').isin(meses_validos)]\n",
    "                \n",
    "                    # Encontrar el valor mínimo por cada mes válido\n",
    "                    idx_maximos_mes = df_filtrado.groupby(df_filtrado.index.to_period('M'))[columna_valor].idxmax()\n",
    "                    max_m = df_filtrado.loc[idx_maximos_mes]\n",
    "                \n",
    "                    # Eliminar las columnas temporales antes de retornar el resultado\n",
    "                    if 'Fecha_temp' in max_m.columns:\n",
    "                        max_m.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                \n",
    "                    return max_m[[columna_valor]]\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    df_mx_m = HRS10_MX_M(dfC_c)\n",
    "                else:\n",
    "                    df_mx_m = HRS10_MX_M(chunk_c)\n",
    "                \n",
    "                # Humedad relativa del aire a 10 cm mínima anual\n",
    "                def HRS10_MN_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodo,_,_,_,complet_anio = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                    if df_c is None or complet_anio is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Filtrar los meses que tienen suficientes datos\n",
    "                    df_c['Fecha_temp'] = df_c.index\n",
    "                    # Eliminar duplicados en el índice\n",
    "                    df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "                    anios_validos = df_c.groupby(df_c.index.to_period('Y')).filter(complet_anio).index.to_period('Y')\n",
    "                \n",
    "                    # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "                    df_filtrado = df_c[df_c.index.to_period('Y').isin(anios_validos)]\n",
    "                \n",
    "                    # Encontrar el valor mínimo por cada mes válido\n",
    "                    idx_minimos_anio = df_filtrado.groupby(df_filtrado.index.to_period('Y'))[columna_valor].idxmin()\n",
    "                    min_a = df_filtrado.loc[idx_minimos_anio]\n",
    "                \n",
    "                    # Eliminar las columnas temporales antes de retornar el resultado\n",
    "                    if 'Fecha_temp' in min_a.columns:\n",
    "                        min_a.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                \n",
    "                    return min_a[[columna_valor]]\n",
    "\n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    df_mn_a = HRS10_MN_A(dfC_c)\n",
    "                else:\n",
    "                    df_mn_a = HRS10_MN_A(chunk_c)\n",
    "                \n",
    "                # Humedad relativa del aire a 10 cm  máxima anual\n",
    "                def HRS10_MX_A(df, columna_fecha='Fecha', columna_valor='Valor', porc_min=0.67):\n",
    "                    # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                    if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                        df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    # Se llama la función de procesamiento de frecuencias\n",
    "                    df_c, periodo,_,_,_,complet_anio = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "                \n",
    "                    if df_c is None or complet_anio is None:\n",
    "                        return None\n",
    "                \n",
    "                    # Filtrar los meses que tienen suficientes datos\n",
    "                    df_c['Fecha_temp'] = df_c.index\n",
    "                    # Eliminar duplicados en el índice\n",
    "                    df_c = df_c[~df_c.index.duplicated(keep='first')]\n",
    "                    anios_validos = df_c.groupby(df_c.index.to_period('Y')).filter(complet_anio).index.to_period('Y')\n",
    "                \n",
    "                    # Filtrar el DataFrame original para incluir solo los meses válidos\n",
    "                    df_filtrado = df_c[df_c.index.to_period('Y').isin(anios_validos)]\n",
    "                \n",
    "                    # Encontrar el valor mínimo por cada mes válido\n",
    "                    idx_maximos_anio = df_filtrado.groupby(df_filtrado.index.to_period('Y'))[columna_valor].idxmax()\n",
    "                    max_a = df_filtrado.loc[idx_maximos_anio]\n",
    "                \n",
    "                    # Eliminar las columnas temporales antes de retornar el resultado\n",
    "                    if 'Fecha_temp' in max_a.columns:\n",
    "                        max_a.drop(columns=['Fecha_temp'], inplace=True)\n",
    "                \n",
    "                    return max_a[[columna_valor]]\n",
    "                    \n",
    "                # Se llama el archivo original según si es dato crudo o con qc\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    df_mx_a = HRS10_MX_A(dfC_c)\n",
    "                else:\n",
    "                    df_mx_a = HRS10_MX_A(chunk_c)\n",
    "        \n",
    "                ### Se crea un nuevo archivo Excel con openpyxl\n",
    "                wb = Workbook()\n",
    "                sheets = {\n",
    "                    'HRS10_MEDIA_H': dfhrs10_med_h, 'HRS10_MEDIA_D': dfhrs10_med_d,\n",
    "                    'HRS10_MEDIA_M': dfhrs10_med_m, 'HRS10_MEDIA_A': dfhrs10_med_a, \n",
    "                    'HRS10_MN_H': df_mn_h,'HRS10_MX_H': df_mx_h,\n",
    "                    'HRS10_MN_D': df_mn_d,'HRS10_MX_D': df_mx_d, \n",
    "                    'HRS10_MN_M': df_mn_m,'HRS10_MX_M': df_mx_m, \n",
    "                    'HRS10_MN_A': df_mn_a,'HRS10_MX_A': df_mx_a \n",
    "                }\n",
    "                \n",
    "                # Si el workbook todavía tiene la hoja por defecto, se elimina\n",
    "                if \"Sheet\" in wb.sheetnames:\n",
    "                    del wb[\"Sheet\"]\n",
    "                \n",
    "                for sheet_name, data in sheets.items():\n",
    "                    ws = wb.create_sheet(title=sheet_name)\n",
    "                    \n",
    "                    # Agregamos los datos al Excel\n",
    "                    if data is None:\n",
    "                        pass\n",
    "                    else:\n",
    "                        for r_idx, row in enumerate(dataframe_to_rows(data, index=True, header=True), 1):\n",
    "                            for c_idx, value in enumerate(row, 1):\n",
    "                                ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "                    \n",
    "                    # Crear una gráfica\n",
    "                    chart = LineChart()\n",
    "                    chart.title = sheet_name\n",
    "                    chart.style = 5\n",
    "                    chart.y_axis.title = 'Humedad del suelo -10cm (%)'\n",
    "                    chart.x_axis.title = 'Fecha'\n",
    "                    \n",
    "                    # Establecer datos para la gráfica\n",
    "                    max_row = ws.max_row\n",
    "                    values = Reference(ws, min_col=2, min_row=2, max_col=2, max_row=max_row)\n",
    "                    dates = Reference(ws, min_col=1, min_row=3, max_col=1, max_row=max_row)\n",
    "                    chart.add_data(values, titles_from_data=True)\n",
    "                    chart.set_categories(dates)\n",
    "                    \n",
    "                    # Quitar la leyenda\n",
    "                    chart.legend = None\n",
    "                    \n",
    "                    # Cambiar el grosor de la línea a 0.5 puntos (equivalente a 50 centésimas de punto)\n",
    "                    for series in chart.series:\n",
    "                        series.graphicalProperties.line.width = 50\n",
    "                        series.graphicalProperties.line.solidFill = \"5499c7\"\n",
    "                        \n",
    "                    # Posicionar la gráfica en el Excel\n",
    "                    ws.add_chart(chart, \"E3\")\n",
    "                \n",
    "                # Nombres archivos\n",
    "                if 'Estado' in chunk.columns:\n",
    "                    nombre_archivo_salida = os.path.join(carpeta, archivo[:22] + '_deriv.xlsx')\n",
    "                else:\n",
    "                    nombre_archivo_salida = os.path.join(carpeta, archivo[:19] + '_deriv.xlsx')\n",
    "                #nombre_archivo_salida = os.path.join(carpeta, archivo[:22] + '_deriv.xlsx') #archivo[:19] el original #archivo[:22] con qc\n",
    "                \n",
    "                # Guardar el archivo Excel\n",
    "                wb.save(nombre_archivo_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8ea6bfa7-5091-42aa-ab44-973ef426e28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_4440\\3456756448.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_4440\\3456756448.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_4440\\3456756448.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en el archivo Estacion_0023195040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0024015110_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0026015010_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0026015040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0026055120_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_4440\\3456756448.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_4440\\3456756448.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_4440\\3456756448.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en el archivo Estacion_0035017020_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0046015030_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_4440\\3456756448.py:13: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error en el archivo Estacion_0054017040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n"
     ]
    }
   ],
   "source": [
    "calc_deriv_HRS10('../../../OE_3_QC_Variables/4_HumedadSuelo/HRS10/QCResult_HRS10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c0262efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_deriv_HRS10('../../../OE_3_QC_Variables/4_HumedadSuelo/HRS10/RawUnmodified_HRS10')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df022d20-772d-4041-a291-da25abf644e6",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0bb96-7e66-4509-90a3-e1a97bf517ec",
   "metadata": {},
   "source": [
    "## Función cálculo masivo de derivadas y alistamiento Cassandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eac274a3-3b32-403e-a606-0004f3c97c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_deriv_HRS10_QC(carpeta):#, chunk_size=540000):\n",
    "    archivos = os.listdir(carpeta)\n",
    "\n",
    "    # Se crean carpetas para cada tipo de DataFrame si no existen\n",
    "    carpetas_salidas = ['HRS10_MEDIA_H_QC', 'HRS10_MEDIA_D_QC', 'HRS10_MEDIA_M_QC', 'HRS10_MEDIA_A_QC']#,\n",
    "                        #'HRS10_MN_H_QC', 'HRS10_MX_H_QC', 'HRS10_MN_D_QC', 'HRS10_MX_D_QC',\n",
    "                        #'HRS10_MN_M_QC', 'HRS10_MX_M_QC', 'HRS10_MN_A_QC', 'HRS10_MX_A_QC']\n",
    "\n",
    "    for cs in carpetas_salidas:\n",
    "        os.makedirs(os.path.join(carpeta, cs), exist_ok=True)\n",
    "    \n",
    "    # Procesar cada archivo en la carpeta\n",
    "    for archivo in archivos:\n",
    "        if archivo.endswith('.csv'):\n",
    "            ruta_archivo = os.path.join(carpeta, archivo)\n",
    "            df = pd.read_csv(ruta_archivo, encoding='latin-1')\n",
    "\n",
    "            # Procesar la fecha y filtrar según estado, como en el ejemplo original\n",
    "            try:\n",
    "                df['event_time'] = pd.to_datetime(df['event_time'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "            except ValueError:\n",
    "                df['event_time'] = pd.to_datetime(df['event_time'], format='%Y-%m-%d %H:%M:%S')\n",
    "                  \n",
    "            try:\n",
    "                dfC = df[df['state'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PC']]))] # Se comenta si es de crudos\n",
    "                station_value = dfC['station'].values[0] #df['station'].values[0]\n",
    "            except IndexError:\n",
    "                print(f\"Error en el archivo {archivo}: dfC está vacío. Saltando al siguiente archivo.\")\n",
    "                continue  # Sale del bucle de chunks y continúa con el siguiente archivo\n",
    "\n",
    "            # Humedad relativa media horaria\n",
    "            def HRS10_MEDIA_H_QC(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.67):  \n",
    "                # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                    df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                    \n",
    "                # Se filtran los que hayan superaddo las pruebas\n",
    "                df_c, periodos, complet_hora, _, _, _ = process_frequencies(df, columna_fecha, freq_csv_path, porc_min)\n",
    "\n",
    "                # Luego de establecer el índice, aplicar resample\n",
    "                df_filtrado = df_c.groupby([df_c.index.date, df_c.index.hour]).filter(complet_hora)\n",
    "                HRS10_60_med_h = df_filtrado[['event_value']].resample('h').mean()\n",
    "\n",
    "                # Cambio de contenido de columnas\n",
    "                HRS10_60_med_h['station'] = df['station'].iloc[0]\n",
    "                HRS10_60_med_h['station'] = HRS10_60_med_h['station'].astype('int64')\n",
    "                HRS10_60_med_h['label'] = 'HRS10_MEDIA_H_QC'\n",
    "                \n",
    "                # Reset index\n",
    "                HRS10_60_med_h.reset_index(inplace=True)\n",
    "                \n",
    "                # Se reordenan las columnas\n",
    "                nuevo_orden = ['station', 'label', 'event_time', 'event_value']\n",
    "                # Reordenar las columnas usando el nuevo orden\n",
    "                HRS10_60_med_h = HRS10_60_med_h[nuevo_orden]\n",
    "            \n",
    "                # Convertir 'event_time' de nuevo a datetime para uniformidad\n",
    "                HRS10_60_med_h['event_time'] = pd.to_datetime(HRS10_60_med_h['event_time'])\n",
    "            \n",
    "                return HRS10_60_med_h\n",
    "\n",
    "            dfhrs10_med_h = HRS10_MEDIA_H_QC(dfC)\n",
    "            \n",
    "            # Humedad relativa media diaria\n",
    "            def HRS10_MEDIA_D_QC(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.67):\n",
    "                if df.empty:\n",
    "                    return df\n",
    "                df.reset_index(inplace=True)\n",
    "                # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                    df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                # Ajustar la hora de cada registro para que corresponda al rango deseado\n",
    "                df[columna_fecha] = df[columna_fecha] - pd.Timedelta(hours=1)\n",
    "                # Establecer la columna de fecha como índice\n",
    "                df.set_index(columna_fecha, inplace=True)\n",
    "                \n",
    "                # Calcular el total de registros esperados por día pluviométrico\n",
    "                total_esperado_por_dia = 24\n",
    "                # Función para verificar si un día pluviométrico tiene suficientes datos\n",
    "                def complet_dia(sub_df):\n",
    "                    return len(sub_df) >= total_esperado_por_dia * porc_min\n",
    "            \n",
    "                # Filtrar los días con suficientes datos y calcular el promedio diario\n",
    "                df_filtrado = df.groupby([df.index.date]).filter(complet_dia)\n",
    "                # Verificar si el DataFrame filtrado está vacío\n",
    "                if df_filtrado.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por días válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return df_filtrado\n",
    "                    \n",
    "                # Se calcula la media\n",
    "                HRS10_60_med_d = df_filtrado[[columna_valor]].resample('D').mean()\n",
    "\n",
    "                # Cambio de contenido de columnas\n",
    "                HRS10_60_med_d['station'] = df['station'].iloc[0]\n",
    "                HRS10_60_med_d['station'] = HRS10_60_med_d['station'].astype('int64')\n",
    "                HRS10_60_med_d['label'] = 'HRS10_MEDIA_D_QC'\n",
    "                \n",
    "                # Reset index\n",
    "                HRS10_60_med_d.reset_index(inplace=True)\n",
    "                \n",
    "                # Se reordenan las columnas\n",
    "                nuevo_orden = ['station', 'label', 'event_time', 'event_value']\n",
    "                # Reordenar las columnas usando el nuevo orden\n",
    "                HRS10_60_med_d = HRS10_60_med_d[nuevo_orden]\n",
    "            \n",
    "                # Convertir 'event_time' de nuevo a datetime para uniformidad\n",
    "                HRS10_60_med_d['event_time'] = pd.to_datetime(HRS10_60_med_d['event_time'])\n",
    "            \n",
    "                return HRS10_60_med_d\n",
    "\n",
    "            dfhrs10_med_d = HRS10_MEDIA_D_QC(dfhrs10_med_h)\n",
    "            \n",
    "            # Humedad relativa media mensual\n",
    "            dfhrs10_med_d_c = dfhrs10_med_d.copy()\n",
    "            def HRS10_MEDIA_M_QC(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.67):    \n",
    "                if df.empty:\n",
    "                    return df\n",
    "                df.reset_index(inplace=True)\n",
    "                # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                    df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                \n",
    "                # Establecer la columna 'event_time' como índice\n",
    "                df.set_index(columna_fecha, inplace=True)\n",
    "                \n",
    "                days_in_month = df.index.to_series().dt.days_in_month\n",
    "                days_in_month = days_in_month.resample('ME').first()\n",
    "                \n",
    "                # Función para verificar si una hora específica tiene suficientes datos\n",
    "                def complet_mes(sub_df):\n",
    "                    mes = sub_df.index[0].month\n",
    "                    total_esperado = days_in_month[days_in_month.index.month == mes].iloc[0]\n",
    "                    return len(sub_df) >= total_esperado * porc_min\n",
    "            \n",
    "                # Luego de establecer el índice, aplicar resample\n",
    "                df_filtrado = df.groupby([df.index.year, df.index.month]).filter(complet_mes)\n",
    "                # Verificar si el DataFrame filtrado está vacío\n",
    "                if df_filtrado.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por meses válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return df_filtrado\n",
    "                    \n",
    "                # Se calcula la media \n",
    "                HRS10_60_med_m = df_filtrado[['event_value']].resample('ME').mean()\n",
    "\n",
    "                # Cambio de contenido de columnas\n",
    "                HRS10_60_med_m['station'] = df['station'].iloc[0]\n",
    "                HRS10_60_med_m['station'] = HRS10_60_med_m['station'].astype('int64')\n",
    "                HRS10_60_med_m['label'] = 'HRS10_MEDIA_M_QC'\n",
    "                \n",
    "                # Reset index\n",
    "                HRS10_60_med_m.reset_index(inplace=True)\n",
    "                \n",
    "                # Se reordenan las columnas\n",
    "                nuevo_orden = ['station', 'label', 'event_time', 'event_value']\n",
    "                # Reordenar las columnas usando el nuevo orden\n",
    "                HRS10_60_med_m = HRS10_60_med_m[nuevo_orden]\n",
    "            \n",
    "                # Convertir 'event_time' de nuevo a datetime para uniformidad\n",
    "                HRS10_60_med_m['event_time'] = pd.to_datetime(HRS10_60_med_m['event_time'])\n",
    "                \n",
    "                return HRS10_60_med_m\n",
    "            \n",
    "            dfhrs10_med_m = HRS10_MEDIA_M_QC(dfhrs10_med_d_c)\n",
    "            \n",
    "            # Humedad relativa media anual\n",
    "            dfhrs10_med_m_c = dfhrs10_med_m.copy()\n",
    "            def HRS10_MEDIA_A_QC(df, columna_fecha='event_time', columna_valor='event_value', porc_min=0.67):\n",
    "                if df.empty:\n",
    "                    return df\n",
    "                df.reset_index(inplace=True)\n",
    "                # Convertir la columna de fecha a datetime si aún no lo es\n",
    "                if not pd.api.types.is_datetime64_any_dtype(df[columna_fecha]):\n",
    "                    df[columna_fecha] = pd.to_datetime(df[columna_fecha])\n",
    "                \n",
    "                # Función para verificar si una hora específica tiene suficientes datos\n",
    "                def complet_anio(sub_df):\n",
    "                    return len(sub_df) >= 12 * porc_min\n",
    "                   \n",
    "                # Antes de resample, establecer la columna 'event_time' como índice\n",
    "                df.set_index(columna_fecha, inplace=True)\n",
    "            \n",
    "                # Luego de establecer el índice, aplicar resample\n",
    "                df_filtrado = df.groupby([df.index.year]).filter(complet_anio)\n",
    "                # Verificar si el DataFrame filtrado está vacío\n",
    "                if df_filtrado.empty:\n",
    "                    print(f\"DataFrame vacío después de filtrar por años válidos. Regresando {archivo[:19]} vacío.\")\n",
    "                    return df_filtrado\n",
    "                    \n",
    "                # Se calcula la media    \n",
    "                HRS10_60_med_a = df_filtrado[['event_value']].resample('YE').mean()\n",
    "\n",
    "                # Cambio de contenido de columnas\n",
    "                HRS10_60_med_a['station'] = df['station'].iloc[0]\n",
    "                HRS10_60_med_a['station'] = HRS10_60_med_a['station'].astype('int64')\n",
    "                HRS10_60_med_a['label'] = 'HRS10_MEDIA_A_QC'\n",
    "                \n",
    "                # Reset index\n",
    "                HRS10_60_med_a.reset_index(inplace=True)\n",
    "                \n",
    "                # Se reordenan las columnas\n",
    "                nuevo_orden = ['station', 'label', 'event_time', 'event_value']\n",
    "                # Reordenar las columnas usando el nuevo orden\n",
    "                HRS10_60_med_a = HRS10_60_med_a[nuevo_orden]\n",
    "            \n",
    "                # Convertir 'event_time' de nuevo a datetime para uniformidad\n",
    "                HRS10_60_med_a['event_time'] = pd.to_datetime(HRS10_60_med_a['event_time'])\n",
    "                \n",
    "                return HRS10_60_med_a\n",
    "            \n",
    "            dfhrs10_med_a = HRS10_MEDIA_A_QC(dfhrs10_med_m_c)\n",
    "\n",
    "            if not dfhrs10_med_h.empty:\n",
    "                dfhrs10_med_h.to_csv(os.path.join(carpeta, 'HRS10_MEDIA_H_QC', f'{archivo[:19]}.csv'), index=False)\n",
    "            if not dfhrs10_med_d.empty:\n",
    "                dfhrs10_med_d.to_csv(os.path.join(carpeta, 'HRS10_MEDIA_D_QC', f'{archivo[:19]}.csv'), index=False)\n",
    "            if not dfhrs10_med_m.empty:\n",
    "                dfhrs10_med_m.to_csv(os.path.join(carpeta, 'HRS10_MEDIA_M_QC', f'{archivo[:19]}.csv'), date_format='%Y-%m', index=False)\n",
    "            if not dfhrs10_med_a.empty:\n",
    "                dfhrs10_med_a.to_csv(os.path.join(carpeta, 'HRS10_MEDIA_A_QC', f'{archivo[:19]}.csv'), date_format='%Y',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90943a6b-2791-482e-b411-611884b55457",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0011115501 vacío.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0016025502 vacío.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0021065501 vacío.\n",
      "DataFrame vacío después de filtrar por meses válidos. Regresando Estacion_0021185090 vacío.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0021255501 vacío.\n",
      "Error en el archivo Estacion_0023195040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0023195501 vacío.\n",
      "Error en el archivo Estacion_0024015110_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0026015010_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "Error en el archivo Estacion_0026015040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "DataFrame vacío después de filtrar por días válidos. Regresando Estacion_0026035501 vacío.\n",
      "Error en el archivo Estacion_0026055120_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0026155502 vacío.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0028025501 vacío.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0032075501 vacío.\n",
      "Error en el archivo Estacion_0035017020_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0044035501 vacío.\n",
      "Error en el archivo Estacion_0046015030_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0051025080 vacío.\n",
      "DataFrame vacío después de filtrar por años válidos. Regresando Estacion_0052055160 vacío.\n",
      "Error en el archivo Estacion_0054017040_qc.csv: dfC está vacío. Saltando al siguiente archivo.\n"
     ]
    }
   ],
   "source": [
    "calc_deriv_HRS10_QC('../../../OE_3_QC_Variables/4_HumedadSuelo/HRS10/ReadyToCassandraFiles_HRS10/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8f8d21-7fc5-42ff-88af-84a73cb7cb5a",
   "metadata": {},
   "source": [
    "### Cálculo promedios horarios mensuales multianuales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15a30dc0-69e9-4016-8d4d-3033f49e5113",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_hmMa_HRS10(carpeta, chunk_size=540000):\n",
    "    archivos = os.listdir(carpeta)\n",
    "\n",
    "    # Se recorre cada archivo en la carpeta\n",
    "    for archivo in archivos:\n",
    "        if archivo.endswith('.csv'):\n",
    "            ruta_archivo = os.path.join(carpeta, archivo)\n",
    "        \n",
    "            # Se procesan los archivos csv por fragmentos\n",
    "            reader = pd.read_csv(ruta_archivo, encoding='latin-1', chunksize=chunk_size)\n",
    "            \n",
    "            for chunk in reader:\n",
    "                # Se generan dataframes analizados\n",
    "                # De cada chunk se transforma a datetime la serie/columna 'Fecha'\n",
    "                try:\n",
    "                    chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S.%f')\n",
    "                except ValueError:\n",
    "                    chunk['Fecha'] = pd.to_datetime(chunk['Fecha'], format='%Y-%m-%d %H:%M:%S')\n",
    "                    chunk = chunk[~chunk['Estado'].apply(lambda x: any([str(x).startswith(prefix) for prefix in ['0PSO0','0PAT','0PER']]))]\n",
    "\n",
    "                # Se hace la agrupación para cálculo de medias horarias mensuales multianuales\n",
    "                hym_ma = chunk['Valor'].groupby(by =[chunk[\"Fecha\"].dt.month, chunk[\"Fecha\"].dt.hour]).mean().unstack(level=0)\n",
    "\n",
    "                # Crear un archivo Excel y agregar los datos\n",
    "                wb = Workbook()\n",
    "                ws = wb.active\n",
    "                ws.title = \"Datos\"\n",
    "                \n",
    "                # Agregar datos al archivo Excel\n",
    "                for r in dataframe_to_rows(hym_ma.reset_index(), index=False, header=True):\n",
    "                    ws.append(r)\n",
    "                \n",
    "                # Crear la gráfica de dispersión\n",
    "                chart = ScatterChart()\n",
    "                chart.title = \"Valores Promedio por Hora y Mes\"\n",
    "                chart.style = 13\n",
    "                chart.x_axis.title = 'Hora del día'\n",
    "                chart.y_axis.title = 'Valor'\n",
    "                \n",
    "                # Aumentar el tamaño del gráfico\n",
    "                chart.width = 20  # Anchura del gráfico (pulgadas)\n",
    "                chart.height = 12  # Altura del gráfico (pulgadas)\n",
    "                \n",
    "                # Fijar el máximo valor del eje x\n",
    "                chart.x_axis.scaling.max = 23\n",
    "                chart.x_axis.scaling.min = 0\n",
    "                chart.x_axis.majorUnit = 1\n",
    "                \n",
    "                # Agregar series a la gráfica\n",
    "                colors = ['1F77B4', 'FF7F0E', '2CA02C', 'D62728', '9467BD', '8C564B', 'E377C2', '7F7F7F', 'BCBD22', '17BECF', 'AEC7E8', 'FFBB78']\n",
    "                for i in range(2, 14):  # Columnas B a M (meses 1 a 12)\n",
    "                    xvalues = Reference(ws, min_col=1, min_row=2, max_row=25)\n",
    "                    yvalues = Reference(ws, min_col=i, min_row=1, max_row=25)\n",
    "                    series = Series(yvalues, xvalues, title_from_data=True)\n",
    "                    series.graphicalProperties.line.solidFill = colors[i % len(colors)]  # Asignar colores a las líneas\n",
    "                    series.graphicalProperties.line.width = 30000  # Ajustar el grosor de las líneas\n",
    "                    series.marker.symbol = 'circle'  # Cambiar el marcador a círculo\n",
    "                    series.marker.size = 5\n",
    "                    series.marker.graphicalProperties.solidFill = colors[i % len(colors)]  # Cambiar el color del marcador\n",
    "                    chart.series.append(series)\n",
    "                \n",
    "                # Insertar la gráfica en la hoja de cálculo\n",
    "                ws.add_chart(chart, \"O2\")\n",
    "\n",
    "                # Nombres archivos\n",
    "                nombre_archivo_salida = os.path.join(carpeta, archivo[:19] + '_hm_ma.xlsx') #archivo[:22] el de qc\n",
    "                # Guardar el archivo Excel\n",
    "                wb.save(nombre_archivo_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e1c99242-480a-4d33-afb9-c9b9bf75ad91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_16064\\2946228248.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_16064\\2946228248.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_16064\\2946228248.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_16064\\2946228248.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_16064\\2946228248.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_16064\\2946228248.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n",
      "C:\\Users\\palvarez\\AppData\\Local\\Temp\\ipykernel_16064\\2946228248.py:12: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  for chunk in reader:\n"
     ]
    }
   ],
   "source": [
    "calc_hmMa_HRS10('../../../OE_3_QC_Variables/4_HumedadSuelo/HRS10/QCResult_HRS10/')  #RawUnmodified_HRS10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "22d9bdeb-8899-45bf-b11b-629548d11d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_hmMa_HRS10('../../../OE_3_QC_Variables/4_HumedadSuelo/HRS10/RawUnmodified_HRS10/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b093d7e7-3ece-44d8-993c-bc18bb75d842",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
